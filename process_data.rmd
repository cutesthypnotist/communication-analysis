
Package installation
```{r}
install.packages("dplyr")
install.packages("tidyr")
install.packages("readr")
install.packages("pwr")
install.packages("fs")
install.packages("purrr")
install.packages("data.table")
install.packages("janitor")
install.packages("magrittr")
install.packages("tidyverse")
install.packages("stringr")
install.packages("ggpubr")
install.packages("ggplot2")
install.packages("rstatix")
install.packages("dtplyr")
install.packages("tidyverse")
install.packages("textclean")
```

Initialize commonly used libraries, variables, and functions.
```{r}

library(tidyr)
library(fs)
library(dplyr)
library(readr)
library(purrr)
library(data.table)
library(pwr)
library(janitor)
library(magrittr) 
library(tidyverse)
library(stringr)
library(ggpubr)
library(ggplot2)
library(rstatix)
library(qqplotr)
library(here)
library(tidyverse)
library(qqplotr)
library(rstatix)
library(broom)
library(knitr)
library(ggpubr)
library(gtools)
library(GGally)
library(correlation)
library(png)
library(patchwork)
library(dtplyr)
library(tidyverse)
library(textclean)


na_strings <- c("NA", "N/A", "na", "n/a", "NULL", "null", "None", "none", "NaN", "nan", "Inf", "-Inf", "inf", "-inf", "", " ")
names_with_indices <- function(data) {
  names_data <- names(data)
  indices <- seq_along(names_data)
  names_indexed <- paste(indices, names_data, sep = ": ")
  return(names_indexed)
}
# Function to fix the header and write a new CSV file
fix_csv_header_id <- function(file_path) {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append ',id' to the first line
  corrected_first_line <- paste(first_line, "id", sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), "_fixed.csv")
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

# Function to fix the header and write a new CSV file with additional columns
fix_csv_header_cols <- function(file_path, additional_columns, suffix = "_fixed.csv") {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append additional columns to the first line
  corrected_first_line <- paste(first_line, additional_columns, sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), suffix)
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

#path_to_files <- "C:\\DataTest\\"
#path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\expLogs\\"
path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\"
options("digits.secs"=6)
```



Test to fix missing column headers using function with parameters.

```{r}
path_to_files <- "C:\\DataTest\\"
files_fixed <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = ".*[A-Z]{3}-\\d{4}-\\d{1,2} \\d{1,2}-\\d{1,2} Rating\\.csv$") %>%
  map(.f = function(file) {
    #cols_to_add <- "id"
    cols_to_add <- "id,rating,bwap,round_id,subject_id,gaze_condition,task_condition"
    suffix <- "_test.csv"
    #suffix <- "_fixed.csv"
    new_file <- fix_csv_header_cols(file, cols_to_add, suffix)
  })
files_fixed
path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\expLogs\\"
```


Fix the missing column headers
```{r}
files_fixed <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = ".*[A-Z]{3}-\\d{4}-\\d{1,2} \\d{1,2}-\\d{1,2} Rating\\.csv$") %>%
  map(.f = function(file) {
    cols_to_add <- "id"
    #cols_to_add <- "id,rating,bwap,round_id,subject_id,gaze_condition,task_condition"
    #suffix <- "_test.csv"
    suffix <- "_fixed.csv"
    new_file <- fix_csv_header_cols(file, cols_to_add, suffix)
  })
files_fixed
```



Find the ones that are still in the older format.
```{r}
subject_ratings <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3} rating_fixed\\.csv$") %>%
  map_dfr(.f = function(subjects_file) {
    print(subjects_file)
    subject_row <- read_csv(subjects_file, na = na_strings, show_col_types = FALSE) %>% clean_names()
    subject_row
  }) 
path_to_files
subject_ratings

subject_ratings <- subject_ratings %>% filter(round_id != 9) %>% mutate(subject_id = substr(subject_id, 1, 7))

# Ensure subject_ratings is a data frame
subject_ratings <- as.data.frame(subject_ratings)


test <- subject_ratings %>% filter(question == "How effective was your guidance in improving the student's focus?")
test


# Add a new column 'renamed_question' and fill it based on conditions
test2 <- subject_ratings %>%
  mutate(renamed_question = question) %>%  # Initialize with existing questions
  mutate(renamed_question = ifelse(grepl("Question - How confident were you in correctly guessing", question), "Confident", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How easy was it to guide", question), "Ease", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How well were you able to predict", question), "Predict", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your understanding", question), "Focus", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your awareness", question), "Awareness", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How complex were your instructions when guiding", question), "Complex", renamed_question))


test3 <- test2 %>% group_by(subject_id, question) %>% summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")

test4 <- test3 %>% filter(question == "Question - How confident did you feel")
test4

```



```{r}

subject_ratings 


aggregated_ratings <- subject_ratings %>%
  mutate(renamed_question = question) %>%  # Initialize with existing questions
  mutate(renamed_question = ifelse(grepl("Question - How confident were you in correctly guessing", question), "Confident", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How easy was it to guide", question), "Ease", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How well were you able to predict", question), "Predict", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your understanding", question), "Focus", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your awareness", question), "Awareness", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How complex were your instructions when guiding", question), "Complex", renamed_question)) %>%
  group_by(subject_id, renamed_question, gaze_condition) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")
  
  aggregated_ratings
  # Split the data frame by question
  questions_list <- split(aggregated_ratings, aggregated_ratings$renamed_question)

  # Function to create and save a boxplot and scatter plot for each question
  plot_question <- function(data, question_name) {
    p <- ggplot(data, aes(x = gaze_condition, y = mean_rating, fill = gaze_condition)) +
      geom_boxplot(alpha = 0.5) +  # Set transparency to see scatter points clearly
      geom_point(position = position_jitter(width = 0.1), color = "black", size = 3, alpha = 0.6) +  # Add jitter to avoid overplotting
      labs(title = question_name, x = "Gaze Condition", y = "Mean Rating") +
      theme_classic() +
      scale_y_continuous(limits = c(1, 5), oob = scales::oob_squish)  # Force all data into the specified range

    # Save the plot
    ggsave(paste0("Boxplot_Scatter_", question_name, ".png"), plot = p, width = 10, height = 8, dpi = 300)
  }

```


```{r}
# Reshape data from long to wide format for each question type and gaze condition
wide_ratings <- aggregated_ratings %>%
  pivot_wider(names_from = c(renamed_question, gaze_condition), 
              values_from = mean_rating,
              names_sep = "_")

# Function to perform Wilcoxon signed-rank test for each question type
perform_wilcox_test <- function(data, question_name) {
  data_on <- data[[paste0(question_name, "_On")]]
  data_off <- data[[paste0(question_name, "_Off")]]
  test_result <- wilcox.test(data_on, data_off, paired = TRUE)
  return(test_result)
}

# List of all question types
question_types <- unique(aggregated_ratings$renamed_question)


wide_ratings
# Pivot data for plotting
wide_ratings_for_plots <- aggregated_ratings %>%
  pivot_wider(names_from = renamed_question, values_from = mean_rating)
  
wide_ratings_for_plots <- wide_ratings_for_plots %>% mutate(gaze_condition = factor(gaze_condition, levels = c("Off", "On")))
  
  #Print out that this is results
  print("Results")
  
  awareness_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Awareness ~ gaze_condition, paired=T) 
  awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condition")
  awareness_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How aware were you of the student's attention?", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Confident
confident_wilcox <- wide_ratings_for_plots |> wilcox_test( Confident ~ gaze_condition, paired=T)
confident_wilcox.p <- confident_wilcox |> add_x_position(x="gaze_condiition")
confident_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Confident)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How confident did you feel when guiding or correcting the student?", x="", y="") +
  stat_pvalue_manual(confident_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Ease

ease_wilcox <- wide_ratings_for_plots |> wilcox_test( Ease ~ gaze_condition, paired=T)
ease_wilcox.p <- ease_wilcox |> add_x_position(x="gaze_condiition")
ease_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Ease)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How easy was it to guide and correct the student?", x="", y="") +
  stat_pvalue_manual(ease_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Focus

focus_wilcox <- wide_ratings_for_plots |> wilcox_test( Focus ~ gaze_condition, paired=T)
focus_wilcox.p <- focus_wilcox |> add_x_position(x="gaze_condiition")
focus_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Focus)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How well did you understand the student's focus?", x="", y="") +
  stat_pvalue_manual(focus_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))



test_result_awareness <- perform_wilcox_test(wide_ratings, "Complex")
test_result_awareness

test_result_confidence <- perform_wilcox_test(wide_ratings, "Confident")
test_result_confidence

test_result_ease <- perform_wilcox_test(wide_ratings, "Ease")
test_result_ease

test_result_focus <- perform_wilcox_test(wide_ratings, "Focus")
test_result_focus

test_result_predict <- perform_wilcox_test(wide_ratings, "Predict")
test_result_predict

awareness_wilcox <- wide_ratings_for_plots |> wilcox_test( Awareness ~ gaze_condition, paired=T) 
awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condiition")
awareness_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="Awareness", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

# Plot w

# Apply the Wilcoxon test to each question type
test_results <- lapply(question_types, function(q) {
  # Perform the test
  test_result <- perform_wilcox_test(wide_ratings, q)
  list(question = q, wilcox_test_result = test_result)
})

# Output the test results
test_results
question_types
wide_ratings
#write_csv(wide_ratings, paste0(path_to_files, "wide_ratings_test.csv"))
```



```{r}

# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(previous_time = lag(min_unity_log_time),
         round_duration = if_else(is.na(previous_time), min_unity_log_time, min_unity_log_time - previous_time))

round_durations

mean_durations <- round_durations %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE))

mean_durations

# Step 4: Reshape data from long to wide format
wide_mean_durations <- mean_durations %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = mean_duration,
              names_prefix = "mean_duration_")

wide_mean_durations


# Assuming wide_mean_durations is already loaded with your data
# Ensure it's in the correct format
wide_mean_durations <- wide_mean_durations %>%
  pivot_longer(cols = starts_with("mean_duration"), names_to = "gaze_condition", values_to = "mean_duration") %>%
  mutate(gaze_condition = sub("mean_duration_", "", gaze_condition))

# Make it a data frame
wide_mean_durations <- wide_mean_durations %>% as.data.frame()
wide_mean_durations
# Perform Wilcoxon signed-rank test using rstatix
duration_wilcox <- wide_mean_durations %>%
  wilcox_test(mean_duration ~ gaze_condition, paired = TRUE)
duration_wilcox.p <- duration_wilcox |> add_x_position(x="gaze_condition")
duration_wilcox 

```



```{r}

# Perform Wilcoxon signed-rank test
wilcox_test_result <- wilcox.test(
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "Off"],
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "On"],
  paired = TRUE
)

# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 350, 
  label = paste("p-value:", formatC(wilcox_test_result$p.value, format = "e", digits = 2))
)

# Plotting the results
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 50)) +
  labs(title = "Comparison of Mean Round (3 Tasks) Durations by Gaze Condition", x = "Gaze Condition", y = "Mean Duration") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```


Creates the round times for each subject, we'll use this a lot.

```{r}
# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            max_unity_log_time = max(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(round_start = lag(max_unity_log_time),
         round_end = min_unity_log_time)

# Compute the times for all the rounds and set the start time for the first round to 0 (as it starts logging on the first task)
rounds_with_times <- setDT(round_durations) %>% 
  .[is.na(round_start), round_start:= 0] %>% 
  .[,min_unity_log_time:=NULL] %>% 
  .[,max_unity_log_time:=NULL] %>% as.data.frame()
rounds_with_times
```

```{r}

```



```{r}
# Now we want to compute end_time for timings.csv files that do not have that column
# Define the path to the root directory containing the subdirectories

find_end_time <- function(start_time, round, task, timings, segments) {
  print(paste("Processing task:", task, "in round:", round, "with start time:", start_time))
  
  # Find the start time of the next task in the same round
  next_task_start_times <- timings %>%
    filter(round == round, task == task, start > start_time) %>%
    arrange(task) %>%
    pull(start)

  print("Next task start times:")
  print(next_task_start_times)

  # Determine the next task start time or use a large number if it's the last task
  next_task_start_time <- if (length(next_task_start_times) > 0) next_task_start_times[1] else Inf

  print("Next task start time:")
  print(next_task_start_time)

  # Find the last segment that starts before the next task starts and ends after the current task's start time
  valid_segments <- segments %>%
    filter(start >= start_time, start < next_task_start_time)

  print("Valid segments:")
  print(valid_segments)

  if (nrow(valid_segments) > 0) {
    # Find the maximum end time among segments that start before the next task
    end_time <- max(valid_segments$end)
  } else {
    # If no valid segments, use the maximum end time of any segment that starts after this task's start time
    end_time <- max(segments %>% filter(start >= start_time) %>% pull(end), na.rm = TRUE)
  }

  print("Calculated end time:")
  print(end_time)

  return(end_time)
}

# Function to process each directory
process_directory <- function(directory_path) {
  segments_path <- file.path(directory_path, "segments.csv")
  timings_path <- file.path(directory_path, "timings.csv")
  
  if (file.exists(segments_path) && file.exists(timings_path)) {
    segments <- read_csv(segments_path)
    timings <- read_csv(timings_path)

    if (!"end" %in% names(timings)) {
      print("Processing timings file without end time for directory:")
      print(directory_path)
      
      # Calculate end times for each row in timings
      timings$end <- mapply(function(start, round, task) {
        find_end_time(start, round, task, timings, segments)
      }, timings$start, timings$round, timings$task)

      # Save the updated timings file in the same directory
      print("Writing updated timings file...")
      print(file.path(directory_path, "timings_updated.csv"))
      
      write_csv(timings, file.path(directory_path, "timings_updated.csv"))
    }
  }
}
# List all directories under the root path and process each one
directories <- fs::dir_ls(path_to_files, recurse = TRUE, type = "directory")
walk(directories, process_directory)

```

```{r}
# Function to process each directory
library(tidyverse)
library(stringr)

process_directory <- function(directory_path) {
  timings_path <- file.path(directory_path, "timings_updated.csv")
  words_path <- file.path(directory_path, "words.csv")
  participant_data_path <- file.path(directory_path, "participantData.csv")
  print(directory_path)
  
  if (file.exists(timings_path) && file.exists(words_path)) {
    timings <- read_csv(timings_path)
    words <- read_csv(words_path)
    participant_data <- read_csv(participant_data_path)
    print(timings_path)
    timings
    words
    participant_data

    # Extract subject_id
    subject_id <- participant_data$subject_id

    
    # Clean and standardize words
    words <- words %>%
      mutate(word = tolower(word),  # Convert to lower case
             word = str_trim(word),  # Trim whitespace
             word = replace_contraction(word),  # Standardize contractions
             word = str_replace_all(word, "[[:punct:]]", ""))  # Remove punctuation using stringr

    words
    print("Words data after cleaning:")
    print(head(words))

    # Merge words data with timings based on overlapping intervals
    words_timings <- words %>%
      mutate(across(c(start, end), as.numeric)) %>%
      full_join(timings, by = character()) %>%
      rename(start_word = start.x, end_word = end.x, start_timing = start.y, end_timing = end.y)

    print("Data after merging and renaming:")
    print(head(words_timings))

    # Apply filter to match words within the timings intervals
    words_timings <- words_timings %>%
      filter(start_word >= start_timing & end_word <= end_timing)

    print("Data after filtering:")
    print(head(words_timings))

    # Count words for each round and task
    word_counts <- words_timings %>%
      group_by(round, task, word) %>%
      summarise(count = n(), .groups = 'drop')
      arrange(desc(count))
      
    # Add subject_id to each row
    word_counts <- word_counts %>%
      mutate(subject_id = subject_id)
    print("Aggregated word counts:")
    print(head(word_counts))
    
    # Optionally, save the result in the same directory
    write_csv(word_counts, file.path(directory_path, "word_counts.csv"))
    
    return(word_counts)
  } else {
    warning("Timings or words file missing in ", directory_path)
  }
}

# List all directories under the root path and process each one
directories <- dir_ls(path_to_files, recurse = TRUE, type = "directory")
results <- map(directories, process_directory)

# Combine all results into a single data frame
#final_results <- bind_rows(results)

```

