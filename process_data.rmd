
Package installation
```{r}
install.packages("dplyr")
install.packages("tidyr")
install.packages("readr")
install.packages("pwr")
install.packages("fs")
install.packages("purrr")
install.packages("data.table")
install.packages("janitor")
install.packages("magrittr")
install.packages("tidyverse")
install.packages("stringr")
install.packages("ggpubr")
install.packages("ggplot2")
install.packages("rstatix")
install.packages("dtplyr")
install.packages("tidyverse")
install.packages("textclean")
```

Initialize commonly used libraries, variables, and functions.
```{r}

library(tidyr)
library(fs)
library(dplyr)
library(readr)
library(purrr)
library(data.table)
library(pwr)
library(janitor)
library(magrittr) 
library(tidyverse)
library(stringr)
library(ggpubr)
library(ggplot2)
library(rstatix)
library(qqplotr)
library(here)
library(tidyverse)
library(qqplotr)
library(rstatix)
library(broom)
library(knitr)
library(ggpubr)
library(gtools)
library(GGally)
library(correlation)
library(png)
library(patchwork)
library(dtplyr)
library(tidyverse)
library(textclean)
# Function to process each directory
library(tidyverse)
library(stringr)

na_strings <- c("NA", "N/A", "na", "n/a", "NULL", "null", "None", "none", "NaN", "nan", "Inf", "-Inf", "inf", "-inf", "", " ")
names_with_indices <- function(data) {
  names_data <- names(data)
  indices <- seq_along(names_data)
  names_indexed <- paste(indices, names_data, sep = ": ")
  return(names_indexed)
}
# Function to fix the header and write a new CSV file
fix_csv_header_id <- function(file_path) {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append ',id' to the first line
  corrected_first_line <- paste(first_line, "id", sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), "_fixed.csv")
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

# Function to fix the header and write a new CSV file with additional columns
fix_csv_header_cols <- function(file_path, additional_columns, suffix = "_fixed.csv") {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append additional columns to the first line
  corrected_first_line <- paste(first_line, additional_columns, sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), suffix)
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

#path_to_files <- "C:\\DataTest\\"
#path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\expLogs\\"
path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\"
options("digits.secs"=6)
```



Test to fix missing column headers using function with parameters.

```{r}
path_to_files <- "C:\\DataTest\\"
files_fixed <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = ".*[A-Z]{3}-\\d{4}-\\d{1,2} \\d{1,2}-\\d{1,2} Rating\\.csv$") %>%
  map(.f = function(file) {
    #cols_to_add <- "id"
    cols_to_add <- "id,rating,bwap,round_id,subject_id,gaze_condition,task_condition"
    suffix <- "_test.csv"
    #suffix <- "_fixed.csv"
    new_file <- fix_csv_header_cols(file, cols_to_add, suffix)
  })
files_fixed
path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\expLogs\\"
```

Test to fix column headers in annotation files.
```{r}
test_file <- "C:\\Users\\Iterated Miku System\\Documents\\communication-analysis"
files_fixed <- fs::dir_ls(path = test_file, recurse = TRUE, regexp = "annotation_test\\.csv$") %>% 
  map(.f = function(file) {
    cols_to_add <- "name,head_angle,eye_angle,head_angle_using_pos,eye_angle_using_pos,distance"
    suffix <- "_fixed.csv"
    new_file <- fix_csv_header_cols(file, cols_to_add, suffix)
  })
files_fixed
```

Now fix the column headers in the annotation files.
```{r}
files_fixed <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "annotation_test\\.csv$") %>% 
  map(.f = function(file) {
    cols_to_add <- "name,head_angle,eye_angle,head_angle_using_pos,eye_angle_using_pos,distance"
    suffix <- "_fixed.csv"
    new_file <- fix_csv_header_cols(file, cols_to_add, suffix)
  })
```


Fix the missing column headers in the ratings files.
```{r}
files_fixed <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_annotation\\.csv$") %>%
  map(.f = function(file) {
    print("Processing file:")
    print(file)
    cols_to_add <- "name,head_angle,eye_angle,head_angle_using_pos,eye_angle_using_pos,distance"
    suffix <- "_fixed.csv"
    new_file <- fix_csv_header_cols(file, cols_to_add, suffix)
  })
files_fixed
```




Find the ones that are still in the older format.
```{r}
subject_ratings <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3} rating_fixed\\.csv$") %>%
  map_dfr(.f = function(subjects_file) {
    print(subjects_file)
    subject_row <- read_csv(subjects_file, na = na_strings, show_col_types = FALSE) %>% clean_names()
    subject_row
  }) 
path_to_files
subject_ratings

subject_ratings <- subject_ratings %>% filter(round_id != 9) %>% mutate(subject_id = substr(subject_id, 1, 7))

# Ensure subject_ratings is a data frame
subject_ratings <- as.data.frame(subject_ratings)


test <- subject_ratings %>% filter(question == "How effective was your guidance in improving the student's focus?")
test


# Add a new column 'renamed_question' and fill it based on conditions
test2 <- subject_ratings %>%
  mutate(renamed_question = question) %>%  # Initialize with existing questions
  mutate(renamed_question = ifelse(grepl("Question - How confident were you in correctly guessing", question), "Confident", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How easy was it to guide", question), "Ease", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How well were you able to predict", question), "Predict", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your understanding", question), "Focus", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your awareness", question), "Awareness", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How complex were your instructions when guiding", question), "Complex", renamed_question))


test3 <- test2 %>% group_by(subject_id, question) %>% summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")

test4 <- test3 %>% filter(question == "Question - How confident did you feel")
test4

```



Creates the round times for each subject, we'll use this a lot.

```{r}
subject_ratings
# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
# We also need task_condition and gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            max_unity_log_time = max(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition),
            task_condition = first(task_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(round_start = lag(max_unity_log_time),
         round_end = min_unity_log_time)

# Compute the times for all the rounds and set the start time for the first round to 0 (as it starts logging on the first task)
rounds_with_times <- setDT(round_durations) %>% 
  .[is.na(round_start), round_start:= 0] %>% 
  .[,min_unity_log_time:=NULL] %>% 
  .[,max_unity_log_time:=NULL] %>% as.data.frame()


# Add a sequential task_id for each entry, incrementing by 3 for each round
# First, expand each round into three tasks
rounds_expanded <- rounds_with_times %>%
  group_by(subject_id, round_id) %>%
  slice(rep(1:n(), each = 3)) %>%
  ungroup()

# Now, assign task_id incrementally across rounds
rounds_expanded <- rounds_expanded %>%
  group_by(subject_id) %>%
  mutate(task_id = row_number()) %>%
  ungroup() %>%
  relocate(task_id, .after = round_id)

# Adjust task_condition based on task_id being the first in each round
rounds_expanded <- rounds_expanded %>%
  group_by(subject_id, round_id) %>%
  mutate(task_condition = if_else(task_id %% 3 == 1, "Incorrect", task_condition)) %>%
  ungroup()

# Adjust task_condition for the second task in each round based on the third task's condition
subject_rounds <- rounds_expanded %>%
  group_by(subject_id, round_id) %>%
  mutate(task_condition = if_else(task_id %% 3 == 2, 
                                  if_else(lead(task_condition, default = "Correct") == "Correct", "Incorrect", "Correct"), 
                                  task_condition)) %>%
  ungroup()

subject_rounds
```




```{r}

subject_ratings 


aggregated_ratings <- subject_ratings %>%
  mutate(renamed_question = question) %>%  # Initialize with existing questions
  mutate(renamed_question = ifelse(grepl("Question - How confident were you in correctly guessing", question), "Confident", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How easy was it to guide", question), "Ease", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How well were you able to predict", question), "Predict", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your understanding", question), "Focus", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your awareness", question), "Awareness", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How complex were your instructions when guiding", question), "Complex", renamed_question)) %>%
  group_by(subject_id, renamed_question, gaze_condition) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")
  
  aggregated_ratings
  # Split the data frame by question
  questions_list <- split(aggregated_ratings, aggregated_ratings$renamed_question)

  # Function to create and save a boxplot and scatter plot for each question
  plot_question <- function(data, question_name) {
    p <- ggplot(data, aes(x = gaze_condition, y = mean_rating, fill = gaze_condition)) +
      geom_boxplot(alpha = 0.5) +  # Set transparency to see scatter points clearly
      geom_point(position = position_jitter(width = 0.1), color = "black", size = 3, alpha = 0.6) +  # Add jitter to avoid overplotting
      labs(title = question_name, x = "Gaze Condition", y = "Mean Rating") +
      theme_classic() +
      scale_y_continuous(limits = c(1, 5), oob = scales::oob_squish)  # Force all data into the specified range

    # Save the plot
    ggsave(paste0("Boxplot_Scatter_", question_name, ".png"), plot = p, width = 10, height = 8, dpi = 300)
  }

```


```{r}
# Reshape data from long to wide format for each question type and gaze condition
wide_ratings <- aggregated_ratings %>%
  pivot_wider(names_from = c(renamed_question, gaze_condition), 
              values_from = mean_rating,
              names_sep = "_")

# Function to perform Wilcoxon signed-rank test for each question type
perform_wilcox_test <- function(data, question_name) {
  data_on <- data[[paste0(question_name, "_On")]]
  data_off <- data[[paste0(question_name, "_Off")]]
  test_result <- wilcox.test(data_on, data_off, paired = TRUE)
  return(test_result)
}

# List of all question types
question_types <- unique(aggregated_ratings$renamed_question)


wide_ratings
# Pivot data for plotting
wide_ratings_for_plots <- aggregated_ratings %>%
  pivot_wider(names_from = renamed_question, values_from = mean_rating)
  
wide_ratings_for_plots <- wide_ratings_for_plots %>% mutate(gaze_condition = factor(gaze_condition, levels = c("Off", "On")))
  
  #Print out that this is results
  print("Results")
  
  awareness_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Awareness ~ gaze_condition, paired=T) 
  awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condition")
  awareness_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How aware were you of the student's attention?", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Confident
confident_wilcox <- wide_ratings_for_plots |> wilcox_test( Confident ~ gaze_condition, paired=T)
confident_wilcox.p <- confident_wilcox |> add_x_position(x="gaze_condiition")
confident_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Confident)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How confident did you feel when guiding or correcting the student?", x="", y="") +
  stat_pvalue_manual(confident_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Ease

ease_wilcox <- wide_ratings_for_plots |> wilcox_test( Ease ~ gaze_condition, paired=T)
ease_wilcox.p <- ease_wilcox |> add_x_position(x="gaze_condiition")
ease_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Ease)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How easy was it to guide and correct the student?", x="", y="") +
  stat_pvalue_manual(ease_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Focus

focus_wilcox <- wide_ratings_for_plots |> wilcox_test( Focus ~ gaze_condition, paired=T)
focus_wilcox.p <- focus_wilcox |> add_x_position(x="gaze_condiition")
focus_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Focus)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How well did you understand the student's focus?", x="", y="") +
  stat_pvalue_manual(focus_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))



test_result_awareness <- perform_wilcox_test(wide_ratings, "Complex")
test_result_awareness

test_result_confidence <- perform_wilcox_test(wide_ratings, "Confident")
test_result_confidence

test_result_ease <- perform_wilcox_test(wide_ratings, "Ease")
test_result_ease

test_result_focus <- perform_wilcox_test(wide_ratings, "Focus")
test_result_focus

test_result_predict <- perform_wilcox_test(wide_ratings, "Predict")
test_result_predict

awareness_wilcox <- wide_ratings_for_plots |> wilcox_test( Awareness ~ gaze_condition, paired=T) 
awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condiition")
awareness_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="Awareness", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

# Plot w

# Apply the Wilcoxon test to each question type
test_results <- lapply(question_types, function(q) {
  # Perform the test
  test_result <- perform_wilcox_test(wide_ratings, q)
  list(question = q, wilcox_test_result = test_result)
})

# Output the test results
test_results
question_types
wide_ratings
#write_csv(wide_ratings, paste0(path_to_files, "wide_ratings_test.csv"))
```



```{r}

# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(previous_time = lag(min_unity_log_time),
         round_duration = if_else(is.na(previous_time), min_unity_log_time, min_unity_log_time - previous_time))

round_durations

mean_durations <- round_durations %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE))

mean_durations

# Step 4: Reshape data from long to wide format
wide_mean_durations <- mean_durations %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = mean_duration,
              names_prefix = "mean_duration_")

wide_mean_durations


# Assuming wide_mean_durations is already loaded with your data
# Ensure it's in the correct format
wide_mean_durations <- wide_mean_durations %>%
  pivot_longer(cols = starts_with("mean_duration"), names_to = "gaze_condition", values_to = "mean_duration") %>%
  mutate(gaze_condition = sub("mean_duration_", "", gaze_condition))

# Make it a data frame
wide_mean_durations <- wide_mean_durations %>% as.data.frame()
wide_mean_durations
# Perform Wilcoxon signed-rank test using rstatix
duration_wilcox <- wide_mean_durations %>%
  wilcox_test(mean_duration ~ gaze_condition, paired = TRUE)
duration_wilcox.p <- duration_wilcox |> add_x_position(x="gaze_condition")
duration_wilcox 

```



```{r}

# Perform Wilcoxon signed-rank test
wilcox_test_result <- wilcox.test(
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "Off"],
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "On"],
  paired = TRUE
)

# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 350, 
  label = paste("p-value:", formatC(wilcox_test_result$p.value, format = "e", digits = 2))
)

# Plotting the results
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 50)) +
  labs(title = "Comparison of Mean Round (3 Tasks) Durations by Gaze Condition", x = "Gaze Condition", y = "Mean Duration") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```





```{r}
# Now we want to compute end_time for timings.csv files that do not have that column
# Define the path to the root directory containing the subdirectories

find_end_time <- function(start_time, round, task, timings, segments) {
  print(paste("Processing task:", task, "in round:", round, "with start time:", start_time))
  
  # Find the start time of the next task in the same round
  next_task_start_times <- timings %>%
    filter(round == round, task == task, start > start_time) %>%
    arrange(task) %>%
    pull(start)

  print("Next task start times:")
  print(next_task_start_times)

  # Determine the next task start time or use a large number if it's the last task
  next_task_start_time <- if (length(next_task_start_times) > 0) next_task_start_times[1] else Inf

  print("Next task start time:")
  print(next_task_start_time)

  # Find the last segment that starts before the next task starts and ends after the current task's start time
  valid_segments <- segments %>%
    filter(start >= start_time, start < next_task_start_time)

  print("Valid segments:")
  print(valid_segments)

  if (nrow(valid_segments) > 0) {
    # Find the maximum end time among segments that start before the next task
    end_time <- max(valid_segments$end)
  } else {
    # If no valid segments, use the maximum end time of any segment that starts after this task's start time
    end_time <- max(segments %>% filter(start >= start_time) %>% pull(end), na.rm = TRUE)
  }

  print("Calculated end time:")
  print(end_time)

  return(end_time)
}

# Function to process each directory
process_directory <- function(directory_path) {
  segments_path <- file.path(directory_path, "segments.csv")
  timings_path <- file.path(directory_path, "timings.csv")
  
  if (file.exists(segments_path) && file.exists(timings_path)) {
    segments <- read_csv(segments_path)
    timings <- read_csv(timings_path)

    if (!"end" %in% names(timings)) {
      print("Processing timings file without end time for directory:")
      print(directory_path)
      
      # Calculate end times for each row in timings
      timings$end <- mapply(function(start, round, task) {
        find_end_time(start, round, task, timings, segments)
      }, timings$start, timings$round, timings$task)

      # Save the updated timings file in the same directory
      print("Writing updated timings file...")
      print(file.path(directory_path, "timings_updated.csv"))
      
      write_csv(timings, file.path(directory_path, "timings_updated.csv"))
    }
  }
}
# List all directories under the root path and process each one
directories <- fs::dir_ls(path_to_files, recurse = TRUE, type = "directory")
walk(directories, process_directory)

```


```{r}
directory_path <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\DIB-019"
  timings_path <- file.path(directory_path, "timings_updated.csv")
  words_path <- file.path(directory_path, "words.csv")
  participant_data_path <- file.path(directory_path, "participantData.csv")
  print(directory_path)
  
    timings <- read_csv(timings_path, na = na_strings, show_col_types = FALSE)
    words <- read_csv(words_path, na = na_strings, show_col_types = FALSE)
    participant_data <- read_csv(participant_data_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
    print(timings_path)
    timings
    words
    participant_data

    # Extract subject_id
    subject_id <- participant_data$subject_id

    
    # Clean and standardize words
    words <- words %>%
      mutate(word = tolower(word),  # Convert to lower case
             word = str_trim(word),  # Trim whitespace
             word = replace_contraction(word),  # Standardize contractions
             word = str_replace_all(word, "[[:punct:]]", ""))  # Remove punctuation using stringr

    words
    print("Words data after cleaning:")
    print(head(words))

    # Merge words data with timings based on overlapping intervals
    words_timings <- words %>%
      mutate(across(c(start, end), as.numeric)) %>%
      full_join(timings, by = character()) %>%
      rename(start_word = start.x, end_word = end.x, start_timing = start.y, end_timing = end.y)

    print("Data after merging and renaming:")
    print(head(words_timings))

    # Apply filter to match words within the timings intervals
    words_timings <- words_timings %>%
      filter(start_word >= start_timing & end_word <= end_timing)

    print("Data after filtering:")
    print(head(words_timings))

    # Count words for each round and task
    word_counts <- words_timings %>%
      group_by(round, task, word) %>%
      summarise(count = n(), .groups = 'drop')
      
    # Add subject_id to each row
    word_counts <- word_counts %>%
      mutate(subject_id = subject_id) %>%
      rename(round_id = round, task_id = task) %>%
      relocate(subject_id, .before = round_id) %>%
      arrange(round_id, task_id, desc(count))


    print("Aggregated word counts:")
    print(head(word_counts))


    subject_rounds <- subject_rounds %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    word_counts <- word_counts %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    

    # Perform the join
    word_counts <- inner_join(subject_rounds, word_counts, by = c("subject_id", "round_id", "task_id"))
    word_counts
    # Optionally, save the result in the same directory
    write_csv(word_counts, file.path(directory_path, "word_counts.csv"))


```


```{r}


process_directory <- function(directory_path) {
  timings_path <- file.path(directory_path, "timings_updated.csv")
  words_path <- file.path(directory_path, "words.csv")
  participant_data_path <- file.path(directory_path, "participantData.csv")
  print(directory_path)
  
  if (file.exists(timings_path) && file.exists(words_path)) {
    timings <- read_csv(timings_path, na = na_strings, show_col_types = FALSE)
    words <- read_csv(words_path, na = na_strings, show_col_types = FALSE)
    participant_data <- read_csv(participant_data_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
    print(timings_path)
    timings
    words
    participant_data

    # Extract subject_id
    subject_id <- participant_data$subject_id

    
    # Clean and standardize words
    words <- words %>%
      mutate(word = tolower(word),  # Convert to lower case
             word = str_trim(word),  # Trim whitespace
             word = replace_contraction(word),  # Standardize contractions
             word = str_replace_all(word, "[[:punct:]]", ""))  # Remove punctuation using stringr

    words
    print("Words data after cleaning:")
    print(head(words))

    # Merge words data with timings based on overlapping intervals
    words_timings <- words %>%
      mutate(across(c(start, end), as.numeric)) %>%
      full_join(timings, by = character()) %>%
      rename(start_word = start.x, end_word = end.x, start_timing = start.y, end_timing = end.y)

    print("Data after merging and renaming:")
    print(head(words_timings))

    # Apply filter to match words within the timings intervals
    words_timings <- words_timings %>%
      filter(start_word >= start_timing & end_word <= end_timing)

    print("Data after filtering:")
    print(head(words_timings))

    # Count words for each round and task
    word_counts <- words_timings %>%
      group_by(round, task, word) %>%
      summarise(count = n(), .groups = 'drop')
      
    # Add subject_id to each row
    word_counts <- word_counts %>%
      mutate(subject_id = subject_id) %>%
      rename(round_id = round, task_id = task) %>%
      relocate(subject_id, .before = round_id) %>%
      arrange(round_id, task_id, desc(count))


    print("Aggregated word counts:")
    print(head(word_counts))


    subject_rounds <- subject_rounds %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    word_counts <- word_counts %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    

    # Perform the join
    word_counts <- inner_join(subject_rounds, word_counts, by = c("subject_id", "round_id", "task_id"))
    word_counts
    # Optionally, save the result in the same directory
    write_csv(word_counts, file.path(directory_path, "word_counts.csv"))
    
    return(word_counts)
  } else {
    warning("Timings or words file missing in ", directory_path)
  }
}

# List all directories under the root path and process each one
directories <- dir_ls(path_to_files, recurse = TRUE, type = "directory")
results <- map(directories, process_directory)

# Combine all results into a single data frame
#final_results <- bind_rows(results)

```

TODO: Plot the word counts for each condition and round.

Start off by figuring out % of time they spent looking at the terrain in each condition/task.

First we do a test with one file

```{r}
#subject_rounds
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
terrain_path <- paste0(path_to_files, "DIB-019\\DIB-019_terrain_fixed.csv")
terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))
terrain_dt <- terrain %>% setDT() %>%
  .[round_id != 0]

#terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
terrain_dt

# Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
terrain_dt <- terrain_dt %>% 
  mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
  filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
  select(-task_id_mod3)  # Remove the helper column

terrain_dt_grouped <- terrain_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_syncs = n(),  # Total number of sync_id entries
    valid_hits = sum(hit_valid, na.rm = TRUE),  # Count of TRUE in hit_valid
    percent_looking_at_terrain = (valid_hits / total_syncs) * 100  # Calculate percentage
  ) %>%
  ungroup()

terrain_dt_grouped
# Calculate the average percentage of time looking at the terrain for each gaze condition
average_percent_by_condition <- terrain_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_percent_looking_at_terrain = mean(percent_looking_at_terrain, na.rm = TRUE)
  ) %>% ungroup()

average_percent_by_condition

```


Mean head_angle_using_dir and eye_angle_using_dir for each condition

```{r}
# Calculate the mean head_angle_using_dir and eye_angle_using_dir for each condition
terrain_dt
# For head_angle_using_dir and eye_angle_using_dir, find the maximum value that is not -10000 and then use that instead of -10000

#First get the maximum for each and add a new column with those, PMAX DOES NOT WORK


max_head_angle <- terrain_dt %>% filter(head_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle = max(head_angle_using_dir, na.rm = TRUE)) %>% ungroup()

max_eye_angle <- terrain_dt %>% filter(eye_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_eye_angle = max(eye_angle_using_dir, na.rm = TRUE)) %>% ungroup()

max_distance <- terrain_dt %>% filter(distance != 10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_distance = max(distance, na.rm = TRUE)) %>% ungroup()

angles_terrain_dt <- merge(terrain_dt, max_head_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
angles_terrain_dt <- merge(angles_terrain_dt, max_eye_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
angles_terrain_dt <- merge(angles_terrain_dt, max_distance, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)

# Now find all -10000 values in eye_angle_using_dir and head_angle_using_dir and replace them with the max values

angles_terrain_dt <- angles_terrain_dt %>% 
  mutate(head_angle_using_dir = ifelse(head_angle_using_dir == -10000, max_head_angle, head_angle_using_dir),
         eye_angle_using_dir = ifelse(eye_angle_using_dir == -10000, max_eye_angle, eye_angle_using_dir),
         distance = ifelse(distance == 10000, max_distance, distance))

  
average_angles_by_condition <- angles_terrain_dt %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    mean_head_angle_using_dir = mean(head_angle_using_dir, na.rm = TRUE),
    mean_eye_angle_using_dir = mean(eye_angle_using_dir, na.rm = TRUE),
    mean_distance = mean(distance, na.rm = TRUE)
  ) %>% ungroup()

average_angles_by_condition


```


#TODO: Some other things to investigate - use head_angle_using_dir and eye_angle_using_dir for angles
Can check the distance fields to see how much movement their eyes did on the terrain
Mean head_angle_using_dir and eye_angle_using_dir for each condition
delta angles should be in angle per second, so we can calculate the average delta angle per second for each condition.


The all files version.

```{r}

# Process each terrain file and calculate the average percentage of time looking at the terrain
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_percent_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_terrain_fixed\\.csv$") %>%
  map_dfr(.f = function(terrain_path) {
    print(terrain_path)
    terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
          mutate(subject_id = substr(subject_id, 1, 7))
    terrain_dt <- terrain %>% setDT() %>%
      .[round_id != 0]

    #terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

    terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
        
    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    #terrain_dt <- terrain_dt %>% 
      #mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
      #filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      #select(-task_id_mod3)  # Remove the helper column    
    
    
    terrain_dt <- terrain_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_syncs = n(),  # Total number of sync_id entries
        valid_hits = sum(hit_valid, na.rm = TRUE),  # Count of TRUE in hit_valid
        percent_looking_at_terrain = (valid_hits / total_syncs) * 100  # Calculate percentage
      ) %>%
      ungroup()
    terrain_dt

    # Calculate the average percentage of time looking at the terrain for each gaze condition
    average_percent_by_condition <- terrain_dt %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_percent_looking_at_terrain = median(percent_looking_at_terrain, na.rm = TRUE)
      ) %>% ungroup()

    average_percent_by_condition
  })
average_percent_by_condition_all

# Process each terrain file and calculate the average head_angle_using_dir and eye_angle_using_dir for each condition
average_angles_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_terrain_fixed\\.csv$") %>%
  map_dfr(.f = function(terrain_path) {
    print(terrain_path)
    terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
          mutate(subject_id = substr(subject_id, 1, 7))
    terrain_dt <- terrain %>% setDT() %>%
      .[round_id != 0]

    #terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

    terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
    terrain_dt
    
    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    #terrain_dt <- terrain_dt %>% 
      #mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
      #filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      #select(-task_id_mod3)  # Remove the helper column
    
    # Calculate the mean head_angle_using_dir and eye_angle_using_dir for each condition
    max_head_angle <- terrain_dt %>% filter(head_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle = max(head_angle_using_dir, na.rm = TRUE)) %>% ungroup()
    
    max_eye_angle <- terrain_dt %>% filter(eye_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_eye_angle = max(eye_angle_using_dir, na.rm = TRUE)) %>% ungroup()

    max_distance <- terrain_dt %>% filter(distance != 10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_distance = max(distance, na.rm = TRUE)) %>% ungroup()
    
    angles_terrain_dt <- merge(terrain_dt, max_head_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_eye_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_distance, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    
    # Now find all -10000 values in eye_angle_using_dir and head_angle_using_dir and replace them with the max values
    
    angles_terrain_dt <- angles_terrain_dt %>% 
      mutate(head_angle_using_dir = ifelse(head_angle_using_dir == -10000, max_head_angle, head_angle_using_dir),
             eye_angle_using_dir = ifelse(eye_angle_using_dir == -10000, max_eye_angle, eye_angle_using_dir),
             distance = ifelse(distance == 10000, max_distance, distance))
    
      
    average_angles_by_condition <- angles_terrain_dt %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        mean_head_angle_using_dir = median(head_angle_using_dir, na.rm = TRUE),
        mean_eye_angle_using_dir = median(eye_angle_using_dir, na.rm = TRUE),
        mean_distance = median(distance, na.rm = TRUE)
      ) %>% ungroup()
    
    average_angles_by_condition
  })



```

Percentage for all conditions
```{r}
# Now pivot it wider
average_percent_by_condition_all <- average_percent_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_percent_looking_at_terrain,
              names_prefix = "average_percent_looking_at_terrain_") %>%
  as.data.frame()


average_percent_by_condition_all

#Now we can plot the results using box and whiskers

average_percent_by_condition_all %>%
  pivot_longer(cols = starts_with("average_percent_looking_at_terrain"), names_to = "gaze_condition", values_to = "average_percent_looking_at_terrain") %>%
  ggplot(aes(x = gaze_condition, y = average_percent_looking_at_terrain, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "Comparison of Average Percentage of Time Looking at Terrain by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Time Looking at Terrain") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```

```{r}
average_angles_by_condition_all

# Assuming average_angles_by_condition_all is a data frame
# Convert to data.table and reshape the data from long to wide format using dcast
average_angles_by_condition_all_dt <- average_angles_by_condition_all %>%
  data.table::as.data.table() %>%
  data.table::dcast(subject_id ~ gaze_condition, 
                    value.var = c("mean_head_angle_using_dir", "mean_eye_angle_using_dir", "mean_distance"),
                    fun.aggregate = mean)
names(average_angles_by_condition_all_dt)
average_angles_by_condition_all_dt

#Now plot each of the angles and distance
# Box and whiskers for head angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_head_angle_using_dir"), names_to = "gaze_condition", values_to = "mean_head_angle_using_dir") %>%
  ggplot(aes(x = gaze_condition, y = mean_head_angle_using_dir, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Head Angle Using Direction by Gaze Condition", x = "Gaze Condition", y = "Mean Head Angle Using Direction") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for eye angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_eye_angle_using_dir"), names_to = "gaze_condition", values_to = "mean_eye_angle_using_dir") %>%
  ggplot(aes(x = gaze_condition, y = mean_eye_angle_using_dir, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Eye Angle Using Direction by Gaze Condition", x = "Gaze Condition", y = "Mean Eye Angle Using Direction") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for distance
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_distance"), names_to = "gaze_condition", values_to = "mean_distance") %>%
  ggplot(aes(x = gaze_condition, y = mean_distance, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +
  labs(title = "Comparison of Mean Distance by Gaze Condition", x = "Gaze Condition", y = "Mean Distance") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))
```

TODO: We need to find the delta eye-angles per second, while eye_angle is always 0 we can instead base it off of eye_angle_using_dir, alternatively we can get it from the metrics.csv file.

