
Package installation
```{r}
install.packages("dplyr")
install.packages("tidyr")
install.packages("readr")
install.packages("pwr")
install.packages("fs")
install.packages("purrr")
install.packages("data.table")
install.packages("janitor")
install.packages("magrittr")
install.packages("tidyverse")
install.packages("stringr")
install.packages("ggpubr")
install.packages("ggplot2")
install.packages("rstatix")
install.packages("dtplyr")
install.packages("tidyverse")
install.packages("textclean")
install.packages("ARTool")
install.packages("emmeans")
```

Initialize commonly used libraries, variables, and functions.
```{r}

library(tidyr)
library(fs)
library(dplyr)
library(readr)
library(purrr)
library(data.table)
library(pwr)
library(janitor)
library(magrittr) 
library(tidyverse)
library(stringr)
library(ggpubr)
library(ggplot2)
library(rstatix)
library(qqplotr)
library(here)
library(tidyverse)
library(qqplotr)
library(rstatix)
library(broom)
library(knitr)
library(ggpubr)
library(gtools)
library(GGally)
library(correlation)
library(png)
library(patchwork)
library(dtplyr)
library(tidyverse)
library(textclean)
# Function to process each directory
library(tidyverse)
library(stringr)
library(patchwork)
library(coin)
library(rcompanion)
library(rstatix)
library(ARTool)
library(emmeans)

na_strings <- c("NA", "N/A", "na", "n/a", "NULL", "null", "None", "none", "NaN", "nan", "Inf", "-Inf", "inf", "-inf", "", " ")
names_with_indices <- function(data) {
  names_data <- names(data)
  indices <- seq_along(names_data)
  names_indexed <- paste(indices, names_data, sep = ": ")
  return(names_indexed)
}
# Function to fix the header and write a new CSV file
fix_csv_header_id <- function(file_path) {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append ',id' to the first line
  corrected_first_line <- paste(first_line, "id", sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), "_fixed.csv")
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

# Function to fix the header and write a new CSV file with additional columns
fix_csv_header_cols <- function(file_path, additional_columns, suffix = "_fixed.csv") {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append additional columns to the first line
  corrected_first_line <- paste(first_line, additional_columns, sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), suffix)
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

# Function to compute Z value and print Z, N, and p
compute_wilcox_summary <- function(wilcox_result) {
  # Calculate Z value using the normal approximation
  # Z = (W - mean(W)) / sd(W)
  # mean(W) = n1 * n2 / 2
  # sd(W) = sqrt(n1 * n2 * (n1 + n2 + 1) / 12)
  wilcox_result <- wilcox_result %>%
    mutate(
      mean_W = n1 * n2 / 2,
      sd_W = sqrt(n1 * n2 * (n1 + n2 + 1) / 12),
      Z = (statistic - mean_W) / sd_W,
      N = n1 + n2
    )
  
  # Print the results in a more readable format
  print(sprintf("Z = %.5f, N = %d, p = %.5f", wilcox_result$Z, wilcox_result$N, wilcox_result$p))
  
  
  # Return the modified result with Z value
  return(wilcox_result)
}

# Function to compute Z value for Wilcoxon signed-rank test results from rstatix
compute_wilcoxon_signed_rank_summary <- function(wilcox_result) {
  # Extract necessary components from the result
  n <- wilcox_result$n1  # Assuming n1 and n2 are the same because it's a paired test
  T <- wilcox_result$statistic
  
  # Calculate mean and standard error under H0
  mn = n * (n + 1) / 4
  se = sqrt(n * (n + 1) * (2 * n + 1) / 24)
  
  # Continuity correction
  correction = 0.5
  
  # Calculate Z value
  Z = (T - mn - correction) / se
  
  # Print formatted results
  print(sprintf("Z = %.4f, N = %d, p = %.5f", Z, n, wilcox_result$p))
  
  # Return a list with the computed values
  return(list(Z = Z, N = n, p = wilcox_result$p))
}


#path_to_files <- "C:\\DataTest\\"
#path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\expLogs\\"
path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\"
options("digits.secs"=6)
```



Creates the round times for each subject, we'll use this a lot.

```{r}
subject_ratings <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3} rating_fixed\\.csv$") %>%
  map_dfr(.f = function(subjects_file) {
    print(subjects_file)
    subject_row <- read_csv(subjects_file, na = na_strings, show_col_types = FALSE) %>% clean_names()
    subject_row
  }) 
path_to_files
subject_ratings

subject_ratings <- subject_ratings %>% filter(round_id != 9) %>% mutate(subject_id = substr(subject_id, 1, 7))

# Ensure subject_ratings is a data frame
subject_ratings <- as.data.frame(subject_ratings)

# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
# We also need task_condition and gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            max_unity_log_time = max(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition),
            task_condition = first(task_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(round_start = lag(max_unity_log_time),
         round_end = min_unity_log_time)

# Compute the times for all the rounds and set the start time for the first round to 0 (as it starts logging on the first task)
rounds_with_times <- setDT(round_durations) %>% 
  .[is.na(round_start), round_start:= 0] %>% 
  .[,min_unity_log_time:=NULL] %>% 
  .[,max_unity_log_time:=NULL] %>% as.data.frame()


# Add a sequential task_id for each entry, incrementing by 3 for each round
# First, expand each round into three tasks
rounds_expanded <- rounds_with_times %>%
  group_by(subject_id, round_id) %>%
  slice(rep(1:n(), each = 3)) %>%
  ungroup()

# Now, assign task_id incrementally across rounds
rounds_expanded <- rounds_expanded %>%
  group_by(subject_id) %>%
  mutate(task_id = row_number()) %>%
  ungroup() %>%
  relocate(task_id, .after = round_id)

# Adjust task_condition based on task_id being the first in each round
rounds_expanded <- rounds_expanded %>%
  group_by(subject_id, round_id) %>%
  mutate(task_condition = if_else(task_id %% 3 == 1, "Incorrect", task_condition)) %>%
  ungroup()

# Adjust task_condition for the second task in each round based on the third task's condition
subject_rounds <- rounds_expanded %>%
  group_by(subject_id, round_id) %>%
  mutate(task_condition = if_else(task_id %% 3 == 2, 
                                  if_else(lead(task_condition, default = "Correct") == "Correct", "Incorrect", "Correct"), 
                                  task_condition)) %>%
  ungroup()

subject_rounds
```




```{r}

subject_ratings 


aggregated_ratings <- subject_ratings %>%
  mutate(renamed_question = question) %>%  # Initialize with existing questions
  mutate(renamed_question = ifelse(grepl("Question - How confident were you in correctly guessing", question), "Confident", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How easy was it to guide", question), "Ease", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How well were you able to predict", question), "Predict", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your understanding", question), "Focus", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your awareness", question), "Awareness", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How complex were your instructions when guiding", question), "Complex", renamed_question))

# So now we want to keep this in a separate variable
processed_ratings <- aggregated_ratings


aggregated_ratings <- aggregated_ratings %>%
  group_by(subject_id, renamed_question, gaze_condition) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")
  
aggregated_ratings2 <- processed_ratings %>%
  group_by(subject_id, renamed_question, task_condition) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")

  processed_ratings
  aggregated_ratings
  aggregated_ratings2
  # Split the data frame by question
  questions_list <- split(aggregated_ratings, aggregated_ratings$renamed_question)

  # Function to create and save a boxplot and scatter plot for each question
  plot_question <- function(data, question_name) {
    p <- ggplot(data, aes(x = gaze_condition, y = mean_rating, fill = gaze_condition)) +
      geom_boxplot(alpha = 0.5) +  # Set transparency to see scatter points clearly
      geom_point(position = position_jitter(width = 0.1), color = "black", size = 3, alpha = 0.6) +  # Add jitter to avoid overplotting
      labs(title = question_name, x = "Gaze Condition", y = "Mean Rating") +
      theme_classic() +
      scale_y_continuous(limits = c(1, 5), oob = scales::oob_squish)  # Force all data into the specified range

    # Save the plot
    ggsave(paste0("Boxplot_Scatter_", question_name, ".png"), plot = p, width = 10, height = 8, dpi = 300)
  }

```




We process the subject ratings and combine the graphs to create the complete ratings plot.
```{r}
# Reshape data from long to wide format for each question type and gaze condition
wide_ratings <- aggregated_ratings %>%
  pivot_wider(names_from = c(renamed_question, gaze_condition), 
              values_from = mean_rating,
              names_sep = "_")

# Function to perform Wilcoxon signed-rank test for each question type
perform_wilcox_test <- function(data, question_name) {
  data_on <- data[[paste0(question_name, "_On")]]
  data_off <- data[[paste0(question_name, "_Off")]]
  test_result <- wilcox.test(data_on, data_off, paired = TRUE)
  return(test_result)
}

# List of all question types
question_types <- unique(aggregated_ratings$renamed_question)


wide_ratings
# Pivot data for plotting
wide_ratings_for_plots <- aggregated_ratings %>%
  pivot_wider(names_from = renamed_question, values_from = mean_rating)
  
wide_ratings_for_plots <- wide_ratings_for_plots %>% mutate(gaze_condition = factor(gaze_condition, levels = c("Off", "On")))
  
  #Print out that this is results
  print("Results")
  
  awareness_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Awareness ~ gaze_condition, paired=T) 
  awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condition")
  awareness_wilcox
  print("Z for Awareness")
  compute_wilcoxon_signed_rank_summary(awareness_wilcox)
  
  
wide_ratings_for_plots

# Filter data for 'On' and 'Off' conditions for the 'Awareness' variable
x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Awareness)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Awareness)

# Calculate the Wilcoxon Z statistic for paired data
z_awareness <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Awareness:", z_awareness))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How aware were you of the student's attention?", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Confident
confident_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Confident ~ gaze_condition, paired=T)
confident_wilcox.p <- confident_wilcox |> add_x_position(x="gaze_condiition")
confident_wilcox
print("Z for Confident")
compute_wilcoxon_signed_rank_summary(confident_wilcox)

x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Confident)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Confident)

z_confident <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Confident:", z_confident))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Confident)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How confident did you feel when guiding or correcting the student?", x="", y="") +
  stat_pvalue_manual(confident_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Ease

ease_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Ease ~ gaze_condition, paired=T)
ease_wilcox.p <- ease_wilcox |> add_x_position(x="gaze_condiition")
ease_wilcox
print("Z for Ease")
compute_wilcoxon_signed_rank_summary(ease_wilcox)
x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Ease)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Ease)

z_ease <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Ease:", z_ease))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Ease)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How easy was it to guide and correct the student?", x="", y="") +
  stat_pvalue_manual(ease_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Focus

focus_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Focus ~ gaze_condition, paired=T)
focus_wilcox.p <- focus_wilcox |> add_x_position(x="gaze_condiition")
focus_wilcox
print("Z for Focus")
compute_wilcoxon_signed_rank_summary(focus_wilcox)

x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Focus)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Focus)

z_focus <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Focus:", z_focus))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Focus)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How well did you understand the student's focus?", x="", y="") +
  stat_pvalue_manual(focus_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Predict

predict_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Predict ~ gaze_condition, paired=T)
predict_wilcox.p <- predict_wilcox |> add_x_position(x="gaze_condiition")
predict_wilcox
print("Z for Predict")
compute_wilcoxon_signed_rank_summary(predict_wilcox)

x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Predict)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Predict)

z_predict <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Predict:", z_predict))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Predict)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How well were you able to predict the student's actions?", x="", y="") +
  stat_pvalue_manual(predict_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))



test_result_awareness <- perform_wilcox_test(wide_ratings, "Complex")
test_result_awareness

test_result_confidence <- perform_wilcox_test(wide_ratings, "Confident")
test_result_confidence

test_result_ease <- perform_wilcox_test(wide_ratings, "Ease")
test_result_ease

test_result_focus <- perform_wilcox_test(wide_ratings, "Focus")
test_result_focus

test_result_predict <- perform_wilcox_test(wide_ratings, "Predict")
test_result_predict

awareness_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Awareness ~ gaze_condition, paired=T) 
awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condiition")
awareness_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="Awareness", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

# Plot w

# Apply the Wilcoxon test to each question type
test_results <- lapply(question_types, function(q) {
  # Perform the test
  test_result <- perform_wilcox_test(wide_ratings, q)
  list(question = q, wilcox_test_result = test_result)
})

# Output the test results
test_results
question_types
wide_ratings
#write_csv(wide_ratings, paste0(path_to_files, "wide_ratings_test.csv"))


# Get a total count of the number of ratings for each question broken down by 1, 2, 3, 4, 5
summed_ratings <- processed_ratings %>%
  group_by(gaze_condition, renamed_question, rating) %>%
  summarise(count = n(), .groups = "drop")

summed_ratings

# Now we want horizontal stacked bar charts for each question type with the legend on the top-center as discrete values

# Plot the stacked bar chart for each question type, so the x-axis is the count, y-axis is the gaze condition, and we have 'stacked' grouped bar charts for each rating to show distribution. Let's start with Awareness


# Define a darker grayscale theme for each rating number using material design shades
rating_colors <- c(
  "1" = "#E0E0E0",  # Material light gray (darker)
  "2" = "#BDBDBD",  # Material gray
  "3" = "#9E9E9E",  # Material medium gray
  "4" = "#757575",  # Material dark gray
  "5" = "#424242"   # Material very dark gray (darker)
)

# Filter the data for Awareness
awareness_data <- summed_ratings %>% filter(renamed_question == "Awareness")
# Ensure the rating is a factor with levels ordered from 1 to 5
awareness_data$rating <- factor(awareness_data$rating, levels = c("1", "2", "3", "4", "5"))

# Create the plot for Awareness
awareness_plot <- ggplot(awareness_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.5) +  # Adjust bar width
  scale_fill_manual(values = rating_colors) +
  labs(title = "Awareness Ratings by Gaze Condition", x = "", y = "Awareness") +
  scale_y_discrete(labels = c("off" = "Gaze Off", "on" = "Gaze On")) +
  theme_minimal() +
  theme(legend.position = "top", legend.justification = "center", 
        axis.text.y = element_text(size = 8),  # Reduce font size
        axis.title.y = element_text(size = 10))

# Plot the Awareness data
awareness_plot


# Filter the data for Awareness and ensure the rating is a factor
awareness_data <- summed_ratings %>% filter(renamed_question == "Awareness")
awareness_data$rating <- factor(awareness_data$rating, levels = c("1", "2", "3", "4", "5"))
awareness_plot <- ggplot(awareness_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.9) +
  scale_fill_manual(values = rating_colors) +
  labs(title = "", x = "", y = "Awareness") +
  scale_y_discrete(labels = c("off" = "Gaze Off", "on" = "Gaze On")) +
  theme_minimal() +
  theme(legend.position = "top", legend.justification = "center", 
        axis.text.y = element_text(size = 8), 
        axis.title.y = element_text(size = 10))

# Repeat for other question types
# Confident
confident_data <- summed_ratings %>% filter(renamed_question == "Confident")
confident_data$rating <- factor(confident_data$rating, levels = c("1", "2", "3", "4", "5"))
confident_plot <- ggplot(confident_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.9) +
  scale_fill_manual(values = rating_colors) +
  labs(title = "", x = "", y = "Confident") +
  theme_minimal() +
  theme(legend.position = "none")

# Ease
ease_data <- summed_ratings %>% filter(renamed_question == "Ease")
ease_data$rating <- factor(ease_data$rating, levels = c("1", "2", "3", "4", "5"))
ease_plot <- ggplot(ease_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.9) +
  scale_fill_manual(values = rating_colors) +
  labs(title = "", x = "", y = "Ease") +
  theme_minimal() +
  theme(legend.position = "none")

# Focus
focus_data <- summed_ratings %>% filter(renamed_question == "Focus")
focus_data$rating <- factor(focus_data$rating, levels = c("1", "2", "3", "4", "5"))
focus_plot <- ggplot(focus_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.9) +
  scale_fill_manual(values = rating_colors) +
  labs(title = "", x = "", y = "Focus") +
  theme_minimal() +
  theme(legend.position = "none")

# Predict
predict_data <- summed_ratings %>% filter(renamed_question == "Predict")
predict_data$rating <- factor(predict_data$rating, levels = c("1", "2", "3", "4", "5"))
predict_plot <- ggplot(predict_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.9) +
  scale_fill_manual(values = rating_colors) +
  labs(title = "", x = "", y = "Predict") +
  theme_minimal() +
  theme(legend.position = "none")

# Combine plots using patchwork
combined_plot <- awareness_plot / confident_plot / ease_plot / focus_plot / predict_plot +
  plot_layout(heights = c(1, 1, 1, 1, 1))  # Equal heights for each plot
combined_plot

summed_ratings
# Sum ratings for off and on conditions for each renamed question
summed_ratings_by_condition <- summed_ratings %>%
  group_by(renamed_question, gaze_condition) %>%
  summarise(count = sum(count), .groups = "drop")
summed_ratings_by_condition

#So there seems to be one extra value for each gaze_condition off question. Which subject_id is this? Can we figure out which subject_id has no corresponding gaze_condition 'on'? If we count responses by each subject_id it should show 4 for each gaze condition
# Count the number of responses for each subject_id and gaze_condition
responses_by_subject <- processed_ratings %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(count = n(), .groups = "drop")

responses_by_subject
#What's the total number of responses overall for awareness?
total_responses <- summed_ratings %>% filter(renamed_question == "Awareness") %>% summarise(total_responses = sum(count))
total_responses
```
```{r awareness-plot, fig.height=2, fig.width=8}
# Filter the data for Awareness
awareness_data <- summed_ratings %>% filter(renamed_question == "Awareness")
# Ensure the rating is a factor with levels ordered from 1 to 5
awareness_data$rating <- factor(awareness_data$rating, levels = c("1", "2", "3", "4", "5"))
ggplot(awareness_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.9) +  # Adjust bar width
  scale_fill_manual(values = rating_colors) +
  labs(title = "", x = "", y = "Awareness") +
  scale_y_discrete(labels = c("off" = "Gaze Off", "on" = "Gaze On")) +
  theme_minimal() +
  theme(legend.position = "top", legend.justification = "center", 
        axis.text.y = element_text(size = 8),  # Reduce font size
        axis.title.y = element_text(size = 10))
```

TODO: The combined plot needs to have the legend labelled from 1 being the lowest to 5 being the highest.

```{r combined-plot, fig.height=10, fig.width=8}
combined_plot <- awareness_plot / confident_plot / ease_plot / focus_plot / predict_plot
combined_plot
```


Here we will want to calculate the mean duration of each round for each subject and gaze condition. We will then perform a Wilcoxon signed-rank test to compare the mean durations between the 'Off' and 'On' gaze conditions.


```{r}

# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(previous_time = lag(min_unity_log_time),
         round_duration = if_else(is.na(previous_time), min_unity_log_time, min_unity_log_time - previous_time))

round_durations

mean_durations <- round_durations %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE))

mean_durations

# Step 4: Reshape data from long to wide format
wide_mean_durations <- mean_durations %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = mean_duration,
              names_prefix = "mean_duration_")

wide_mean_durations


# Assuming wide_mean_durations is already loaded with your data
# Ensure it's in the correct format
wide_mean_durations <- wide_mean_durations %>%
  pivot_longer(cols = starts_with("mean_duration"), names_to = "gaze_condition", values_to = "mean_duration") %>%
  mutate(gaze_condition = sub("mean_duration_", "", gaze_condition))

# Make it a data frame
wide_mean_durations <- wide_mean_durations %>% as.data.frame()
wide_mean_durations
# Perform Wilcoxon signed-rank test using rstatix
duration_wilcox <- wide_mean_durations %>%
  rstatix::wilcox_test(mean_duration ~ gaze_condition, paired = TRUE)
duration_wilcox.p <- duration_wilcox |> add_x_position(x="gaze_condition")
duration_wilcox 
print("Z for Duration")
compute_wilcox_summary(duration_wilcox)
x_data <- wide_mean_durations %>% 
  filter(gaze_condition == "On") %>% 
  pull(mean_duration)
y_data <- wide_mean_durations %>% 
  filter(gaze_condition == "Off") %>% 
  pull(mean_duration)

z_duration <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
z_duration
print(paste("Z for Duration:", z_duration))


```



```{r}

# Perform Wilcoxon signed-rank test
wilcox_test_result <- wilcox.test(
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "Off"],
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "On"],
  paired = TRUE
)

# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 350, 
  label = paste("p-value:", formatC(wilcox_test_result$p.value, format = "e", digits = 2))
)

# Plotting the results
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 50)) +
  labs(title = "Comparison of Mean Round (3 Tasks) Durations by Gaze Condition", x = "Gaze Condition", y = "Mean Duration") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```

Paper-friendly version of the plot that can be customized.
```{r duration-plot, fig.height=3, fig.width=3}
# Plotting the results. We don't want the p-value label. We don't want to have the x-axis labeled as Gaze Condition. The x-axis labels should instead be "Gaze Off" and "Gaze On". We do not want the legend. We do not want the title. Finally, we want to have error bars show up as well.

wide_mean_durations
# Calculate summary statistics
summary_round_durations <- round_durations %>%
  group_by(gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE),
            sd_duration = sd(round_duration, na.rm = TRUE))

summary_round_durations
# Plot using wide_mean_durations for the boxplot and jitter, and summary_mean_durations for error bars with horizontal caps
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot(width = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5) +
  geom_errorbar(data = summary_round_durations, aes(ymin = mean_duration - sd_duration, ymax = mean_duration + sd_duration, x = gaze_condition), width = 0.6, color = "black") +
  # Adding horizontal end caps
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration - sd_duration, yend = mean_duration - sd_duration), color = "black") +
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration + sd_duration, yend = mean_duration + sd_duration), color = "black") +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 100)) +
  labs(title = "", x = "", y = "Mean Duration (in seconds)") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))
```





```{r}
# UHH don't run this unless you need to recalculate timings
# Now we want to compute end_time for timings.csv files that do not have that column
# Define the path to the root directory containing the subdirectories

find_end_time <- function(start_time, round, task, timings, segments) {
  print(paste("Processing task:", task, "in round:", round, "with start time:", start_time))
  
  # Find the start time of the next task in the same round
  next_task_start_times <- timings %>%
    filter(round == round, task == task, start > start_time) %>%
    arrange(task) %>%
    pull(start)

  print("Next task start times:")
  print(next_task_start_times)

  # Determine the next task start time or use a large number if it's the last task
  next_task_start_time <- if (length(next_task_start_times) > 0) next_task_start_times[1] else Inf

  print("Next task start time:")
  print(next_task_start_time)

  # Find the last segment that starts before the next task starts and ends after the current task's start time
  valid_segments <- segments %>%
    filter(start >= start_time, start < next_task_start_time)

  print("Valid segments:")
  print(valid_segments)

  if (nrow(valid_segments) > 0) {
    # Find the maximum end time among segments that start before the next task
    end_time <- max(valid_segments$end)
  } else {
    # If no valid segments, use the maximum end time of any segment that starts after this task's start time
    end_time <- max(segments %>% filter(start >= start_time) %>% pull(end), na.rm = TRUE)
  }

  print("Calculated end time:")
  print(end_time)

  return(end_time)
}

# Function to process each directory
process_directory <- function(directory_path) {
  segments_path <- file.path(directory_path, "segments.csv")
  timings_path <- file.path(directory_path, "timings.csv")
  
  if (file.exists(segments_path) && file.exists(timings_path)) {
    segments <- read_csv(segments_path)
    timings <- read_csv(timings_path)

    if (!"end" %in% names(timings)) {
      print("Processing timings file without end time for directory:")
      print(directory_path)
      
      # Calculate end times for each row in timings
      timings$end <- mapply(function(start, round, task) {
        find_end_time(start, round, task, timings, segments)
      }, timings$start, timings$round, timings$task)

      # Save the updated timings file in the same directory
      print("Writing updated timings file...")
      print(file.path(directory_path, "timings_updated.csv"))
      
      write_csv(timings, file.path(directory_path, "timings_updated.csv"))
    }
  }
}
# List all directories under the root path and process each one
directories <- fs::dir_ls(path_to_files, recurse = TRUE, type = "directory")
walk(directories, process_directory)

```


```{r}
directory_path <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\QWE-042"
timings_path <- file.path(directory_path, "timings_updated.csv")
words_path <- file.path(directory_path, "words.csv")
participant_data_path <- file.path(directory_path, "participantData.csv")
print(directory_path)
  
timings <- read_csv(timings_path, na = na_strings, show_col_types = FALSE)
words <- read_csv(words_path, na = na_strings, show_col_types = FALSE)
participant_data <- read_csv(participant_data_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
print(timings_path)
    #timings
    #words
    #participant_data

    # Extract subject_id
    subject_id <- participant_data$subject_id

    
    # Clean and standardize words
    words <- words %>%
      mutate(word = tolower(word),  # Convert to lower case
             word = str_trim(word),  # Trim whitespace
             word = replace_contraction(word),  # Standardize contractions
             word = str_replace_all(word, "[[:punct:]]", ""))  # Remove punctuation using stringr

    #words
    #print("Words data after cleaning:")
    #print(head(words))

    # Merge words data with timings based on overlapping intervals
    words_timings <- words %>%
      mutate(across(c(start, end), as.numeric)) %>%
      full_join(timings, by = character()) %>%
      rename(start_word = start.x, end_word = end.x, start_timing = start.y, end_timing = end.y)
    words_timings


    # Apply filter to match words within the timings intervals
    words_timings <- words_timings %>%
      filter(start_word >= start_timing & end_word <= end_timing)
  
    # Count words for each round and task
    word_counts <- words_timings %>%
      group_by(round, task, word) %>%
      summarise(count = n(), .groups = 'drop')
    
    word_counts
      
    # Add subject_id to each row
    word_counts <- word_counts %>%
      mutate(subject_id = subject_id) %>%
      rename(round_id = round, task_id = task) %>%
      relocate(subject_id, .before = round_id) %>%
      arrange(round_id, task_id, desc(count))
    
    word_counts
    participant_data
    subject_rounds

    subject_rounds <- subject_rounds %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    word_counts <- word_counts %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    

    # Perform the join
    word_counts <- inner_join(subject_rounds, word_counts, by = c("subject_id", "round_id", "task_id"))
    word_counts

    # Optionally, save the result in the same directory
    write_csv(word_counts, file.path(directory_path, "word_counts.csv"))


```



```{r}


process_directory <- function(directory_path) {
  timings_path <- file.path(directory_path, "timings_updated.csv")
  words_path <- file.path(directory_path, "words.csv")
  participant_data_path <- file.path(directory_path, "participantData.csv")
  print(directory_path)
  
  if (file.exists(timings_path) && file.exists(words_path)) {
    timings <- read_csv(timings_path, na = na_strings, show_col_types = FALSE)
    words <- read_csv(words_path, na = na_strings, show_col_types = FALSE)
    participant_data <- read_csv(participant_data_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
    print(timings_path)
    timings
    words
    participant_data

    # Extract subject_id
    subject_id <- participant_data$subject_id

    
    # Clean and standardize words
    words <- words %>%
      mutate(word = tolower(word),  # Convert to lower case
             word = str_trim(word),  # Trim whitespace
             word = replace_contraction(word),  # Standardize contractions
             word = str_replace_all(word, "[[:punct:]]", ""))  # Remove punctuation using stringr

    words
    print("Words data after cleaning:")
    print(head(words))

    # Merge words data with timings based on overlapping intervals
    words_timings <- words %>%
      mutate(across(c(start, end), as.numeric)) %>%
      full_join(timings, by = character()) %>%
      rename(start_word = start.x, end_word = end.x, start_timing = start.y, end_timing = end.y)

    print("Data after merging and renaming:")
    print(head(words_timings))

    # Apply filter to match words within the timings intervals
    words_timings <- words_timings %>%
      filter(start_word >= start_timing & end_word <= end_timing)

    print("Data after filtering:")
    print(head(words_timings))

    # Count words for each round and task
    word_counts <- words_timings %>%
      group_by(round, task, word) %>%
      summarise(count = n(), .groups = 'drop')
      
    # Add subject_id to each row
    word_counts <- word_counts %>%
      mutate(subject_id = subject_id) %>%
      rename(round_id = round, task_id = task) %>%
      relocate(subject_id, .before = round_id) %>%
      arrange(round_id, task_id, desc(count))


    print("Aggregated word counts:")
    print(head(word_counts))


    subject_rounds <- subject_rounds %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    word_counts <- word_counts %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    

    # Perform the join
    word_counts <- inner_join(subject_rounds, word_counts, by = c("subject_id", "round_id", "task_id"))
    word_counts
    # Optionally, save the result in the same directory
    write_csv(word_counts, file.path(directory_path, "word_counts.csv"))
    
    return(word_counts)
  } else {
    warning("Timings or words file missing in ", directory_path)
  }
}

# List all directories under the root path and process each one
directories <- dir_ls(path_to_files, recurse = TRUE, type = "directory")
results <- map(directories, process_directory)

# Combine all results into a single data frame
#final_results <- bind_rows(results)

```

TODO: Plot the word counts for each condition and round.

Start off by figuring out % of time they spent looking at the terrain in each condition/task.

First we do a test with one file

```{r}
#subject_rounds
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
terrain_path <- paste0(path_to_files, "DIB-019\\DIB-019_terrain_fixed.csv")
terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))
terrain_dt <- terrain %>% setDT() %>%
  .[round_id != 0]

#terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
terrain_dt

# Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
terrain_dt <- terrain_dt %>% 
  mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
  filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
  select(-task_id_mod3)  # Remove the helper column

terrain_dt_grouped <- terrain_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_syncs = n(),  # Total number of sync_id entries
    valid_hits = sum(hit_valid, na.rm = TRUE),  # Count of TRUE in hit_valid
    percent_looking_at_terrain = (valid_hits / total_syncs) * 100  # Calculate percentage
  ) %>%
  ungroup()

terrain_dt_grouped
# Calculate the average percentage of time looking at the terrain for each gaze condition
average_percent_by_condition <- terrain_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_percent_looking_at_terrain = mean(percent_looking_at_terrain, na.rm = TRUE)
  ) %>% ungroup()

average_percent_by_condition

```


Mean head_angle_using_dir and eye_angle_using_dir for each condition

```{r}
# Calculate the mean head_angle_using_dir and eye_angle_using_dir for each condition
terrain_dt
# For head_angle_using_dir and eye_angle_using_dir, find the maximum value that is not -10000 and then use that instead of -10000

#First get the maximum for each and add a new column with those, PMAX DOES NOT WORK


max_head_angle <- terrain_dt %>% filter(head_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle = max(head_angle_using_dir, na.rm = TRUE)) %>% ungroup()

max_eye_angle <- terrain_dt %>% filter(eye_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_eye_angle = max(eye_angle_using_dir, na.rm = TRUE)) %>% ungroup()

max_distance <- terrain_dt %>% filter(distance != 10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_distance = max(distance, na.rm = TRUE)) %>% ungroup()

angles_terrain_dt <- merge(terrain_dt, max_head_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
angles_terrain_dt <- merge(angles_terrain_dt, max_eye_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
angles_terrain_dt <- merge(angles_terrain_dt, max_distance, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)

# Now find all -10000 values in eye_angle_using_dir and head_angle_using_dir and replace them with the max values

angles_terrain_dt <- angles_terrain_dt %>% 
  mutate(head_angle_using_dir = ifelse(head_angle_using_dir == -10000, max_head_angle, head_angle_using_dir),
         eye_angle_using_dir = ifelse(eye_angle_using_dir == -10000, max_eye_angle, eye_angle_using_dir),
         distance = ifelse(distance == 10000, max_distance, distance))

  
average_angles_by_condition <- angles_terrain_dt %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    mean_head_angle_using_dir = mean(head_angle_using_dir, na.rm = TRUE),
    mean_eye_angle_using_dir = mean(eye_angle_using_dir, na.rm = TRUE),
    mean_distance = mean(distance, na.rm = TRUE)
  ) %>% ungroup()

average_angles_by_condition


```


#TODO: Some other things to investigate - use head_angle_using_dir and eye_angle_using_dir for angles
Can check the distance fields to see how much movement their eyes did on the terrain
Mean head_angle_using_dir and eye_angle_using_dir for each condition
delta angles should be in angle per second, so we can calculate the average delta angle per second for each condition.


The all files version.

```{r}

# Process each terrain file and calculate the average percentage of time looking at the terrain
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_percent_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_terrain_fixed\\.csv$") %>%
  map_dfr(.f = function(terrain_path) {
    print(terrain_path)
    terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
          mutate(subject_id = substr(subject_id, 1, 7))
    terrain_dt <- terrain %>% setDT() %>%
      .[round_id != 0]

    #terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

    terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
        
    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    terrain_dt <- terrain_dt %>% 
      mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
      filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      select(-task_id_mod3)  # Remove the helper column    
    
    
    terrain_dt <- terrain_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_syncs = n(),  # Total number of sync_id entries
        valid_hits = sum(hit_valid, na.rm = TRUE),  # Count of TRUE in hit_valid
        percent_looking_at_terrain = (valid_hits / total_syncs) * 100  # Calculate percentage
      ) %>%
      ungroup()
    terrain_dt

    # Calculate the average percentage of time looking at the terrain for each gaze condition
    average_percent_by_condition <- terrain_dt %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_percent_looking_at_terrain = median(percent_looking_at_terrain, na.rm = TRUE)
      ) %>% ungroup()

    average_percent_by_condition
  })
average_percent_by_condition_all

# Process each terrain file and calculate the average head_angle_using_dir and eye_angle_using_dir for each condition
average_angles_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_terrain_fixed\\.csv$") %>%
  map_dfr(.f = function(terrain_path) {
    print(terrain_path)
    terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
          mutate(subject_id = substr(subject_id, 1, 7))
    terrain_dt <- terrain %>% setDT() %>%
      .[round_id != 0]

    #terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

    terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
    terrain_dt
    
    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    terrain_dt <- terrain_dt %>% 
      mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
      filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      select(-task_id_mod3)  # Remove the helper column
    
    # Calculate the mean head_angle_using_dir and eye_angle_using_dir for each condition
    max_head_angle <- terrain_dt %>% filter(head_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle = max(head_angle_using_dir, na.rm = TRUE)) %>% ungroup()
    
    max_eye_angle <- terrain_dt %>% filter(eye_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_eye_angle = max(eye_angle_using_dir, na.rm = TRUE)) %>% ungroup()

    max_distance <- terrain_dt %>% filter(distance != 10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_distance = max(distance, na.rm = TRUE)) %>% ungroup()
    
    max_head_angle2 <- terrain_dt %>% filter(head_angle != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle2 = max(head_angle, na.rm = TRUE)) %>% ungroup()
    
    
    angles_terrain_dt <- merge(terrain_dt, max_head_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_eye_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_distance, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_head_angle2, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    
    
    # Now find all -10000 values in eye_angle_using_dir and head_angle_using_dir and replace them with the max values
    
    angles_terrain_dt <- angles_terrain_dt %>% 
      mutate(head_angle_using_dir = ifelse(head_angle_using_dir == -10000, max_head_angle, head_angle_using_dir),
             eye_angle_using_dir = ifelse(eye_angle_using_dir == -10000, max_eye_angle, eye_angle_using_dir),
             distance = ifelse(distance == 10000, max_distance, distance))
    
      
    average_angles_by_condition <- angles_terrain_dt %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        mean_head_angle_using_dir = median(head_angle_using_dir, na.rm = TRUE),
        mean_eye_angle_using_dir = median(eye_angle_using_dir, na.rm = TRUE),
        mean_distance = median(distance, na.rm = TRUE),
        mean_head_angle2 = median(head_angle, na.rm = TRUE)
      ) %>% ungroup()
    
    average_angles_by_condition
  })



```

Percentage for all conditions
```{r}
# Now pivot it wider
average_percent_by_condition_all <- average_percent_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_percent_looking_at_terrain,
              names_prefix = "average_percent_looking_at_terrain_") %>%
  as.data.frame()


average_percent_by_condition_all

#Now we can plot the results using box and whiskers

average_percent_by_condition_all %>%
  pivot_longer(cols = starts_with("average_percent_looking_at_terrain"), names_to = "gaze_condition", values_to = "average_percent_looking_at_terrain") %>%
  ggplot(aes(x = gaze_condition, y = average_percent_looking_at_terrain, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "Comparison of Average Percentage of Time Looking at Terrain by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Time Looking at Terrain") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```

```{r}
average_angles_by_condition_all

# Assuming average_angles_by_condition_all is a data frame
# Convert to data.table and reshape the data from long to wide format using dcast
average_angles_by_condition_all_dt <- average_angles_by_condition_all %>%
  data.table::as.data.table() %>%
  data.table::dcast(subject_id ~ gaze_condition, 
                    value.var = c("mean_head_angle_using_dir", "mean_eye_angle_using_dir", "mean_distance", "mean_head_angle2"),
                    fun.aggregate = mean)
names(average_angles_by_condition_all_dt)
average_angles_by_condition_all_dt

#Now plot each of the angles and distance
# Box and whiskers for head angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_head_angle_using_dir"), names_to = "gaze_condition", values_to = "mean_head_angle_using_dir") %>%
  ggplot(aes(x = gaze_condition, y = mean_head_angle_using_dir, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Head Angle Using Direction by Gaze Condition", x = "Gaze Condition", y = "Mean Head Angle Using Direction") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for eye angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_eye_angle_using_dir"), names_to = "gaze_condition", values_to = "mean_eye_angle_using_dir") %>%
  ggplot(aes(x = gaze_condition, y = mean_eye_angle_using_dir, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Eye Angle Using Direction by Gaze Condition", x = "Gaze Condition", y = "Mean Eye Angle Using Direction") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for distance
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_distance"), names_to = "gaze_condition", values_to = "mean_distance") %>%
  ggplot(aes(x = gaze_condition, y = mean_distance, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +
  labs(title = "Comparison of Mean Distance by Gaze Condition", x = "Gaze Condition", y = "Mean Distance") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for head angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_head_angle2"), names_to = "gaze_condition", values_to = "mean_head_angle2") %>%
  ggplot(aes(x = gaze_condition, y = mean_head_angle2, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Head Angle by Gaze Condition", x = "Gaze Condition", y = "Mean Head Angle") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))
```

TODO: I found no differences but we can further group it by task condition and task type if we want.


Now we do it for the student


Single file version to test % of time looking at object

```{r}

#C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/DIB-019_object.csv
object_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/DIB-019_object.csv"
print(object_path)
object_data <- read_csv(object_path, na = na_strings, show_col_types = FALSE) %>%
  clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))
object_dt <- object_data %>% setDT() %>%
  .[round_id != 0]

object_dt <- merge(object_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
object_dt <- object_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

# Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
object_dt <- object_dt %>%
  mutate(task_id_mod3 = task_id %% 3 == 0) %>%
  filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
  select(-task_id_mod3)  # Remove the helper column

# Calculate looking at object based on eye_angle threshold
object_dt <- object_dt %>%
  mutate(looking_at_object = eye_angle_using_pos < 25)  # True if eye_angle is under 25, indicating looking at the object
object_dt

object_dt_grouped <- object_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_syncs = n(),  # Total number of sync_id entries
    valid_hits = sum(looking_at_object, na.rm = TRUE),   # Count of entries where looking at object
    percent_looking_at_object = (valid_hits / total_syncs) * 100  # Calculate percentage
  ) %>%
  ungroup()

# Calculate the average percentage of time looking at the object for each gaze condition
average_percent_by_condition <- object_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_percent_looking_at_object = median(percent_looking_at_object, na.rm = TRUE)
  ) %>%
  ungroup()

average_percent_by_condition
```

Now we do it for all files

```{r}

# Process each object file and calculate the average percentage of time looking at the object with eye_angle over 25
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_object_percent_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_object\\.csv$") %>%
  map_dfr(.f = function(object_path) {
    print(object_path)
    object_data <- read_csv(object_path, na = na_strings, show_col_types = FALSE) %>%
      clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))
    object_dt <- object_data %>% setDT() %>%
      .[round_id != 0]

    object_dt <- merge(object_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    object_dt <- object_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    object_dt <- object_dt %>%
      mutate(task_id_mod3 = task_id %% 3 == 0) %>%
      filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      select(-task_id_mod3)  # Remove the helper column
    
    # Calculate looking at object based on eye_angle threshold
    object_dt <- object_dt %>%
      mutate(looking_at_object = eye_angle_using_pos < 25)  # True if eye_angle is under 25, indicating looking at the object


    object_dt_grouped <- object_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_syncs = n(),  # Total number of sync_id entries
        valid_hits = sum(looking_at_object, na.rm = TRUE),   # Count of entries where looking at object
        percent_looking_at_object = (valid_hits / total_syncs) * 100  # Calculate percentage
      ) %>%
      ungroup()

    # Calculate the average percentage of time looking at the object for each gaze condition
    average_percent_by_condition <- object_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_percent_looking_at_object = median(percent_looking_at_object, na.rm = TRUE)
      ) %>%
      ungroup()

    average_percent_by_condition
  })
average_object_percent_by_condition_all
```

```{r}
# Now pivot it wider

average_object_percent_by_condition_all_wide <- average_object_percent_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_percent_looking_at_object,
              names_prefix = "average_percent_looking_at_object_") %>%
  as.data.frame()

average_object_percent_by_condition_all_wide

#Now we can plot the results using box and whiskers
average_object_percent_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_percent_looking_at_object"), names_to = "gaze_condition", values_to = "average_percent_looking_at_object") %>%
  ggplot(aes(x = gaze_condition, y = average_percent_looking_at_object, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 60), breaks = seq(0, 60, 5)) +
  labs(title = "Comparison of Average Percentage of Time Looking at Student by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Time Looking at Student") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```
# There might be a significant difference here so let's test it with wilcoxon.

```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_object_percent_by_condition_all
average_object_percent_by_condition_all_wide

#So for future reference wilcoxon wants to see subject_id, gaze_condition, and average_percent_looking_at_object in that format, so we have that in average_object_percent_by_condition_all

#Now we can do the wilcoxon test
looking_at_object_wilcox <- average_object_percent_by_condition_all %>%
  rstatix::wilcox_test(average_percent_looking_at_object ~ gaze_condition, paired = TRUE)
looking_at_object_wilcox.p <- looking_at_object_wilcox %>%
  add_x_position(x = "gaze_condition")
looking_at_object_wilcox


#Okay there is a sig diff, now we do it the wilcox.test way so we can plot it

looking_at_object_wilcox_test <- wilcox.test(average_object_percent_by_condition_all$average_percent_looking_at_object[average_object_percent_by_condition_all$gaze_condition == "Off"], average_object_percent_by_condition_all$average_percent_looking_at_object[average_object_percent_by_condition_all$gaze_condition == "On"], paired = TRUE)
looking_at_object_wilcox_test
print("Z for Looking at Object")
#compute_wilcox_summary(looking_at_object_wilcox_test)

x_data <- average_object_percent_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_percent_looking_at_object)
y_data <- average_object_percent_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_percent_looking_at_object)
z_object <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
print(paste("Z for Object:", z_object))

# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 50, 
  label = paste("p-value:", formatC(looking_at_object_wilcox_test$p.value, format = "f", digits = 6))
)

average_object_percent_by_condition_all
ggplot(average_object_percent_by_condition_all, aes(x = gaze_condition, y = average_percent_looking_at_object, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 50), breaks = seq(0, 50, 5)) +
  labs(title = "Comparison of Average Percentage of Time Looking at Student by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Time Looking at Student") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)


```
Defined as anytime the eye gaze ray hit the student collider (capsule collider containing student).



We'll check annotations later but now let's check the word count distrbution.
First just check total words per condition

Single file test
```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")

word_count_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/word_counts.csv"
print(word_count_path)

word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_character(), task_id = col_character())) %>% clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))

# Remove start_round and end_round columns
subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))
word_count_data

word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
word_count_dt

# Need to compute the task_type column based on mod
# mod 3 == 1 -> "Locate"
# mod 3 == 2 -> "Puzzle"
# mod 3 == 0 -> "Remaining"

word_count_dt <- word_count_dt %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
word_count_dt

# Calculate the total word count for each condition
word_count_dt_grouped <- word_count_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_words = sum(count, na.rm = TRUE)  # Total word count
  ) %>%
  ungroup()

# Now calculate the mean word count for each condition
average_word_count_by_condition <- word_count_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_word_count = mean(total_words, na.rm = TRUE)
  ) %>%
  ungroup()
average_word_count_by_condition

```


All files version

```{r}
# Process each word count file and calculate the average word count for each condition
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_word_count_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)

    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"

    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    
    # Calculate the total word count for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_words = sum(count, na.rm = TRUE)  # Total word count
      ) %>%
      ungroup()
    
    # Now calculate the mean word count for each condition
    average_word_count_by_condition <- word_count_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_word_count = mean(total_words, na.rm = TRUE)
      ) %>%
      ungroup()
    
    average_word_count_by_condition
  })
average_word_count_by_condition_all

word_count_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)

    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"

    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    
    # Calculate the total word count for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_words = sum(count, na.rm = TRUE)  # Total word count
      ) %>%
      ungroup()
    word_count_dt_grouped
  })

word_count_by_condition_all
```

Now we plot it

```{r}
# Now pivot it wider
average_word_count_by_condition_all_wide <- average_word_count_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_word_count,
              names_prefix = "average_word_count_") %>%
  as.data.frame()

average_word_count_by_condition_all_wide

#Now we can plot the results using box and whiskers
average_word_count_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_word_count"), names_to = "gaze_condition", values_to = "average_word_count") %>%
  ggplot(aes(x = gaze_condition, y = average_word_count, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "Comparison of Average Word Count by Gaze Condition", x = "Gaze Condition", y = "Average Word Count") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```

We may have a significant difference so we should test and see.

```{r}
# Now we can do the wilcoxon test
word_count_wilcox <- average_word_count_by_condition_all %>%
  rstatix::wilcox_test(average_word_count ~ gaze_condition, paired = TRUE)
word_count_wilcox.p <- word_count_wilcox %>%
  add_x_position(x = "gaze_condition")
word_count_wilcox
print("Z for Word Count")
compute_wilcox_summary(word_count_wilcox)

x_data <- average_word_count_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_word_count)
y_data <- average_word_count_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_word_count)
z_word_count <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Word Count:", z_word_count))
```
Looks like we have a significant difference, let's plot it.

```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 100, 
  label = paste("p-value:", formatC(word_count_wilcox$p, format = "f", digits = 6))
)

average_word_count_by_condition_all
ggplot(average_word_count_by_condition_all, aes(x = gaze_condition, y = average_word_count, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "Comparison of Average Words Spoken Per Target Set by Gaze Condition", x = "Gaze Condition", y = "Average Word Count") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```

So people spoke less when they saw the student's gaze. Let's see whether this made them give more 'directional' instructions. Let's first check the percentage of directional instructions.

Directional words:
- right
- left
- down
- up
- straight
- above
- below
- forward
- behind
- back
- between
- over
- under
- towards

Okay so single file test first.

```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
directional_words <- c("right", "left", "down", "up", "straight", "above", "below", "forward", "behind", "back", "between", "over", "under", "towards")


word_count_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/word_counts.csv"
print(word_count_path)

word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))

# Remove start_round and end_round columns
subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))
word_count_data

word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
word_count_dt

# Need to compute the task_type column based on mod
# mod 3 == 1 -> "Locate"
# mod 3 == 2 -> "Puzzle"
# mod 3 == 0 -> "Remaining"

word_count_dt <- word_count_dt %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
word_count_dt

# Now we calculate the total number of directional words for each condition
word_count_dt_grouped <- word_count_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_directional_words = sum(count[word %in% directional_words], na.rm = TRUE)  # Total number of directional words
  ) %>%
  ungroup()
word_count_dt_grouped
# Now calculate the percentage of directional words for each condition
average_directional_words_by_condition <- word_count_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_directional_words = mean(total_directional_words, na.rm = TRUE)
  ) %>%
  ungroup()
average_directional_words_by_condition


```

Now we do it for all files

```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
#directional_words <- c("right", "left", "down", "up", "straight", "above", "below", "forward", "behind", "back", "between", "over", "under", "towards")
directional_words <- c("right", "left", "down", "up")
# Process each word count file and calculate the average percentage of directional words for each condition
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_directional_words_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
    
    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"
    
    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    word_count_dt
    
    # Now we calculate the total number of directional words for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_directional_words = sum(count[word %in% directional_words], na.rm = TRUE)  # Total number of directional words
      ) %>%
      ungroup()
    word_count_dt_grouped
    # Now calculate the percentage of directional words for each condition
    average_directional_words_by_condition <- word_count_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_directional_words = mean(total_directional_words, na.rm = TRUE)
      ) %>%
      ungroup()
    average_directional_words_by_condition
})

average_directional_words_by_condition_all
```
Now we plot it

```{r}

# Now pivot it wider
average_directional_words_by_condition_all_wide <- average_directional_words_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_directional_words,
              names_prefix = "average_directional_words_") %>%
  as.data.frame()

average_directional_words_by_condition_all_wide

#Now we can plot the results using box and whiskers

average_directional_words_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_directional_words"), names_to = "gaze_condition", values_to = "average_directional_words") %>%
  ggplot(aes(x = gaze_condition, y = average_directional_words, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +
  labs(title = "Comparison of Average Percentage of Directional Words by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Directional Words") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))


```

We may have a significant difference so we should test and see.

```{r}

# Now we can do the wilcoxon test
directional_words_wilcox <- average_directional_words_by_condition_all %>%
  rstatix::wilcox_test(average_directional_words ~ gaze_condition, paired = TRUE)
directional_words_wilcox.p <- directional_words_wilcox %>%
  add_x_position(x = "gaze_condition")
directional_words_wilcox

print("Z for Directional Words")
compute_wilcox_summary(directional_words_wilcox)
compute_wilcoxon_signed_rank_summary(directional_words_wilcox)
x_data <- average_directional_words_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_directional_words)
y_data <- average_directional_words_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_directional_words)

z_directional_words <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
print(paste("Z for Directional Words:", z_directional_words))
```

Looks like we have a significant difference, let's plot it.

```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 5, 
  label = paste("p-value:", formatC(directional_words_wilcox$p, format = "f", digits = 6))
)

average_directional_words_by_condition_all
ggplot(average_directional_words_by_condition_all, aes(x = gaze_condition, y = average_directional_words, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 6), breaks = seq(0, 6, 1)) +
  labs(title = "Comparison of Average Number of Directional Words Per TS by Gaze Condition", x = "Gaze Condition", y="Average Number of Directional Words") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```


We want to do similar things for simple affirmations and negations. Let's first check the percentage of affirmations and negations.

Affirmations:
- yes
- correct
- right
- true
- sure
- okay
- yep
- yeah
- affirmative
- indeed
- absolutely
- agreed
- confirmed
- fine
- good
- great
- right
- understood
- OK
- agreed
- fine
- good

Negations:
- no
- incorrect
- wrong
- false
- not
- never
- nope
- nah

Single file test first

```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
affirmations <- c("yes", "correct", "right", "true", "sure", "okay", "yep", "yeah", "affirmative", "indeed", "absolutely", "agreed", "confirmed", "fine", "good", "great", "right", "understood", "OK", "agreed", "fine", "good")
negations <- c("no", "incorrect", "wrong", "false", "not", "never", "nope", "nah")

word_count_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/word_counts.csv"
print(word_count_path)

word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))

# Remove start_round and end_round columns
subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))
word_count_data

word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
word_count_dt

# Need to compute the task_type column based on mod
# mod 3 == 1 -> "Locate"
# mod 3 == 2 -> "Puzzle"
# mod 3 == 0 -> "Remaining"

word_count_dt <- word_count_dt %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
word_count_dt

# Now we calculate the total number of affirmations and negations for each condition
word_count_dt_grouped <- word_count_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_affirmations = sum(count[word %in% affirmations], na.rm = TRUE),  # Total number of affirmations
    total_negations = sum(count[word %in% negations], na.rm = TRUE)  # Total number of negations
  ) %>%
  ungroup()
word_count_dt_grouped
# Now calculate the percentage of affirmations and negations for each condition
average_affirmations_negations_by_condition <- word_count_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_affirmations = mean(total_affirmations, na.rm = TRUE),
    average_negations = mean(total_negations, na.rm = TRUE)
  ) %>%
  ungroup()
average_affirmations_negations_by_condition

```

Now we do it for all files


```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
affirmations <- c("yes", "correct", "right", "true", "sure", "okay", "yep", "yeah", "affirmative", "indeed", "absolutely", "agreed", "confirmed", "fine", "good", "great", "right", "understood", "OK", "agreed", "fine", "good")
negations <- c("no", "incorrect", "wrong", "false", "not", "never", "nope", "nah")
# Process each word count file and calculate the average percentage of directional words for each condition
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_affirmations_negations_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
    
    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"
    
    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    word_count_dt
    
    # Now we calculate the total number of affirmations and negations for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_affirmations = sum(count[word %in% affirmations], na.rm = TRUE),  # Total number of affirmations
        total_negations = sum(count[word %in% negations], na.rm = TRUE)  # Total number of negations
      ) %>%
      ungroup()
    
    # Now calculate the percentage of affirmations and negations for each condition
    average_affirmations_negations_by_condition <- word_count_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_affirmations = mean(total_affirmations, na.rm = TRUE),
        average_negations = mean(total_negations, na.rm = TRUE)
      ) %>%
      ungroup()
    
    average_affirmations_negations_by_condition
  })
average_affirmations_negations_by_condition_all
```

Now we plot it (we will want to plot affirmations and negations separately)
Let's start with affirmations

```{r}

# Now pivot it wider
average_affirmations_negations_by_condition_all_wide <- average_affirmations_negations_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_affirmations,
              names_prefix = "average_affirmations_") %>%
  as.data.frame()

average_affirmations_negations_by_condition_all_wide

#Now we can plot the results using box and whiskers

average_affirmations_negations_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_affirmations"), names_to = "gaze_condition", values_to = "average_affirmations") %>%
  ggplot(aes(x = gaze_condition, y = average_affirmations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +
  labs(title = "Comparison of Average Percentage of Affirmations by Gaze Condition", x = "Gaze Condition", y="Average Number of Affirmations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```

We may have a significant difference so we should test and see.


```{r}

# Now we can do the wilcoxon test
affirmations_wilcox <- average_affirmations_negations_by_condition_all %>%
  rstatix::wilcox_test(average_affirmations ~ gaze_condition, paired = TRUE)
affirmations_wilcox.p <- affirmations_wilcox %>%
  add_x_position(x = "gaze_condition")
affirmations_wilcox

print("Z for Affirmations")
compute_wilcox_summary(affirmations_wilcox)

average_affirmations_negations_by_condition_all

x_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_affirmations)
y_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_affirmations)

z_affirmations <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
print(paste("Z for Affirmations:", z_affirmations))

```

Looks like we have a significant difference, let's plot it.


```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 8, 
  label = paste("p-value:", formatC(affirmations_wilcox$p, format = "f", digits = 6))
)

average_affirmations_negations_by_condition_all

ggplot(average_affirmations_negations_by_condition_all, aes(x = gaze_condition, y = average_affirmations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 1)) +
  labs(title = "Comparison of Average Number of Affirmations Per Target Set by Gaze Condition", x = "Gaze Condition", y= "Average Number of Affirmations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)

```

Now we do the same for negations

```{r}

# Now pivot it wider
average_affirmations_negations_by_condition_all_wide <- average_affirmations_negations_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_negations,
              names_prefix = "average_negations_") %>%
  as.data.frame()

average_affirmations_negations_by_condition_all_wide

#Now we can plot the results using box and whiskers

average_affirmations_negations_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_negations"), names_to = "gaze_condition", values_to = "average_negations") %>%
  ggplot(aes(x = gaze_condition, y = average_negations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 5), breaks = seq(0, 5, 1)) +
  labs(title = "Comparison of Average Number of Corrections by Gaze Condition", x = "Gaze Condition", y= "Average Number of Negations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))


```

We may have a significant difference so we should test and see.

```{r}

# Now we can do the wilcoxon test

negations_wilcox <- average_affirmations_negations_by_condition_all %>%
  rstatix::wilcox_test(average_negations ~ gaze_condition, paired = TRUE)

negations_wilcox.p <- negations_wilcox %>%
  add_x_position(x = "gaze_condition")
negations_wilcox

print("Z for Negations")
compute_wilcox_summary(negations_wilcox)

x_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_negations)
y_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_negations)

z_negations <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Negations:", z_negations))

```

Looks like we have a significant difference, let's plot it.

```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 4, 
  label = paste("p-value:", formatC(negations_wilcox$p, format = "f", digits = 6))
)

average_affirmations_negations_by_condition_all

ggplot(average_affirmations_negations_by_condition_all, aes(x = gaze_condition, y = average_negations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 4), breaks = seq(0, 4, 1)) +
  labs(title = "Comparison of Average Number of Corrections Per Target Set by Gaze Condition", x = "Gaze Condition", y= "Average Number of Negations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)

```

Key takeaways -
- People spoke less overall when they saw the student's gaze
- People needed to correct the student more when they did not see the student's gaze
- People needed to use more directional words to guide the student when they did not see the student's gaze
- People needed to use more affirmations when they did not see the student's gaze
- People looked at the student more when they did not see the student's gaze
- We found decreased usage of descriptive guidance strategies and increased usage of spatial-based and minimal guidance strategies when the student's gaze was visible.


Load up the instructional strategies data
```{r}
# Load the instructional strategies data
instructional_strategies_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/instructional_strategies.csv"

instructional_strategies_data <- read_csv(instructional_strategies_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
# Rename round to round_id and task to task_id
instructional_strategies_data
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
instructional_strategies_data_dt <- instructional_strategies_data %>% setDT() %>% setkey("subject_id", "round_id", "task_id")


terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)



```


Now we want to just do the background questionnaire data

```{r}
# Load the background questionnaire data
background_questionnaire_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/questionnaireData.csv"

background_questionnaire_data <- read_csv(background_questionnaire_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
background_questionnaire_data

# Now we just want to summarize the median age and get the count of each gender (female, male, non-binary), we also need a count of vr_experience, tutor_experience, and geology_experience, we also need the min and max age
background_questionnaire_summary <- background_questionnaire_data %>%
  summarise(
    median_age = median(age, na.rm = TRUE),
    min_age = min(age, na.rm = TRUE),
    max_age = max(age, na.rm = TRUE),
    count_female = sum(gender == "Female", na.rm = TRUE),
    count_male = sum(gender == "Male", na.rm = TRUE),
    count_non_binary = sum(gender == "Non-Binary", na.rm = TRUE),
    count_vr_experience = sum(vr_experience, na.rm = TRUE),
    count_tutor_experience = sum(tutor_experience, na.rm = TRUE),
    count_geology_experience = sum(geology_experience, na.rm = TRUE)
  )
background_questionnaire_summary

```




We now want alllllllll the plots in one place with patchwork that are in paper-friendly format so for example:
```{r duration-plot, fig.height=3, fig.width=3}
# Plotting the results. We don't want the p-value label. We don't want to have the x-axis labeled as Gaze Condition. The x-axis labels should instead be "Gaze Off" and "Gaze On". We do not want the legend. We do not want the title. Finally, we want to have error bars show up as well.

wide_mean_durations
# Calculate summary statistics
summary_round_durations <- round_durations %>%
  group_by(gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE),
            sd_duration = sd(round_duration, na.rm = TRUE))

summary_round_durations
# Plot using wide_mean_durations for the boxplot and jitter, and summary_mean_durations for error bars with horizontal caps
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot(width = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5) +
  geom_errorbar(data = summary_round_durations, aes(ymin = mean_duration - sd_duration, ymax = mean_duration + sd_duration, x = gaze_condition), width = 0.6, color = "black") +
  # Adding horizontal end caps
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration - sd_duration, yend = mean_duration - sd_duration), color = "black") +
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration + sd_duration, yend = mean_duration + sd_duration), color = "black") +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 100)) +
  labs(title = "", x = "", y = "Mean Duration (in seconds)") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))
```

```{r word-count-plot, fig.height=3, fig.width=3}
# Summary statistics
word_count_by_condition_all
summary_word_count <- word_count_by_condition_all %>%
  group_by(gaze_condition) %>%
  summarise(mean_word_count = mean(total_words, na.rm = TRUE),
            sd_word_count = sd(total_words, na.rm = TRUE))
average_word_count_by_condition_all
summary_word_count
word_count_by_condition_all
# Plot with errors bars
ggplot(word_count_by_condition_all, aes(x = gaze_condition, y = average_word_count, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  #geom_errorbar(data = summary_word_count, aes(ymin = mean_word_count - sd_word_count, ymax = mean_word_count + sd_word_count, x = gaze_condition), width = 0.6, color = "black") +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "", x = "", y = "Average Word Count") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))
```


Finally, combine with patchwork.
```{r combined-all-plots, fig.height=3, fig.width=3}
```


Load all the data and combine the segments and timings into one data frame.
```{r}

# Load the data
directory_path <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\QWE-042"
timings_path <- file.path(directory_path, "timings_updated.csv")
segments_path <- file.path(directory_path, "segments.csv")

timings <- read_csv(timings_path, show_col_types = FALSE)
segments <- read_csv(segments_path, show_col_types = FALSE)

# Process segments and join with timings
segments_per_round <- segments %>%
  mutate(text = tolower(text),  # Convert text to lower case
         text = str_trim(text),  # Trim whitespace
         text = replace_contraction(text),  # Replace contractions using textclean
         text = str_replace_all(text, "[[:punct:]]", "")) %>%  # Remove punctuation
  mutate(across(c(start, end), as.numeric))
  

timings_dt <- setDT(timings)
segments_per_round_dt <- setDT(segments_per_round) %>% .[timings_dt, on = .(start <= end, end >= start), allow.cartesian = TRUE]
segments_per_round <- segments_per_round_dt %>% as.data.frame()
    
  #right_join(timings, by = character()) %>%
  #filter(start >= start.x & end <= end.x) %>%
  #select(round, task, start = start.x, end = end.x, text)
segments_per_round <- segments_per_round %>% relocate(round, task, start, end, text)
# Combine text for each round and task
combined_text_per_round <- segments_per_round %>%
  group_by(round, task, start, end) %>%
  summarise(combined_text = paste(text, collapse = " "), .groups = 'drop')  # Combine text entries into a single string

combined_text_per_round
# Save the result to a new CSV file
#write_csv(combined_text_per_round, file.path(directory_path, "combined_text_per_round.csv"))

# Print path to confirm where the file is saved
#print(file.path(directory_path, "segments_per_round.csv"))

```


Now we want to do it for all the participants using map_dfr
```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
combined_segments_per_round_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/timings_updated.csv$") %>%
  map_dfr(.f = function(timings_updated_path) {
    print(timings_updated_path)
    segments_path <- str_replace(timings_updated_path, "timings_updated.csv", "segments.csv")
    timings <- read_csv(timings_updated_path, show_col_types = FALSE)
    segments <- read_csv(segments_path, show_col_types = FALSE)
    
    # Process segments and join with timings
    segments_per_round <- segments %>%
      mutate(text = tolower(text),  # Convert text to lower case
             text = str_trim(text),  # Trim whitespace
             text = replace_contraction(text),  # Replace contractions using textclean
             text = str_replace_all(text, "[[:punct:]]", "")) %>%  # Remove punctuation
      mutate(across(c(start, end), as.numeric))
      
    
    timings_dt <- setDT(timings)
    segments_per_round_dt <- setDT(segments_per_round) %>% .[timings_dt, on = .(start <= end, end >= start), allow.cartesian = TRUE]
    segments_per_round <- segments_per_round_dt %>% as.data.frame()
        
      #right_join(timings, by = character()) %>%
      #filter(start >= start.x & end <= end.x) %>%
      #select(round, task, start = start.x, end = end.x, text)
    segments_per_round <- segments_per_round %>% relocate(round, task, start, end, text)
    # Combine text for each round and task
    combined_text_per_round <- segments_per_round %>%
      group_by(round, task, start, end) %>%
      summarise(combined_text = paste(text, collapse = " "), .groups = 'drop')  # Combine text entries into a single string
    # Now we just want to add in the subject id to the round and task
    subject_id <- substr(basename(dirname(timings_updated_path)), 1, 7)
    combined_text_per_round <- combined_text_per_round %>% mutate(subject_id = subject_id)
    combined_text_per_round
  })
#move subject_id before round
combined_segments_per_round_all <- combined_segments_per_round_all %>% relocate(subject_id, .before = round)
combined_segments_per_round_all

# Save the result to a new CSV file
write_csv(combined_segments_per_round_all, file.path(path_to_files, "combined_segments_per_round_all.csv"))
```

We also want to have a file that just is subject_rounds written to a csv file as that includes every subject.
```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
subject_rounds_dt
write_csv(subject_rounds_dt, file.path(path_to_files, "subject_rounds.csv"))
```



Now we want to load up instructional_strategies.csv in the root and merge with subject_rounds_dt on round = round_id and task = task_id (the columns in instructional strategies will need to be renamed first)

```{r}
instructional_strategies_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/instructional_strategies.csv"
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
instructional_strategies_data <- read_csv(instructional_strategies_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
instructional_strategies_data <- instructional_strategies_data %>% rename(round_id = round, task_id = task)
instructional_strategies_data_dt <- instructional_strategies_data %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
instructional_strategies_data_dt
instructional_strategies_data_dt_merged <- merge(instructional_strategies_data_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)
instructional_strategies_data_dt_merged
# Rename instructional strategy values 'dir' to 'Spatial', 'min' to 'Minimal', 'Max' to 'Descriptive'
instructional_strategies_data_dt_merged <- instructional_strategies_data_dt_merged %>%
  mutate(across(c("instructional_strategy"), ~str_replace_all(., c("dir" = "Spatial", "min" = "Minimal", "max" = "Descriptive"))))
instructional_strategies_data_dt_merged

# Find all NA values in gaze condition and instructional strategy
instructional_strategies_data_dt_merged_na <- instructional_strategies_data_dt_merged %>% filter(is.na(gaze_condition) | is.na(instructional_strategy))
instructional_strategies_data_dt_merged_na
# now we want to show a multi bar plot of the count of each instructional strategy within each gaze condition (on or off), so it should show on the x-axis On and Off and each bar should be the count of each instructional strategy, also the x-axis gaze group label on/off should be on the bottom)
instructional_strategies_data_dt_merged %>%
  ggplot(aes(x = gaze_condition, fill = instructional_strategy)) +
  geom_bar(position = "dodge") +
  labs(title = "", x = "Gaze Condition", y = "# of Trials Where Subjects Used This Strategy") +
  theme_minimal() +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))

```


So this gives us a multi group bar chart of the count of each instructional strategy within each gaze condition, but now we want to group it further by the task type (Locate, Alignment, Task) and the task condition. The task type can be figured out from the task_id, specifically if task_id %% 3 == 1 then it is Locate, if task_id %% 3 == 2 then it is Alignment, and if task_id %% 3 == 0 then it is Match. Furthermore, thet ask_condition is already in the correct format. We will want to facet by task type and task condition.
```{r}
# Now we want to group it further by the task type and task condition
instructional_strategies_data_dt_merged_tt_tc <- instructional_strategies_data_dt_merged %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Alignment"] %>% .[task_id %% 3 == 0, task_type := "Match"] %>% as.data.frame()
instructional_strategies_data_dt_merged_tt_tc

# We want to filter out the Locate task from the facet for now. We'll just look at it later individually.
instructional_strategies_data_dt_merged_tt_tc_nl <- instructional_strategies_data_dt_merged_tt_tc %>% filter(task_type != "Locate")
instructional_strategies_data_dt_merged_tt_tc_nl
# Now we want to facet by task type and task condition, the task condition should be labelled from "Correct" to "Student was Correct after instruction"
# and "Incorrect" to "Student made an Error after instruction", also it would be easier to swap the gaze condition and task condition so the top level group is gaze and the second level group is task condition
# Now we want to group it further by the task type and task condition
instructional_strategies_data_dt_merged_tt_tc <- instructional_strategies_data_dt_merged %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>%
  .[task_id %% 3 == 1, task_type := "Locate"] %>%
  .[task_id %% 3 == 2, task_type := "Alignment"] %>%
  .[task_id %% 3 == 0, task_type := "Match"] %>%
  as.data.frame()
instructional_strategies_data_dt_merged_tt_tc

# We want to filter out the Locate task from the facet for now. We'll just look at it later individually.
instructional_strategies_data_dt_merged_tt_tc_nl <- instructional_strategies_data_dt_merged_tt_tc %>% filter(task_type != "Locate")
instructional_strategies_data_dt_merged_tt_tc_nl

# Now we want to facet by task type and task condition, the task condition should be labelled from "Correct" to "Student was Correct after instruction"
# and "Incorrect" to "Student made an Error after instruction", also it would be easier to swap the gaze condition and task condition so the top level group is gaze and the second level group is task condition
instructional_strategies_data_dt_merged_tt_tc_nl %>%
  mutate(task_condition = recode(task_condition, 
                                 "Correct" = "Student Correct", 
                                 "Incorrect" = "Student Error"),
         gaze_condition = recode(gaze_condition, 
                                 "On" = "Gaze On", 
                                 "Off" = "Gaze Off")) %>%
  ggplot(aes(x = task_condition, fill = instructional_strategy)) +
  geom_bar(position = "dodge") +
  labs(title = "", x = "", y = "# of Trials Where Subjects Used This Strategy") +
  theme_minimal() +
  scale_x_discrete(labels = c("Student Correct", "Student Error")) +
  facet_grid(task_type ~ gaze_condition) +
  theme(
    strip.text = element_text(size = 10, face = "bold"),
    axis.text.x = element_text(size = 10, face = "bold", angle = 45, hjust = 1),
    axis.text.y = element_text(size = 10, face = "bold"),
    axis.title = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 10)
  )
```