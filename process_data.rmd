
Package installation
```{r}
install.packages("dplyr")
install.packages("tidyr")
install.packages("readr")
install.packages("pwr")
install.packages("fs")
install.packages("purrr")
install.packages("data.table")
install.packages("janitor")
install.packages("magrittr")
install.packages("tidyverse")
install.packages("stringr")
install.packages("ggpubr")
install.packages("ggplot2")
install.packages("rstatix")
install.packages("dtplyr")
install.packages("tidyverse")
install.packages("textclean")
install.packages("ARTool")
install.packages("emmeans")
```



```{r}
# List of required packages
required_packages <- c(
  "tidyr", "fs", "dplyr", "readr", "purrr", "data.table", "pwr", "janitor", 
  "magrittr", "tidyverse", "stringr", "ggpubr", "ggplot2", "rstatix", "qqplotr", 
  "here", "broom", "knitr", "gtools", "GGally", "correlation", "png", "patchwork", 
  "dtplyr", "textclean", "coin", "rcompanion", "ARTool", "emmeans", "multcomp"
)

# Install missing packages
install_if_missing <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

# Apply function to all required packages
lapply(required_packages, install_if_missing)

# Load all packages
invisible(lapply(required_packages, library, character.only = TRUE))

```

Initialize commonly used libraries, variables, and functions.
```{r}

library(tidyr)
library(fs)
library(dplyr)
library(readr)
library(purrr)
library(data.table)
library(pwr)
library(janitor)
library(magrittr) 
library(tidyverse)
library(stringr)
library(ggpubr)
library(ggplot2)
library(rstatix)
library(qqplotr)
library(here)
library(tidyverse)
library(qqplotr)
library(rstatix)
library(broom)
library(knitr)
library(ggpubr)
library(gtools)
library(GGally)
library(correlation)
library(png)
library(patchwork)
library(dtplyr)
library(tidyverse)
library(textclean)
# Function to process each directory
library(tidyverse)
library(stringr)
library(patchwork)
library(coin)
library(rcompanion)
library(rstatix)
library(ARTool)
library(emmeans)

na_strings <- c("NA", "N/A", "na", "n/a", "NULL", "null", "None", "none", "NaN", "nan", "Inf", "-Inf", "inf", "-inf", "", " ")
names_with_indices <- function(data) {
  names_data <- names(data)
  indices <- seq_along(names_data)
  names_indexed <- paste(indices, names_data, sep = ": ")
  return(names_indexed)
}
# Function to fix the header and write a new CSV file
fix_csv_header_id <- function(file_path) {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append ',id' to the first line
  corrected_first_line <- paste(first_line, "id", sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), "_fixed.csv")
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

# Function to fix the header and write a new CSV file with additional columns
fix_csv_header_cols <- function(file_path, additional_columns, suffix = "_fixed.csv") {
  # Read the first line to check headers
  con <- file(file_path, open = "r")
  first_line <- readLines(con, n = 1)
  close(con)
  
  # Append additional columns to the first line
  corrected_first_line <- paste(first_line, additional_columns, sep = ",")
  
  # Read the remaining lines of the original file
  remaining_lines <- read_lines(file_path, skip = 1)
  
  # Combine the corrected first line with the remaining lines
  corrected_content <- c(corrected_first_line, remaining_lines)
  
  # Define the new file path
  new_file_path <- paste0(sub(".csv", "", file_path), suffix)
  
  # Write the corrected content to the new file
  writeLines(corrected_content, new_file_path)
  
  return(new_file_path)
}

# Function to compute Z value and print Z, N, and p
compute_wilcox_summary <- function(wilcox_result) {
  # Calculate Z value using the normal approximation
  # Z = (W - mean(W)) / sd(W)
  # mean(W) = n1 * n2 / 2
  # sd(W) = sqrt(n1 * n2 * (n1 + n2 + 1) / 12)
  wilcox_result <- wilcox_result %>%
    mutate(
      mean_W = n1 * n2 / 2,
      sd_W = sqrt(n1 * n2 * (n1 + n2 + 1) / 12),
      Z = (statistic - mean_W) / sd_W,
      N = n1 + n2
    )
  
  # Print the results in a more readable format
  print(sprintf("Z = %.5f, N = %d, p = %.5f", wilcox_result$Z, wilcox_result$N, wilcox_result$p))
  
  
  # Return the modified result with Z value
  return(wilcox_result)
}

# Function to compute Z value for Wilcoxon signed-rank test results from rstatix
compute_wilcoxon_signed_rank_summary <- function(wilcox_result) {
  # Extract necessary components from the result
  n <- wilcox_result$n1  # Assuming n1 and n2 are the same because it's a paired test
  T <- wilcox_result$statistic
  
  # Calculate mean and standard error under H0
  mn = n * (n + 1) / 4
  se = sqrt(n * (n + 1) * (2 * n + 1) / 24)
  
  # Continuity correction
  correction = 0.5
  
  # Calculate Z value
  Z = (T - mn - correction) / se
  
  # Print formatted results
  print(sprintf("Z = %.4f, N = %d, p = %.5f", Z, n, wilcox_result$p))
  
  # Return a list with the computed values
  return(list(Z = Z, N = n, p = wilcox_result$p))
}


#path_to_files <- "C:\\DataTest\\"
#path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\expLogs\\"
path_to_files <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\"
options("digits.secs"=6)
```







Creates the round times for each subject, we'll use this a lot.

```{r}
subject_ratings <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3} rating_fixed\\.csv$") %>%
  map_dfr(.f = function(subjects_file) {
    print(subjects_file)
    subject_row <- read_csv(subjects_file, na = na_strings, show_col_types = FALSE) %>% clean_names()
    subject_row
  }) 
path_to_files
subject_ratings

subject_ratings <- subject_ratings %>% filter(round_id != 9) %>% mutate(subject_id = substr(subject_id, 1, 7))

# Ensure subject_ratings is a data frame
subject_ratings <- as.data.frame(subject_ratings)

# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
# We also need task_condition and gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            max_unity_log_time = max(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition),
            task_condition = first(task_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(round_start = lag(max_unity_log_time),
         round_end = min_unity_log_time)

# Compute the times for all the rounds and set the start time for the first round to 0 (as it starts logging on the first task)
rounds_with_times <- setDT(round_durations) %>% 
  .[is.na(round_start), round_start:= 0] %>% 
  .[,min_unity_log_time:=NULL] %>% 
  .[,max_unity_log_time:=NULL] %>% as.data.frame()


# Add a sequential task_id for each entry, incrementing by 3 for each round
# First, expand each round into three tasks
rounds_expanded <- rounds_with_times %>%
  group_by(subject_id, round_id) %>%
  slice(rep(1:n(), each = 3)) %>%
  ungroup()

# Now, assign task_id incrementally across rounds
rounds_expanded <- rounds_expanded %>%
  group_by(subject_id) %>%
  mutate(task_id = row_number()) %>%
  ungroup() %>%
  relocate(task_id, .after = round_id)

# Adjust task_condition based on task_id being the first in each round
rounds_expanded <- rounds_expanded %>%
  group_by(subject_id, round_id) %>%
  mutate(task_condition = if_else(task_id %% 3 == 1, "Incorrect", task_condition)) %>%
  ungroup()

# Adjust task_condition for the second task in each round based on the third task's condition
subject_rounds <- rounds_expanded %>%
  group_by(subject_id, round_id) %>%
  mutate(task_condition = if_else(task_id %% 3 == 2, 
                                  if_else(lead(task_condition, default = "Correct") == "Correct", "Incorrect", "Correct"), 
                                  task_condition)) %>%
  ungroup()

subject_rounds
```


```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
combined_segments_per_round_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/timings_updated.csv$") %>%
  map_dfr(.f = function(timings_updated_path) {
    print(timings_updated_path)
    segments_path <- str_replace(timings_updated_path, "timings_updated.csv", "segments.csv")
    timings <- read_csv(timings_updated_path, show_col_types = FALSE)
    segments <- read_csv(segments_path, show_col_types = FALSE)
    
    # Process segments and join with timings
    segments_per_round <- segments %>%
      mutate(text = tolower(text),  # Convert text to lower case
             text = str_trim(text),  # Trim whitespace
             text = replace_contraction(text),  # Replace contractions using textclean
             text = str_replace_all(text, "[[:punct:]]", "")) %>%  # Remove punctuation
      mutate(across(c(start, end), as.numeric))
      
    
    timings_dt <- setDT(timings)
    segments_per_round_dt <- setDT(segments_per_round) %>% .[timings_dt, on = .(start <= end, end >= start), allow.cartesian = TRUE]
    segments_per_round <- segments_per_round_dt %>% as.data.frame()
        
      #right_join(timings, by = character()) %>%
      #filter(start >= start.x & end <= end.x) %>%
      #select(round, task, start = start.x, end = end.x, text)
    segments_per_round <- segments_per_round %>% relocate(round, task, start, end, text)
    # Combine text for each round and task
    combined_text_per_round <- segments_per_round %>%
      group_by(round, task, start, end) %>%
      summarise(combined_text = paste(text, collapse = " "), .groups = 'drop')  # Combine text entries into a single string
    # Now we just want to add in the subject id to the round and task
    subject_id <- substr(basename(dirname(timings_updated_path)), 1, 7)
    combined_text_per_round <- combined_text_per_round %>% mutate(subject_id = subject_id)
    combined_text_per_round
  })
#move subject_id before round
combined_segments_per_round_all <- combined_segments_per_round_all %>% relocate(subject_id, .before = round)
combined_segments_per_round_all

# Save the result to a new CSV file
write_csv(combined_segments_per_round_all, file.path(path_to_files, "combined_segments_per_round_all.csv"))


#So instead load this from the csv now
combined_segments_per_round_all_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/combined_segments_per_round_all.csv"
combined_segments_per_round_all <- read_csv(combined_segments_per_round_all_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
combined_segments_per_round_all
```

Now we want to load up instructional_strategies.csv in the root and merge with subject_rounds_dt on round = round_id and task = task_id (the columns in instructional strategies will need to be renamed first)

```{r}
instructional_strategies_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/instructional_strategies.csv"
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
instructional_strategies_data <- read_csv(instructional_strategies_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
instructional_strategies_data <- instructional_strategies_data %>% rename(round_id = round, task_id = task)
instructional_strategies_data_dt <- instructional_strategies_data %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
instructional_strategies_data_dt
instructional_strategies_data_dt_merged <- merge(instructional_strategies_data_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)
instructional_strategies_data_dt_merged
# Rename instructional strategy values 'dir' to 'Spatial', 'min' to 'Minimal', 'Max' to 'Descriptive'
instructional_strategies_data_dt_merged <- instructional_strategies_data_dt_merged %>%
  mutate(across(c("instructional_strategy"), ~str_replace_all(., c("dir" = "Spatial", "min" = "Minimal", "max" = "Descriptive"))))
instructional_strategies_data_dt_merged 

# Find all NA values in gaze condition and instructional strategy
instructional_strategies_data_dt_merged_na <- instructional_strategies_data_dt_merged %>% filter(is.na(gaze_condition) | is.na(instructional_strategy))
instructional_strategies_data_dt_merged_na
# now we want to show a multi bar plot of the count of each instructional strategy within each gaze condition (on or off), so it should show on the x-axis On and Off and each bar should be the count of each instructional strategy, also the x-axis gaze group label on/off should be on the bottom)
instructional_strategies_data_dt_merged %>%
  ggplot(aes(x = gaze_condition, fill = instructional_strategy)) +
  geom_bar(position = "dodge") +
  labs(title = "", x = "Gaze Condition", y = "# of Trials Where Subjects Used This Strategy") +
  theme_minimal() +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))

```


So this gives us a multi group bar chart of the count of each instructional strategy within each gaze condition, but now we want to group it further by the task type (Locate, Alignment, Task) and the task condition. The task type can be figured out from the task_id, specifically if task_id %% 3 == 1 then it is Locate, if task_id %% 3 == 2 then it is Alignment, and if task_id %% 3 == 0 then it is Match. Furthermore, thet ask_condition is already in the correct format. We will want to facet by task type and task condition.
```{r}
# Now we want to group it further by the task type and task condition
instructional_strategies_data_dt_merged_tt_tc <- instructional_strategies_data_dt_merged %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Alignment"] %>% .[task_id %% 3 == 0, task_type := "Match"] %>% as.data.frame()
instructional_strategies_data_dt_merged_tt_tc

# We want to filter out the Locate task from the facet for now. We'll just look at it later individually.
instructional_strategies_data_dt_merged_tt_tc_nl <- instructional_strategies_data_dt_merged_tt_tc %>% filter(task_type != "Locate")
instructional_strategies_data_dt_merged_tt_tc_nl
# Now we want to facet by task type and task condition, the task condition should be labelled from "Correct" to "Student was Correct after instruction"
# and "Incorrect" to "Student made an Error after instruction", also it would be easier to swap the gaze condition and task condition so the top level group is gaze and the second level group is task condition
# Now we want to group it further by the task type and task condition
instructional_strategies_data_dt_merged_tt_tc <- instructional_strategies_data_dt_merged %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>%
  .[task_id %% 3 == 1, task_type := "Locate"] %>%
  .[task_id %% 3 == 2, task_type := "Alignment"] %>%
  .[task_id %% 3 == 0, task_type := "Match"] %>%
  as.data.frame()
instructional_strategies_data_dt_merged_tt_tc

# We want to filter out the Locate task from the facet for now. We'll just look at it later individually.
instructional_strategies_data_dt_merged_tt_tc_nl <- instructional_strategies_data_dt_merged_tt_tc %>% filter(task_type != "Locate")
instructional_strategies_data_dt_merged_tt_tc_nl
# Now we want to facet by task type and task condition, the task condition should be labelled from "Correct" to "Student was Correct after instruction"
# and "Incorrect" to "Student made an Error after instruction", also it would be easier to swap the gaze condition and task condition so the top level group is gaze and the second level group is task condition
instructional_strategies_data_dt_merged_tt_tc_nl %>%
  mutate(task_condition = recode(task_condition, 
                                 "Correct" = "Student Correct", 
                                 "Incorrect" = "Student Error"),
         gaze_condition = recode(gaze_condition, 
                                 "On" = "Gaze On", 
                                 "Off" = "Gaze Off")) %>%
  ggplot(aes(x = task_condition, fill = instructional_strategy)) +
  geom_bar(position = "dodge") +
  labs(title = "", x = "", y = "# of Trials Where Subjects Used This Strategy") +
  theme_minimal() +
  scale_x_discrete(labels = c("Student Correct", "Student Error")) +
  facet_grid(task_type ~ gaze_condition) +
  theme(
    strip.text = element_text(size = 10, face = "bold"),
    axis.text.x = element_text(size = 10, face = "bold", angle = 45, hjust = 1),
    axis.text.y = element_text(size = 10, face = "bold"),
    axis.title = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 10)
  )
```

So the instructional strategy table we want to join on anything we awnt to analyze is this one:
```{r}
instructional_strategies_data_dt_merged_tt_tc
# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(previous_time = lag(min_unity_log_time),
         round_duration = if_else(is.na(previous_time), min_unity_log_time, min_unity_log_time - previous_time))

round_durations

mean_durations <- round_durations %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE))

mean_durations

# Step 4: Reshape data from long to wide format
wide_mean_durations <- mean_durations %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = mean_duration,
              names_prefix = "mean_duration_")

wide_mean_durations


# Assuming wide_mean_durations is already loaded with your data
# Ensure it's in the correct format
wide_mean_durations <- wide_mean_durations %>%
  pivot_longer(cols = starts_with("mean_duration"), names_to = "gaze_condition", values_to = "mean_duration") %>%
  mutate(gaze_condition = sub("mean_duration_", "", gaze_condition))

# Make it a data frame
wide_mean_durations <- wide_mean_durations %>% as.data.frame()
wide_mean_durations
# Perform Wilcoxon signed-rank test using rstatix
duration_wilcox <- wide_mean_durations %>%
  rstatix::wilcox_test(mean_duration ~ gaze_condition, paired = TRUE)
duration_wilcox.p <- duration_wilcox |> add_x_position(x="gaze_condition")
duration_wilcox 
print("Z for Duration")
compute_wilcox_summary(duration_wilcox)
x_data <- wide_mean_durations %>% 
  filter(gaze_condition == "On") %>% 
  pull(mean_duration)
y_data <- wide_mean_durations %>% 
  filter(gaze_condition == "Off") %>% 
  pull(mean_duration)

z_duration <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
z_duration
print(paste("Z for Duration:", z_duration))

```


Join this table on the combined segments table using task_id = task

```{r}
# First rename round and task to round_id and task_id
library(dplyr)
combined_segments_per_round_all_renamed <- combined_segments_per_round_all %>% rename(round_id = round, task_id = task, task_start = start, task_end = end)
combined_segments_per_round_all_dt <- combined_segments_per_round_all_renamed %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
# Now join it on the instructional_strategies_data_dt_merged_tt_tc table
combined_segments_per_round_all_dt_merged <- merge(combined_segments_per_round_all_dt, instructional_strategies_data_dt_merged_tt_tc, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

combined_segments_per_round_all_dt_merged
#Remove the combined_text column
combined_segments_per_round_all_dt_merged <- combined_segments_per_round_all_dt_merged %>% dplyr::select(-combined_text)
combined_segments_per_round_all_dt_merged

# We want to get the duration of each task
combined_segments_per_round_all_dt_merged <- combined_segments_per_round_all_dt_merged %>% mutate(task_duration = task_start - task_end)
combined_segments_per_round_all_dt_merged

#Now reorder it so that task duration is before task_start
combined_segments_per_round_all_dt_merged <- combined_segments_per_round_all_dt_merged %>% relocate(task_duration, .before = task_start) %>% relocate(gaze_condition, .before = task_duration) %>% relocate(instructional_strategy, .before = task_duration) %>% relocate(task_condition, .before = instructional_strategy)
#truncate task_duration to 3 decimal places
combined_segments_per_round_all_dt_merged$task_duration <- round(combined_segments_per_round_all_dt_merged$task_duration, 3)
# Now join round_durations on the subject_id and round_id and we only want the round_durations column
#If you get an error you will need to run the section where it computes the round_durations later below
round_durations_dt <- round_durations %>% setDT() %>% setkey("subject_id", "round_id") %>% dplyr::select(subject_id, round_id, round_duration)
combined_segments_per_round_all_dt_merged <- merge(combined_segments_per_round_all_dt_merged, round_durations_dt, by = c("subject_id", "round_id"), all.x = TRUE)
combined_segments_per_round_all_dt_merged

#Round round_duration to 3 decimal places and move it after task_duration
combined_segments_per_round_all_dt_merged$round_duration <- round(combined_segments_per_round_all_dt_merged$round_duration, 3)
combined_segments_per_round_all_dt_merged <- combined_segments_per_round_all_dt_merged %>% relocate(round_duration, .after = task_duration)

# Now save it to a new csv
write_csv(combined_segments_per_round_all_dt_merged, file.path(path_to_files, "combined_segments_per_round_all_dt_merged.csv"))

# Check the counts
test_for_fun <- combined_segments_per_round_all_dt_merged %>%
  group_by(gaze_condition) %>%
  summarise(count = n(), .groups = "drop")

test_for_fun


# Get the mean task_duration by each instructional strategy
task_durations <- combined_segments_per_round_all_dt_merged %>%
  group_by(subject_id, instructional_strategy, gaze_condition) %>%
  summarise(mean_task_duration = median(task_duration, na.rm = TRUE), .groups = "drop")

task_durations

# Reorder instructional_strategy levels to match desired plot order 
task_durations$instructional_strategy <- factor(task_durations$instructional_strategy, levels = c("Descriptive", "Spatial", "Minimal"))


# First do a simple box and whiskers
ggplot(task_durations, aes(x = gaze_condition, y = mean_task_duration, fill = instructional_strategy)) +
  geom_boxplot() +
  labs(title = "Task Duration by Instructional Strategy and Gaze Condition",
       x = "Gaze Condition", 
       y = "Mean Task Duration (s)") +
  theme_minimal() +
  scale_fill_manual(values = c("Descriptive" = "#FFD180", "Spatial" = "#81D4FA", "Minimal" = "#FF7043")) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.text.x = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 12)
  )
# Now make it so that it's grouped by instructional strategy and there's two colors for gaze condition (so 2 boxes for each instructional strategy, 6 boxes total)
ggplot(task_durations, aes(x = instructional_strategy, y = mean_task_duration, fill = gaze_condition)) +
  geom_boxplot(position = position_dodge(width = 0.9)) +
  labs(title = "Task Duration by Instructional Strategy and Gaze Condition",
       x = "Instructional Strategy", 
       y = "Mean Task Duration (s)") +
  theme_minimal() +
  scale_fill_manual(values = c("On" = "#FFD180", "Off" = "#81D4FA")) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.text.x = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 12)
  )

# Plotting
library(ggplot2)
library(dplyr)

# Calculate summary statistics (mean, Q1, Q3)
summary_data <- task_durations %>%
  group_by(gaze_condition, instructional_strategy) %>%
  summarise(
    mean = median(mean_task_duration),
    ymin = quantile(mean_task_duration, 0.25),  # Q1
    ymax = quantile(mean_task_duration, 0.75),  # Q3
    .groups = 'drop'
  )

# Material-like colors for each instructional strategy
material_colors <- c("Descriptive" = "#FFD180", "Spatial" = "#81D4FA", "Minimal" = "#FF7043")

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = instructional_strategy),
           stat = "identity", position = position_dodge(width = 0.9), width = 0.25, color = "black", alpha = 0.7) +
  
  # 2. Boxplot elements: IQR and median
  geom_errorbar(data = summary_data, aes(x = gaze_condition, ymin = ymin, ymax = ymax, group = instructional_strategy),
                position = position_dodge(width = 0.9), width = 0.2, size = 1) +
  
  # 3. Violin plot
  geom_violin(data = task_durations, aes(x = gaze_condition, y = mean_task_duration, fill = instructional_strategy),
              position = position_dodge(width = 0.9), trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
  geom_jitter(data = task_durations, aes(x = gaze_condition, y = mean_task_duration, color = instructional_strategy),
              position = position_jitterdodge(jitter.width = 0.15, dodge.width = 0.9), size = 1.5, alpha = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  scale_color_manual(values = material_colors) +
  
  # Labels
  labs(title = "Task Duration by Instructional Strategy and Gaze Condition",
       x = "Gaze Condition", 
       y = "Median Task Duration (s)") +
  # Legend
  guides(fill = guide_legend(title = "Instructional Strategy"),
         color = guide_legend(title = "Instructional Strategy")) +
  # Theme
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.text.x = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 12)
  )

```


```{r}
library(dplyr)
library(ggplot2)
library(scales)

# Keep palette consistent with prior figures
strategy_palette <- c(
  "Descriptive" = "#FFD180",
  "Spatial"     = "#81D4FA",
  "Minimal"     = "#FF7043"
)

# Aggregate across ALL subjects and compute per-gaze proportions
pie_by_gaze <- instructional_strategies_data_dt_merged_tt_tc %>%
  filter(!is.na(instructional_strategy), !is.na(gaze_condition)) %>%
  mutate(
    instructional_strategy = factor(instructional_strategy,
                                    levels = c("Descriptive", "Spatial", "Minimal")),
    # Ensure readable facet labels regardless of upstream coding
    gaze_condition = recode(gaze_condition, "On" = "Gaze On", "Off" = "Gaze Off")
  ) %>%
  count(gaze_condition, instructional_strategy, name = "n") %>%
  group_by(gaze_condition) %>%
  mutate(
    pct = n / sum(n),
    # Hide labels for slivers to avoid collisions; adjust 0.035 as needed
    lbl = ifelse(pct < 0.035, "", paste0(instructional_strategy, "\n", n, " (",
                                         percent(pct, accuracy = 0.1), ")"))
  ) %>%
  ungroup()

# Two pie charts faceted by gaze condition
ggplot(pie_by_gaze, aes(x = 1, y = pct, fill = instructional_strategy)) +
  geom_col(width = 1, color = "white") +
  coord_polar(theta = "y") +
  facet_wrap(~ gaze_condition, ncol = 2) +
  scale_fill_manual(values = strategy_palette, drop = FALSE) +
  geom_text(aes(label = lbl), position = position_stack(vjust = 0.5), size = 4) +
  labs(title = "Instructional Strategies Across All Subjects by Gaze Condition",
       fill = "Strategy") +
  theme_void(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "bottom"
  )

# --- Optional donut variant (uncomment to use) ---
# ggplot(pie_by_gaze, aes(x = 2, y = pct, fill = instructional_strategy)) +
#   geom_col(width = 0.8, color = "white") +
#   coord_polar(theta = "y") +
#   facet_wrap(~ gaze_condition, ncol = 2) +
#   scale_fill_manual(values = strategy_palette, drop = FALSE) +
#   xlim(0.5, 2.5) +
#   geom_text(aes(label = lbl), position = position_stack(vjust = 0.5), size = 4) +
#   labs(title = "Instructional Strategies Across All Subjects by Gaze Condition",
#        fill = "Strategy") +
#   theme_void(base_size = 14) +
#   theme(plot.title = element_text(hjust = 0.5, face = "bold"),
#         legend.position = "bottom")

```


```{r}
#summary_data

#task_durations

combined_segments_per_round_all_dt_merged
# First remove all task_type Locaet
combined_segments_per_round_all_dt_merged_nl <- combined_segments_per_round_all_dt_merged %>% filter(task_type != "Locate")
combined_segments_per_round_all_dt_merged_nl
task_durations_cond <- combined_segments_per_round_all_dt_merged %>%
  group_by(subject_id,
           instructional_strategy,
           task_condition) %>%
  summarise(
    mean_task_duration = median(task_duration, na.rm = TRUE),  # median üî¢
    .groups = "drop"
  )

# 2. Factor‚Äêlevel ordering for consistent legend
task_durations_cond$instructional_strategy <- factor(
  task_durations_cond$instructional_strategy,
  levels = c("Descriptive", "Spatial", "Minimal")
)

# 3. Box-and-whisker plot by task_condition
ggplot(task_durations_cond,
       aes(x = task_condition, 
           y = mean_task_duration, 
           fill = instructional_strategy)) +
  geom_boxplot(position = position_dodge(width = 0.8), width = 0.6) +  # dodge for side-by-side
  labs(
    title = "Median Task Duration by Instructional Strategy and Task Condition",
    x     = "Task Condition",
    y     = "Median Task Duration (s)"
  ) +
  scale_fill_manual(
    values = c("Descriptive" = "#FFD180", 
               "Spatial"     = "#81D4FA", 
               "Minimal"     = "#FF7043")
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title       = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position  = "bottom",
    axis.text.x      = element_text(size = 12, face = "bold"),
    axis.text.y      = element_text(size = 12)
  )
# If needed, exclude a specific task_type (e.g., "Locate")
combined_nolocate <- combined_segments_per_round_all_dt_merged %>%
  filter(task_type != "Locate")  # filter üö´

# Compute per‚Äêsubject medians for task_type
task_durations_type <- combined_nolocate %>%
  group_by(subject_id,
           instructional_strategy,
           task_type) %>%
  summarise(
    mean_task_duration = median(task_duration, na.rm = TRUE),
    .groups = "drop"
  )

# Ensure factor ordering
task_durations_type$instructional_strategy <- factor(
  task_durations_type$instructional_strategy,
  levels = c("Descriptive", "Spatial", "Minimal")
)

# Plot
ggplot(task_durations_type,
       aes(x = task_type, 
           y = mean_task_duration, 
           fill = instructional_strategy)) +
  geom_boxplot(position = position_dodge(width = 0.8), width = 0.6) +
  labs(
    title = "Median Task Duration by Instructional Strategy and Task Type",
    x     = "Task Type",
    y     = "Median Task Duration (s)"
  ) +
  scale_fill_manual(
    values = c("Descriptive" = "#FFD180", 
               "Spatial"     = "#81D4FA", 
               "Minimal"     = "#FF7043")
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title       = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position  = "bottom",
    axis.text.x      = element_text(size = 12, face = "bold", angle = 45, hjust = 1),
    axis.text.y      = element_text(size = 12)
  )



```

```{r my_big_plot, fig.width=12, fig.height=6, dpi=300}
library(dplyr)
library(ggplot2)

# 1. For task_condition
task_durations_cond2 <- combined_segments_per_round_all_dt_merged %>%
  group_by(
    subject_id,
    instructional_strategy,
    task_condition,
    gaze_condition               # include this column üîç [dplyr group_by](https://www.google.com/search?q=dplyr+group_by)
  ) %>%
  summarise(
    mean_task_duration = median(task_duration, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    instructional_strategy = factor(
      instructional_strategy,
      levels = c("Descriptive","Spatial","Minimal")
    )
  )

# 2. For task_type (excluding "Locate")
task_durations_type2 <- combined_segments_per_round_all_dt_merged %>%
  filter(task_type != "Locate") %>%
  group_by(
    subject_id,
    instructional_strategy,
    task_type,
    gaze_condition               # include here too
  ) %>%
  summarise(
    mean_task_duration = median(task_duration, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    instructional_strategy = factor(
      instructional_strategy,
      levels = c("Descriptive","Spatial","Minimal")
    )
  )

ggplot(task_durations_cond2,
       aes(
         x    = task_condition,
         y    = mean_task_duration,
         fill = instructional_strategy
       )) +
  geom_boxplot(
    position = position_dodge(width = 0.8), 
    width    = 0.6
  ) +
  facet_wrap(
    ~ gaze_condition, 
    ncol   = 2                     # two panels side by side üé® [facet_wrap](https://www.google.com/search?q=ggplot2+facet_wrap)
  ) +
  labs(
    title = "Median Task Duration by Instructional Strategy\nand Task Condition, by Gaze Condition",
    x     = "Task Condition",
    y     = "Median Duration (s)"
  ) +
  scale_fill_manual(
    values = c(
      "Descriptive" = "#FFD180",
      "Spatial"     = "#81D4FA",
      "Minimal"     = "#FF7043"
    )
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text      = element_text(size = 12, face = "bold"),
    plot.title      = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.text.x     = element_text(size = 12, face = "bold"),
    axis.text.y     = element_text(size = 12)
  )
ggplot(task_durations_type2,
       aes(
         x    = gaze_condition,
         y    = mean_task_duration,
         fill = instructional_strategy
       )) +
  geom_boxplot(
    position = position_dodge(width = 0.8),
    width    = 0.6
  ) +
  facet_wrap(
    ~ task_type,
    scales = "free_x",            # allow differing x‚Äêaxis labels length
    ncol   = 2
  ) +
  labs(
    title = "Median Task Duration by Instructional Strategy\nand Task Type, by Gaze Condition",
    x     = "Gaze Condition",
    y     = "Median Duration (s)"
  ) +
  scale_fill_manual(
    values = c(
      "Descriptive" = "#FFD180",
      "Spatial"     = "#81D4FA",
      "Minimal"     = "#FF7043"
    )
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text      = element_text(size = 12, face = "bold"),
    plot.title      = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.text.x     = element_text(size = 11, face = "bold", angle = 45, hjust = 1),
    axis.text.y     = element_text(size = 12)
  )


task_durations_type2

#Check the counts of each instructional strategy within each gaze condition

task_durations_type2_count <- task_durations_type2 %>%
  group_by(subject_id) %>%
  summarise(count = n(), .groups = "drop")

task_durations_type2_count

```

TODO: Now let's investiga

```{r}
#Find the outliers
# Calculating IQR and identifying outliers
task_durations <- task_durations_type2 %>%
  group_by(gaze_condition, instructional_strategy) %>%
  mutate(
    Q1 = quantile(mean_task_duration, 0.25),
    Q3 = quantile(mean_task_duration, 0.75),
    IQR = Q3 - Q1,
    Lower_Bound = Q1 - 1.5 * IQR,
    Upper_Bound = Q3 + 1.5 * IQR
  ) %>%
  ungroup() %>%
  mutate(Outlier = mean_task_duration < Lower_Bound | mean_task_duration > Upper_Bound)

# Optionally, filter to see only outliers
outliers <- task_durations %>%
  filter(Outlier)

# Output outliers data
print(outliers)

```


```{r}

# Get the mean task_duration by each instructional strategy
task_durations_clean <- combined_segments_per_round_all_dt_merged %>%
  group_by(subject_id, instructional_strategy, task_condition, gaze_condition) %>%
  summarise(mean_task_duration = median(task_duration, na.rm = TRUE), .groups = "drop") %>%
  complete(subject_id, instructional_strategy, gaze_condition, 
           fill = list(mean_task_duration = NA))

#First check that the count of trials across condition is evenly split.
# First start with gaze condition
# Check the count of trials across gaze condition
task_durations_gaze_count <- task_durations_clean %>%
  group_by(gaze_condition) %>%
  summarise(count = n(), .groups = "drop")
task_durations_gaze_count
# It's not split evenly, so figure out counts of gaze condition broken down by subject

task_durations_gaze_count_subject <- task_durations_clean %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(count = n(), .groups = "drop")
task_durations_gaze_count_subject

```

```{r my_big_plot, fig.width=12, fig.height=6, dpi=300}
library(dplyr)
library(ggplot2)
library(tidyr)
library(patchwork)

task_durations_clean <- task_durations_cond2






# First, create aggregated data for pie charts
# We'll sum total durations to show proportional time spent in each condition

# 1. Overall distribution by Strategy √ó Correctness √ó Gaze
pie_data_full <- task_durations_clean %>%
  group_by(instructional_strategy, task_condition, gaze_condition) %>%
  summarise(
    total_duration = sum(mean_task_duration, na.rm = TRUE),
    n_subjects = n(),
    .groups = "drop"
  ) %>%
  mutate(
    combined_label = paste(instructional_strategy, task_condition, gaze_condition, sep = "\n")
  )

# 2. Create nested pie charts - one for each instructional strategy
pie_by_strategy <- task_durations_clean %>%
  group_by(instructional_strategy, task_condition, gaze_condition) %>%
  summarise(
    total_duration = sum(mean_task_duration, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(instructional_strategy) %>%
  mutate(
    pct = total_duration / sum(total_duration),
    label = paste0(task_condition, "\n", gaze_condition, "\n", 
                   round(pct * 100, 1), "%")
  )

# Create individual pie charts for each strategy
p1 <- ggplot(pie_by_strategy %>% filter(instructional_strategy == "Descriptive"),
             aes(x = "", y = total_duration, 
                 fill = interaction(task_condition, gaze_condition))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Descriptive Strategy", fill = "Condition") +
  scale_fill_manual(values = c("Incorrect.Off" = "#FF9999",
                               "Incorrect.On" = "#FF6666",
                               "Correct.Off" = "#66CCCC",
                               "Correct.On" = "#00CCCC"),
                    labels = c("Incorrect.Off" = "Incorrect/Off",
                              "Incorrect.On" = "Incorrect/On",
                              "Correct.Off" = "Correct/Off",
                              "Correct.On" = "Correct/On")) +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

p2 <- ggplot(pie_by_strategy %>% filter(instructional_strategy == "Spatial"),
             aes(x = "", y = total_duration, 
                 fill = interaction(task_condition, gaze_condition))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Spatial Strategy", fill = "Condition") +
  scale_fill_manual(values = c("Incorrect.Off" = "#FF9999",
                               "Incorrect.On" = "#FF6666",
                               "Correct.Off" = "#66CCCC",
                               "Correct.On" = "#00CCCC"),
                    labels = c("Incorrect.Off" = "Incorrect/Off",
                              "Incorrect.On" = "Incorrect/On",
                              "Correct.Off" = "Correct/Off",
                              "Correct.On" = "Correct/On")) +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

p3 <- ggplot(pie_by_strategy %>% filter(instructional_strategy == "Minimal"),
             aes(x = "", y = total_duration, 
                 fill = interaction(task_condition, gaze_condition))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Minimal Strategy", fill = "Condition") +
  scale_fill_manual(values = c("Incorrect.Off" = "#FF9999",
                               "Incorrect.On" = "#FF6666",
                               "Correct.Off" = "#66CCCC",
                               "Correct.On" = "#00CCCC"),
                    labels = c("Incorrect.Off" = "Incorrect/Off",
                              "Incorrect.On" = "Incorrect/On",
                              "Correct.Off" = "Correct/Off",
                              "Correct.On" = "Correct/On")) +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

# Combine the three pie charts
combined_pies <- p1 + p2 + p3 + 
  plot_layout(ncol = 3) +
  plot_annotation(title = "Task Duration Distribution by Strategy and Conditions",
                  theme = theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5)))

print(combined_pies)

# 3. Alternative: Nested donut charts for each gaze condition
pie_by_gaze <- task_durations_clean %>%
  group_by(gaze_condition, instructional_strategy, task_condition) %>%
  summarise(
    total_duration = sum(mean_task_duration, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(gaze_condition) %>%
  mutate(
    pct = total_duration / sum(total_duration),
    label = paste0(round(pct * 100, 1), "%")
  )

# Donut chart for Gaze Off
p_off <- ggplot(pie_by_gaze %>% filter(gaze_condition == "Off"),
                aes(x = 2, y = total_duration, 
                    fill = interaction(instructional_strategy, task_condition))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  xlim(0.5, 2.5) +  # Creates the donut hole
  geom_text(aes(label = label), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze Off", fill = "Strategy/Correctness") +
  scale_fill_manual(values = c("Descriptive.Incorrect" = "#FFB366",
                               "Descriptive.Correct" = "#FFD180",
                               "Spatial.Incorrect" = "#4FC3F7",
                               "Spatial.Correct" = "#81D4FA",
                               "Minimal.Incorrect" = "#FF5722",
                               "Minimal.Correct" = "#FF7043"),
                    labels = c("Descriptive.Incorrect" = "Desc/Incorrect",
                              "Descriptive.Correct" = "Desc/Correct",
                              "Spatial.Incorrect" = "Spatial/Incorrect",
                              "Spatial.Correct" = "Spatial/Correct",
                              "Minimal.Incorrect" = "Min/Incorrect",
                              "Minimal.Correct" = "Min/Correct")) +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14))

# Donut chart for Gaze On
p_on <- ggplot(pie_by_gaze %>% filter(gaze_condition == "On"),
               aes(x = 2, y = total_duration, 
                   fill = interaction(instructional_strategy, task_condition))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  xlim(0.5, 2.5) +  # Creates the donut hole
  geom_text(aes(label = label), 
            position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze On", fill = "Strategy/Correctness") +
  scale_fill_manual(values = c("Descriptive.Incorrect" = "#FFB366",
                               "Descriptive.Correct" = "#FFD180",
                               "Spatial.Incorrect" = "#4FC3F7",
                               "Spatial.Correct" = "#81D4FA",
                               "Minimal.Incorrect" = "#FF5722",
                               "Minimal.Correct" = "#FF7043"),
                    labels = c("Descriptive.Incorrect" = "Desc/Incorrect",
                              "Descriptive.Correct" = "Desc/Correct",
                              "Spatial.Incorrect" = "Spatial/Incorrect",
                              "Spatial.Correct" = "Spatial/Correct",
                              "Minimal.Incorrect" = "Min/Incorrect",
                              "Minimal.Correct" = "Min/Correct")) +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14))

# Combine the donut charts
donut_combined <- p_off + p_on +
  plot_layout(ncol = 2) +
  plot_annotation(title = "Task Duration Distribution by Gaze Condition",
                  subtitle = "Showing Strategy √ó Correctness proportions within each gaze condition",
                  theme = theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
                               plot.subtitle = element_text(size = 12, hjust = 0.5)))

print(donut_combined)

# 4. Sunburst-style nested pie (simulated with multiple rings)
# This shows the hierarchical nature: Gaze -> Strategy -> Correctness

# First ring: Gaze condition
gaze_totals <- task_durations_clean %>%
  group_by(gaze_condition) %>%
  summarise(total = sum(mean_task_duration, na.rm = TRUE), .groups = "drop") %>%
  mutate(level = "inner", start = lag(cumsum(total), default = 0), end = cumsum(total))

# Second ring: Strategy within gaze
strategy_totals <- task_durations_clean %>%
  group_by(gaze_condition, instructional_strategy) %>%
  summarise(total = sum(mean_task_duration, na.rm = TRUE), .groups = "drop") %>%
  arrange(gaze_condition, instructional_strategy) %>%
  mutate(level = "middle", 
         cumtotal = cumsum(total),
         start = lag(cumtotal, default = 0), 
         end = cumtotal)

# Third ring: Correctness within strategy and gaze
correct_totals <- task_durations_clean %>%
  group_by(gaze_condition, instructional_strategy, task_condition) %>%
  summarise(total = sum(mean_task_duration, na.rm = TRUE), .groups = "drop") %>%
  arrange(gaze_condition, instructional_strategy, task_condition) %>%
  mutate(level = "outer",
         cumtotal = cumsum(total),
         start = lag(cumtotal, default = 0), 
         end = cumtotal)

# Create a hierarchical pie chart using facets
# Prepare data for stacked bars that will become rings
ring_data <- task_durations_clean %>%
  group_by(gaze_condition, instructional_strategy, task_condition) %>%
  summarise(duration = sum(mean_task_duration, na.rm = TRUE), .groups = "drop") %>%
  mutate(
    full_label = paste(gaze_condition, instructional_strategy, task_condition, sep = "-")
  )

# Create the sunburst chart
sunburst <- ggplot(ring_data) +
  # Outer ring (all details)
  geom_bar(aes(x = 3, y = duration, 
               fill = interaction(gaze_condition, instructional_strategy, task_condition)),
           stat = "identity", width = 0.5) +
  # Middle ring (strategy level)
  geom_bar(data = ring_data %>% 
             group_by(gaze_condition, instructional_strategy) %>% 
             summarise(duration = sum(duration), .groups = "drop"),
           aes(x = 2, y = duration, 
               fill = interaction(gaze_condition, instructional_strategy)),
           stat = "identity", width = 0.5) +
  # Inner ring (gaze level)
  geom_bar(data = ring_data %>% 
             group_by(gaze_condition) %>% 
             summarise(duration = sum(duration), .groups = "drop"),
           aes(x = 1, y = duration, fill = gaze_condition),
           stat = "identity", width = 0.5) +
  coord_polar("y", start = 0) +
  xlim(0, 3.5) +
  labs(title = "Hierarchical Task Duration Distribution",
       subtitle = "Inner: Gaze | Middle: Strategy | Outer: Correctness") +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        legend.position = "bottom") +
  scale_fill_manual(values = c(
    # Gaze only (inner ring)
    "Off" = "#FFB366",
    "On" = "#81D4FA",
    # Gaze.Strategy (middle ring)
    "Off.Descriptive" = "#FFD180",
    "Off.Spatial" = "#A5D6F7",
    "Off.Minimal" = "#FF7043",
    "On.Descriptive" = "#FFC947",
    "On.Spatial" = "#81D4FA", 
    "On.Minimal" = "#FF5722",
    # Full details (outer ring) - using slightly different shades
    "Off.Descriptive.Incorrect" = "#FFCC80",
    "Off.Descriptive.Correct" = "#FFE0B2",
    "Off.Spatial.Incorrect" = "#90CAF9",
    "Off.Spatial.Correct" = "#BBDEFB",
    "Off.Minimal.Incorrect" = "#FF8A65",
    "Off.Minimal.Correct" = "#FFAB91",
    "On.Descriptive.Incorrect" = "#FFB74D",
    "On.Descriptive.Correct" = "#FFCC80",
    "On.Spatial.Incorrect" = "#64B5F6",
    "On.Spatial.Correct" = "#90CAF9",
    "On.Minimal.Incorrect" = "#FF5722",
    "On.Minimal.Correct" = "#FF7043"
  ))

print(sunburst)
```

```{r}

#Double check 
```


```{r my_big_plot, fig.width=12, fig.height=6, dpi=300}
library(dplyr)
library(ggplot2)
library(tidyr)
library(patchwork)
library(sunburstR)
# Install ggsunburst if needed: devtools::install_github("didacs/ggsunburst")
library(ggsunburst)
library(reticulate)

# Define consistent color palette for instructional strategies
strategy_palette <- c(
  "Descriptive" = "#FFD180",
  "Spatial" = "#81D4FA",
  "Minimal" = "#FF7043"
)

# ============================================================================
# 1. INDIVIDUAL PIE CHARTS - With counts and percentages
# ============================================================================

# Create pie data for each combination with counts
pie_data_individual <- task_durations_clean %>%
  group_by(gaze_condition, task_condition, instructional_strategy) %>%
  summarise(
    total_duration = sum(mean_task_duration, na.rm = TRUE),
    n_count = n(),  # Add count of observations
    .groups = "drop"
  ) %>%
  group_by(gaze_condition, task_condition) %>%
  mutate(
    pct = total_duration / sum(total_duration),
    # Include both count and percentage in label
    label = paste0(instructional_strategy, "\n", 
                   "(", n_count, ")\n",
                   round(pct * 100, 1), "%")
  )

# Create 4 pie charts (2x2 grid) with counts
p_off_incorrect <- ggplot(
  pie_data_individual %>% filter(gaze_condition == "Off", task_condition == "Incorrect"),
  aes(x = "", y = total_duration, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze Off - Incorrect", fill = "Strategy") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "none"
  )

p_off_correct <- ggplot(
  pie_data_individual %>% filter(gaze_condition == "Off", task_condition == "Correct"),
  aes(x = "", y = total_duration, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze Off - Correct", fill = "Strategy") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "none"
  )

p_on_incorrect <- ggplot(
  pie_data_individual %>% filter(gaze_condition == "On", task_condition == "Incorrect"),
  aes(x = "", y = total_duration, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze On - Incorrect", fill = "Strategy") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "none"
  )

p_on_correct <- ggplot(
  pie_data_individual %>% filter(gaze_condition == "On", task_condition == "Correct"),
  aes(x = "", y = total_duration, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze On - Correct", fill = "Strategy") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "bottom"
  )

# Combine all 4 pie charts in a 2x2 grid
individual_pies <- (p_off_incorrect | p_off_correct) / (p_on_incorrect | p_on_correct) +
  plot_annotation(
    title = "Instructional Strategy Distribution by Gaze and Correctness",
    subtitle = "Showing counts (n) and percentages for each strategy",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5)
    )
  )

print(individual_pies)

# ============================================================================
# 2. SUNBURSTR - Proper hierarchy with corrected path structure
# ============================================================================

# Make sure we're using sunburstR package specifically
# If you get an error, install it with: install.packages("sunburstR")
library(sunburstR)

# Prepare data for sunburstR with full hierarchy
sunburst_data <- task_durations_clean %>%
  group_by(gaze_condition, task_condition, instructional_strategy) %>%
  summarise(
    duration = sum(mean_task_duration, na.rm = TRUE),
    count = n(),
    .groups = "drop"
  )

# Create paths for sunburstR - it needs the full hierarchical path
sunburst_r_full <- sunburst_data %>%
  mutate(
    # Create hierarchical paths
    path = paste(
      paste0("Gaze ", gaze_condition),
      task_condition,
      instructional_strategy,
      sep = "-"
    ),
    size = round(duration, 2)
  ) %>%
  select(path, size)

# Create the full sunburst with all three levels
# Use sunburstR:: to ensure we're using the right function
sb_full <- sunburstR::sunburst(sunburst_r_full)

print(sb_full)
cat("\n=== Full Sunburst: Gaze ‚Üí Correctness ‚Üí Strategy ===\n")

# Alternative: Create two separate sunbursts for each gaze condition
# This makes it easier to see the correctness ‚Üí strategy breakdown

# Sunburst for Gaze Off
sunburst_off <- sunburst_data %>%
  filter(gaze_condition == "Off") %>%
  mutate(
    path = paste(task_condition, instructional_strategy, sep = "-"),
    size = round(duration, 2)
  ) %>%
  select(path, size)

sb_off <- sunburstR::sunburst(sunburst_off)

print(sb_off)
cat("\n=== Gaze Off: Correctness ‚Üí Strategy ===\n")

# Sunburst for Gaze On
sunburst_on <- sunburst_data %>%
  filter(gaze_condition == "On") %>%
  mutate(
    path = paste(task_condition, instructional_strategy, sep = "-"),
    size = round(duration, 2)
  ) %>%
  select(path, size)

sb_on <- sunburstR::sunburst(sunburst_on)

print(sb_on)
cat("\n=== Gaze On: Correctness ‚Üí Strategy ===\n")

# ============================================================================
# 3. PLOTLY SUNBURST - More reliable alternative
# ============================================================================

library(plotly)

# Prepare data for plotly sunburst (more reliable than ggsunburst)
# Build the hierarchical structure

# Create labels and parents for plotly
plotly_data <- data.frame(
  labels = character(),
  parents = character(),
  values = numeric(),
  ids = character(),
  stringsAsFactors = FALSE
)

# Add root
plotly_data <- rbind(plotly_data, data.frame(
  labels = "Total",
  parents = "",
  values = sum(sunburst_data$duration),
  ids = "Total"
))

# Add Gaze level
gaze_summary <- sunburst_data %>%
  group_by(gaze_condition) %>%
  summarise(total = sum(duration), .groups = "drop")

for(i in 1:nrow(gaze_summary)) {
  plotly_data <- rbind(plotly_data, data.frame(
    labels = paste("Gaze", gaze_summary$gaze_condition[i]),
    parents = "Total",
    values = gaze_summary$total[i],
    ids = paste("Gaze", gaze_summary$gaze_condition[i])
  ))
}

# Add Correctness level
correct_summary <- sunburst_data %>%
  group_by(gaze_condition, task_condition) %>%
  summarise(total = sum(duration), .groups = "drop")

for(i in 1:nrow(correct_summary)) {
  plotly_data <- rbind(plotly_data, data.frame(
    labels = correct_summary$task_condition[i],
    parents = paste("Gaze", correct_summary$gaze_condition[i]),
    values = correct_summary$total[i],
    ids = paste("Gaze", correct_summary$gaze_condition[i], 
                correct_summary$task_condition[i], sep = "-")
  ))
}

# Add Strategy level
for(i in 1:nrow(sunburst_data)) {
  plotly_data <- rbind(plotly_data, data.frame(
    labels = sunburst_data$instructional_strategy[i],
    parents = paste("Gaze", sunburst_data$gaze_condition[i], 
                   sunburst_data$task_condition[i], sep = "-"),
    values = sunburst_data$duration[i],
    ids = paste("Gaze", sunburst_data$gaze_condition[i],
               sunburst_data$task_condition[i],
               sunburst_data$instructional_strategy[i], sep = "-")
  ))
}

# Create color vector
colors <- c(
  "white",  # Total
  "#FFA726", "#42A5F5",  # Gaze Off, On
  "#FF8A65", "#FFB74D", "#5C6BC0", "#29B6F6",  # Correctness combinations
  # Strategy combinations
  "#FFD180", "#81D4FA", "#FF7043",
  "#FFE082", "#90CAF9", "#FF8A65",
  "#FFCC80", "#64B5F6", "#FF5722",
  "#FFE0B2", "#BBDEFB", "#FFAB91"
)

# Create the plotly sunburst
p_sunburst <- plot_ly(
  data = plotly_data,
  ids = ~ids,
  labels = ~labels,
  parents = ~parents,
  values = ~values,
  type = 'sunburst',
  branchvalues = "total",
  marker = list(colors = colors[1:nrow(plotly_data)]),
  textinfo = "label+percent parent"
) %>%
  layout(
    title = "Task Duration Hierarchy: Gaze ‚Üí Correctness ‚Üí Strategy",
    margin = list(l = 0, r = 0, t = 50, b = 0)
  )

print(p_sunburst)

# ============================================================================
# 4. Alternative approach: Create a proper sunburst with treemapify
# ============================================================================


library(treemapify)
library(ggplot2)

# Prepare nested data
nested_data <- sunburst_data %>%
  mutate(
    path1 = paste0("Gaze ", gaze_condition),
    path2 = paste(paste0("Gaze ", gaze_condition), task_condition, sep = " > "),
    path3 = paste(paste0("Gaze ", gaze_condition), task_condition, instructional_strategy, sep = " > ")
  )

# Create a treemap as alternative visualization
treemap_plot <- ggplot(nested_data, aes(
  area = duration,
  fill = instructional_strategy,
  label = paste0(instructional_strategy, "\n", round(duration, 1), "s"),
  subgroup = task_condition,
  subgroup2 = gaze_condition
)) +
  geom_treemap() +
  geom_treemap_subgroup2_border(color = "white", size = 3) +
  geom_treemap_subgroup_border(color = "white", size = 2) +
  geom_treemap_text(
    fontface = "bold",
    colour = "white",
    place = "centre",
    grow = FALSE,
    size = 10
  ) +
  geom_treemap_subgroup2_text(
    place = "top",
    alpha = 0.7,
    fontface = "bold",
    size = 14
  ) +
  geom_treemap_subgroup_text(
    place = "topleft",
    alpha = 0.5,
    fontface = "italic",
    size = 11
  ) +
  scale_fill_manual(values = strategy_palette) +
  labs(
    title = "Task Duration Treemap",
    subtitle = "Hierarchical view: Gaze > Correctness > Strategy",
    fill = "Strategy"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    legend.position = "bottom"
  )

print(treemap_plot)
```

```{r my_balanced_sunburst, fig.width=12, fig.height=8, dpi=300}
library(dplyr)
library(ggplot2)
library(tidyr)
library(patchwork)
library(plotly)

# Define consistent color palette for instructional strategies
strategy_palette <- c(
  "Descriptive" = "#FFD180",
  "Spatial" = "#81D4FA", 
  "Minimal" = "#FF7043"
)

# ============================================================================
# 1. CREATE COMPLETE BALANCED DATA WITH COUNTS
# ============================================================================

# Get all possible combinations and ensure balanced representation
all_subjects <- combined_segments_per_round_all_dt_merged %>%
  distinct(subject_id) %>%
  pull(subject_id)

all_gaze <- c("Off", "On")
all_strategies <- c("Descriptive", "Spatial", "Minimal")
all_correctness <- c("Correct", "Incorrect")

# Create complete grid of all possible combinations
complete_combinations <- expand_grid(
  subject_id = all_subjects,
  gaze_condition = all_gaze,
  instructional_strategy = all_strategies,
  task_condition = all_correctness
)

# Get actual counts from the data
actual_counts <- combined_segments_per_round_all_dt_merged %>%
  filter(!is.na(instructional_strategy) & !is.na(task_condition)) %>%
  group_by(subject_id, gaze_condition, instructional_strategy, task_condition) %>%
  summarise(count = n(), .groups = "drop")

# Join to get complete data with 0s for missing combinations
balanced_data <- complete_combinations %>%
  left_join(actual_counts, by = c("subject_id", "gaze_condition", "instructional_strategy", "task_condition")) %>%
  mutate(count = ifelse(is.na(count), 0, count))

# Aggregate to get total counts for each combination
sunburst_counts <- balanced_data %>%
  group_by(gaze_condition, instructional_strategy, task_condition) %>%
  summarise(total_count = sum(count), .groups = "drop")

print("Sunburst counts summary:")
print(sunburst_counts)

# Check balance
balance_check <- sunburst_counts %>%
  group_by(gaze_condition) %>%
  summarise(total = sum(total_count), .groups = "drop")
print("Balance check by gaze condition:")
print(balance_check)

correctness_check <- sunburst_counts %>%
  group_by(task_condition) %>%
  summarise(total = sum(total_count), .groups = "drop")
print("Balance check by correctness:")
print(correctness_check)

# ============================================================================
# 2. CREATE INDIVIDUAL PIE CHARTS WITH COUNTS
# ============================================================================

# Create pie data with proper percentages
pie_data_balanced <- sunburst_counts %>%
  group_by(gaze_condition, task_condition) %>%
  mutate(
    pct = total_count / sum(total_count),
    label = paste0(instructional_strategy, "\n", 
                   "(n=", total_count, ")\n",
                   round(pct * 100, 1), "%")
  )

# Create 4 pie charts (2x2 grid)
p_off_incorrect <- ggplot(
  pie_data_balanced %>% filter(gaze_condition == "Off", task_condition == "Incorrect"),
  aes(x = "", y = total_count, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze Off - Incorrect") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "none"
  )

p_off_correct <- ggplot(
  pie_data_balanced %>% filter(gaze_condition == "Off", task_condition == "Correct"),
  aes(x = "", y = total_count, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze Off - Correct") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "none"
  )

p_on_incorrect <- ggplot(
  pie_data_balanced %>% filter(gaze_condition == "On", task_condition == "Incorrect"),
  aes(x = "", y = total_count, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze On - Incorrect") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "none"
  )

p_on_correct <- ggplot(
  pie_data_balanced %>% filter(gaze_condition == "On", task_condition == "Correct"),
  aes(x = "", y = total_count, fill = instructional_strategy)
) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 3) +
  labs(title = "Gaze On - Correct") +
  scale_fill_manual(values = strategy_palette) +
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 12),
    legend.position = "bottom"
  )

# Combine all 4 pie charts
individual_pies <- (p_off_incorrect | p_off_correct) / (p_on_incorrect | p_on_correct) +
  plot_annotation(
    title = "Instructional Strategy Distribution by Gaze and Correctness",
    subtitle = "Showing counts (n) and percentages for each strategy",
    theme = theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 12, hjust = 0.5)
    )
  )

print(individual_pies)


```

```{r my_balanced_sunburst, fig.width=12, fig.height=8, dpi=300}

# ============================================================================
# 3. PLOTLY SUNBURST WITH BALANCED HIERARCHY
# ============================================================================

# Build hierarchical structure for plotly sunburst
plotly_data <- data.frame(
  labels = character(),
  parents = character(), 
  values = numeric(),
  ids = character(),
  stringsAsFactors = FALSE
)

# Total count
total_count <- sum(sunburst_counts$total_count)

# Add root (50/50 split enforced)
plotly_data <- rbind(plotly_data, data.frame(
  labels = "Total",
  parents = "",
  values = total_count,
  ids = "Total"
))

# Add Gaze level (forced 50/50)
gaze_value <- total_count / 2  # Force equal split
for(gaze in all_gaze) {
  plotly_data <- rbind(plotly_data, data.frame(
    labels = paste("Gaze", gaze),
    parents = "Total",
    values = gaze_value,
    ids = paste("Gaze", gaze)
  ))
}

# Add Correctness level (forced 50/50 within each gaze)
correctness_value <- gaze_value / 2  # Force equal split within gaze
for(gaze in all_gaze) {
  for(correct in all_correctness) {
    plotly_data <- rbind(plotly_data, data.frame(
      labels = correct,
      parents = paste("Gaze", gaze),
      values = correctness_value,
      ids = paste("Gaze", gaze, correct, sep = "-")
    ))
  }
}

# Add Strategy level (use actual proportions within each correctness group)
for(i in 1:nrow(sunburst_counts)) {
  # Calculate what proportion this should be of the correctness_value
  group_total <- sunburst_counts %>%
    filter(gaze_condition == sunburst_counts$gaze_condition[i],
           task_condition == sunburst_counts$task_condition[i]) %>%
    summarise(total = sum(total_count)) %>%
    pull(total)
  
  # Proportion within this correctness group
  proportion <- sunburst_counts$total_count[i] / group_total
  strategy_value <- correctness_value * proportion
  
  plotly_data <- rbind(plotly_data, data.frame(
    labels = paste0(sunburst_counts$instructional_strategy[i], " (n=", sunburst_counts$total_count[i], ")"),
    parents = paste("Gaze", sunburst_counts$gaze_condition[i], 
                   sunburst_counts$task_condition[i], sep = "-"),
    values = strategy_value,
    ids = paste("Gaze", sunburst_counts$gaze_condition[i],
               sunburst_counts$task_condition[i],
               sunburst_counts$instructional_strategy[i], sep = "-")
  ))
}

# Create color vector
colors <- c(
  "#FFFFFF",  # Total (white)
  "#FFA726", "#42A5F5",  # Gaze Off, On
  "#FF8A65", "#FFB74D", "#5C6BC0", "#29B6F6",  # Correctness combinations
  # Strategy combinations (using strategy palette)
  strategy_palette["Descriptive"], strategy_palette["Spatial"], strategy_palette["Minimal"],  # Off-Incorrect
  strategy_palette["Descriptive"], strategy_palette["Spatial"], strategy_palette["Minimal"],  # Off-Correct  
  strategy_palette["Descriptive"], strategy_palette["Spatial"], strategy_palette["Minimal"],  # On-Incorrect
  strategy_palette["Descriptive"], strategy_palette["Spatial"], strategy_palette["Minimal"]   # On-Correct
)

# Create the balanced plotly sunburst
p_sunburst_balanced <- plot_ly(
  data = plotly_data,
  ids = ~ids,
  labels = ~labels,
  parents = ~parents,
  values = ~values,
  type = 'sunburst',
  branchvalues = "total",
  marker = list(colors = colors[1:nrow(plotly_data)]),
  textinfo = "label+percent parent",
  width = 1000,
  height = 1000
) %>%
  layout(
    title = list(
      text = "Balanced Task Count Hierarchy: Gaze (50/50) ‚Üí Correctness (50/50) ‚Üí Strategy (Actual Proportions)",
      font = list(size = 16)
    ),
    margin = list(l = 50, r = 50, t = 100, b = 50),
    font = list(size = 16),
    width = 1000,
    height = 1000
  )

print(p_sunburst_balanced)

# ============================================================================
# 4. SUMMARY STATISTICS
# ============================================================================

# Print summary of actual vs balanced data
cat("\n=== SUMMARY STATISTICS ===\n")
cat("Total observations:", total_count, "\n")

strategy_summary <- sunburst_counts %>%
  group_by(instructional_strategy) %>%
  summarise(total = sum(total_count), pct = round(total/total_count*100, 1), .groups = "drop")

cat("\nStrategy distribution overall:\n")
print(strategy_summary)

gaze_strategy_summary <- sunburst_counts %>%
  group_by(gaze_condition, instructional_strategy) %>%
  summarise(total = sum(total_count), .groups = "drop") %>%
  group_by(gaze_condition) %>%
  mutate(pct = round(total/sum(total)*100, 1))

cat("\nStrategy distribution by gaze condition:\n")
print(gaze_strategy_summary)
```


```{r my_big_plot, fig.width=12, fig.height=6, dpi=300}
library(dplyr)
library(ggplot2)
library(tidyr)
library(patchwork)
# Install ggsunburst if needed: devtools::install_github("didacs/ggsunburst")
library(ggsunburst)

# Define consistent color palette for instructional strategies
strategy_palette <- c(
  "Descriptive" = "#FFD180",
  "Spatial" = "#81D4FA",
  "Minimal" = "#FF7043"
)

# ============================================================================
# 1. PREPARE DATA AND CONVERT TO NEWICK FORMAT
# ============================================================================

# Aggregate the data
sunburst_data_agg <- task_durations_clean %>%
  group_by(gaze_condition, task_condition, instructional_strategy) %>%
  summarise(
    duration = sum(mean_task_duration, na.rm = TRUE),
    count = n(),
    .groups = "drop"
  ) %>%
  # Create readable labels with counts
  mutate(
    gaze_label = paste0("Gaze_", gaze_condition),
    task_label = paste0(task_condition, "_n", count),
    strategy_label = paste0(instructional_strategy, "_", round(duration, 1))
  )

# Function to convert hierarchical data to newick format
create_newick_from_data <- function(data) {
  # Group by gaze and task conditions to build the tree structure
  gaze_groups <- split(data, data$gaze_condition)
  
  gaze_trees <- map(gaze_groups, function(gaze_data) {
    task_groups <- split(gaze_data, gaze_data$task_condition)
    
    task_trees <- map(task_groups, function(task_data) {
      # Create leaf nodes for strategies
      strategies <- paste0(task_data$strategy_label, ":", task_data$duration)
      paste0("(", paste(strategies, collapse = ","), ")", task_data$task_label[1])
    })
    
    paste0("(", paste(task_trees, collapse = ","), ")", gaze_data$gaze_label[1])
  })
  
  # Combine into full tree
  newick <- paste0("(", paste(gaze_trees, collapse = ","), ");")
  return(newick)
}

# Create newick string
newick_string <- create_newick_from_data(sunburst_data_agg)
cat("Newick format tree:\n", newick_string, "\n\n")

# ============================================================================
# 2. EXTRACT SUNBURST DATA AND CREATE PLOT
# ============================================================================


# Extract sunburst coordinates using ggsunburst
sb_data <- sunburst_data(newick_string)

# Create the main sunburst plot
p_sunburst_main <- ggplot(sb_data$rects) +
  geom_rect(
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = name),
    color = "white", 
    size = 0.5
  ) +
  coord_polar() +
  scale_fill_manual(
    values = c(
      # Set colors based on the hierarchy level and content
      # You may need to adjust these based on the actual names in sb_data$rects
      "Gaze_Off" = "#FFA726",
      "Gaze_On" = "#42A5F5",
      "Correct" = "#4CAF50",
      "Incorrect" = "#FF5722",
      "Descriptive" = "#FFD180",
      "Spatial" = "#81D4FA", 
      "Minimal" = "#FF7043"
    ),
    na.value = "grey80"
  ) +
  labs(
    title = "Task Duration Hierarchy: Gaze ‚Üí Correctness ‚Üí Strategy",
    subtitle = "Sunburst diagram showing nested relationships",
    fill = "Category"
  ) +
  theme_void() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.position = "right"
  )

print(p_sunburst_main)

# ============================================================================
# 3. ALTERNATIVE: MANUAL NEWICK FOR CLEANER LABELS
# ============================================================================

# Create a simpler newick structure with cleaner names
create_simple_newick <- function(data) {
  # Simplify the data for cleaner visualization
  simple_data <- data %>%
    mutate(
      # Use duration as the "branch length" in newick format
      duration_scaled = round(duration / max(duration) * 10, 1)
    )
  
  # Build newick manually for better control
  gaze_off <- simple_data %>% filter(gaze_condition == "Off")
  gaze_on <- simple_data %>% filter(gaze_condition == "On")
  
  # Function to create subtree for each gaze condition
  create_gaze_subtree <- function(gaze_data, gaze_name) {
    correct <- gaze_data %>% filter(task_condition == "Correct")
    incorrect <- gaze_data %>% filter(task_condition == "Incorrect")
    
    # Create strategy leaves for each task condition
    create_strategy_leaves <- function(task_data) {
      if(nrow(task_data) == 0) return("EmptyTask:1")
      leaves <- paste0(task_data$instructional_strategy, ":", task_data$duration_scaled)
      paste0("(", paste(leaves, collapse = ","), ")")
    }
    
    correct_subtree <- create_strategy_leaves(correct)
    incorrect_subtree <- create_strategy_leaves(incorrect)
    
    paste0("(", correct_subtree, "Correct,", incorrect_subtree, "Incorrect)", gaze_name)
  }
  
  off_tree <- create_gaze_subtree(gaze_off, "Off")
  on_tree <- create_gaze_subtree(gaze_on, "On")
  
  newick <- paste0("(", off_tree, ",", on_tree, ");")
  return(newick)
}

# Create simplified newick
simple_newick <- create_simple_newick(sunburst_data_agg)
cat("Simplified newick:\n", simple_newick, "\n\n")

# Extract data and create plot
sb_simple <- sunburst_data(simple_newick)

p_sunburst_simple <- ggplot(sb_simple$rects) +
  geom_rect(
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = name),
    color = "white", 
    size = 0.5
  ) +
  coord_polar() +
  scale_fill_manual(
    values = c(
      # Gaze conditions
      "Off" = "#FFA726", "On" = "#42A5F5",
      # Task conditions  
      "Correct" = "#4CAF50", "Incorrect" = "#FF5722",
      # Strategies
      "Descriptive" = "#FFD180", "Spatial" = "#81D4FA", "Minimal" = "#FF7043"
    ),
    na.value = "grey70"
  ) +
  labs(
    title = "Simplified Sunburst: Gaze ‚Üí Task ‚Üí Strategy",
    subtitle = "Cleaner hierarchical visualization",
    fill = "Level"
  ) +
  theme_void() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.position = "bottom"
  )

print(p_sunburst_simple)

# ============================================================================
# 4. ICICLE PLOT (RECTANGULAR VERSION)
# ============================================================================

# Create icicle plot (rectangular sunburst)
p_icicle <- ggplot(sb_simple$rects) +
  geom_rect(
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = name),
    color = "white", 
    size = 0.5
  ) +
  scale_y_reverse() +  # Important for proper icicle orientation
  scale_fill_manual(
    values = c(
      "Off" = "#FFA726", "On" = "#42A5F5",
      "Correct" = "#4CAF50", "Incorrect" = "#FF5722",
      "Descriptive" = "#FFD180", "Spatial" = "#81D4FA", "Minimal" = "#FF7043"
    ),
    na.value = "grey70"
  ) +
  labs(
    title = "Icicle Plot: Hierarchical Task Duration",
    subtitle = "Rectangular version of the sunburst",
    fill = "Level",
    x = "Proportion", y = "Hierarchy Level"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.position = "right",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

print(p_icicle)

# ============================================================================
# 5. INSPECT THE DATA STRUCTURE
# ============================================================================

# Print the structure of extracted data for debugging
cat("Structure of extracted sunburst data:\n")
str(sb_simple)

cat("\nRectangle data preview:\n")
print(head(sb_simple$rects))

cat("\nUnique names in the data:\n")
print(unique(sb_simple$rects$name))

# ============================================================================
# 6. SUMMARY INFORMATION
# ============================================================================

# Create summary table
summary_stats <- sunburst_data_agg %>%
  group_by(gaze_condition) %>%
  mutate(
    gaze_total = sum(duration),
    gaze_pct = round((duration/gaze_total)*100, 1)
  ) %>%
  ungroup() %>%
  arrange(gaze_condition, task_condition, desc(duration)) %>%
  select(
    `Gaze` = gaze_condition,
    `Task` = task_condition,
    `Strategy` = instructional_strategy,
    `Duration` = duration,
    `Count` = count,
    `% of Gaze` = gaze_pct
  )

cat("\nSummary Statistics:\n")
print(summary_stats)

# ============================================================================
# 7. COMBINED VISUALIZATION
# ============================================================================

# Combine sunburst and icicle for comparison
combined_viz <- p_sunburst_simple | p_icicle
combined_viz <- combined_viz + 
  plot_annotation(
    title = "Hierarchical Task Duration Analysis",
    subtitle = "Sunburst (left) and Icicle (right) representations",
    theme = theme(
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(size = 14, hjust = 0.5)
    )
  )

print(combined_viz)
```

Detect outliers per task_condition + gaze_condition + instructional_strategy
```{r}
library(dplyr)

outliers_cond <- task_durations_cond %>%
  group_by(
    gaze_condition,
    task_condition,
    instructional_strategy
  ) %>%
  mutate(
    Q1    = quantile(mean_task_duration, 0.25),    # lower quartile {üî¢} [quantile](https://www.google.com/search?q=R+quantile)
    Q3    = quantile(mean_task_duration, 0.75),    # upper quartile
    IQR   = Q3 - Q1,                               # interquartile range
    lower = Q1 - 1.5 * IQR,                        # whisker lower bound {üìè} [IQR method](https://www.google.com/search?q=IQR+outlier+detection)
    upper = Q3 + 1.5 * IQR                         # whisker upper bound
  ) %>%
  filter(
    mean_task_duration < lower |
    mean_task_duration > upper
  ) %>%
  ungroup() %>%
  select(
    subject_id,
    instructional_strategy,
    gaze_condition,
    task_condition,
    mean_task_duration
  )

# View the ‚Äúwho‚Äù of your outliers:
print(outliers_cond)
```



Don't run this it doesn't work, it's just 
```{r my_big_plot, fig.width=12, fig.height=6, dpi=300}
# 0. Load libraries
library(dplyr)    # data manipulation üßÆ [dplyr group_by](https://www.google.com/search?q=dplyr+group_by)
library(ggplot2)  # plotting üìä [ggplot2 boxplot](https://www.google.com/search?q=ggplot2+boxplot)

# 1. Compute per-subject median durations, grouped by strategy √ó condition √ó gaze
task_durations_cond <- combined_segments_per_round_all_dt_merged %>%
  group_by(
    subject_id,
    instructional_strategy,
    task_condition,
    gaze_condition      # include for faceting üîç [dplyr group_by multiple](https://www.google.com/search?q=dplyr+group_by+multiple)
  ) %>%
  summarise(
    mean_task_duration = median(task_duration, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    instructional_strategy = factor(
      instructional_strategy,
      levels = c("Descriptive","Spatial","Minimal")
    )
  )

# 2. Detect outliers via boxplot.stats()
outliers_cond <- task_durations_cond %>%
  group_split(gaze_condition, task_condition, instructional_strategy) %>%
  map_dfr(~ {
    # extract numeric outliers for this group
    ovals <- boxplot.stats(.x$mean_task_duration)$out
    filter(.x, mean_task_duration %in% ovals)
  })

# 2b. Alternatively, IQR‚Äêbased detection:
# outliers_cond_iqr <- task_durations_cond %>%
#   group_by(gaze_condition, task_condition, instructional_strategy) %>%
#   mutate(
#     Q1    = quantile(mean_task_duration, 0.25),
#     Q3    = quantile(mean_task_duration, 0.75),
#     IQR   = Q3 - Q1,
#     lower = Q1 - 1.5 * IQR,
#     upper = Q3 + 1.5 * IQR
#   ) %>%
#   filter(mean_task_duration < lower | mean_task_duration > upper) %>%
#   ungroup()

# 3. Inspect the outliers
print(outliers_cond)

# 4. Remove outlier rows from the original data
clean_data <- combined_segments_per_round_all_dt_merged %>%
  # drop any record whose median summary is flagged above
  semi_join(outliers_cond %>%
              select(subject_id, instructional_strategy, task_condition, gaze_condition) %>%
              mutate(flag = TRUE),
            by = c("subject_id", "instructional_strategy", "task_condition", "gaze_condition")) %>%
  anti_join(
    outliers_cond %>%
      select(subject_id, instructional_strategy, task_condition, gaze_condition),
    by = c("subject_id", "instructional_strategy", "task_condition", "gaze_condition")
  )

# 5. Recompute medians on cleaned data
task_durations_clean <- clean_data %>%
  group_by(
    subject_id,
    instructional_strategy,
    task_condition,
    gaze_condition
  ) %>%
  summarise(
    mean_task_duration = median(task_duration, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    instructional_strategy = factor(
      instructional_strategy,
      levels = c("Descriptive","Spatial","Minimal")
    )
  )

# 6. Plot the cleaned boxplot
ggplot(task_durations_clean,
       aes(
         x    = task_condition,
         y    = mean_task_duration,
         fill = instructional_strategy
       )) +
  geom_boxplot(
    position = position_dodge(width = 0.8),  # side-by-side boxes
    width    = 0.6
  ) +
  facet_wrap(
    ~ gaze_condition,
    ncol   = 2                               # two panels
  ) +
  labs(
    title = "Median Task Duration by Strategy and Task Condition\n(Outlier Removed)",
    x     = "Task Condition",
    y     = "Median Duration (s)"
  ) +
  scale_fill_manual(values = c(
    "Descriptive" = "#FFD180",
    "Spatial"     = "#81D4FA",
    "Minimal"     = "#FF7043"
  )) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text      = element_text(size = 12, face = "bold"),
    plot.title      = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.text.x     = element_text(size = 12, face = "bold"),
    axis.text.y     = element_text(size = 12)
  )

```


```{r my_big_plot, fig.width=12, fig.height=6, dpi=300}
# --- Libraries ---------------------------------------------------------------
library(dplyr)    # wrangling  üìö
library(ggplot2)  # plotting   üìä
library(purrr)    # map_dfr    üß∞  # needed for group_split/map_dfr
# refs: dplyr anti_join üîó https://www.google.com/search?q=dplyr+anti_join
#       ggplot facet_wrap üîó https://www.google.com/search?q=ggplot2+facet_wrap

# --- 1) Per-subject medians (strategy √ó task_condition √ó gaze) ---------------
task_durations_cond <- combined_segments_per_round_all_dt_merged %>%
  group_by(subject_id, instructional_strategy, task_condition, gaze_condition) %>%
  summarise(mean_task_duration = median(task_duration, na.rm = TRUE), .groups = "drop") %>%
  mutate(
    instructional_strategy = factor(instructional_strategy, levels = c("Descriptive","Spatial","Minimal")),
    task_condition         = factor(task_condition,         levels = c("Incorrect","Correct")),
    gaze_condition         = factor(gaze_condition,         levels = c("Off","On"))
  )

# --- 2) Outliers per group (boxplot.stats) -----------------------------------
outliers_cond <- task_durations_cond %>%
  group_split(gaze_condition, task_condition, instructional_strategy) %>%
  map_dfr(~{
    outs <- boxplot.stats(.x$mean_task_duration)$out
    filter(.x, mean_task_duration %in% outs)
  })

# If you prefer IQR explicitly, uncomment this and use `outliers_cond_iqr` instead:
# outliers_cond <- task_durations_cond %>%
#   group_by(gaze_condition, task_condition, instructional_strategy) %>%
#   mutate(
#     Q1=quantile(mean_task_duration, .25), Q3=quantile(mean_task_duration, .75),
#     IQR=Q3-Q1, lower=Q1-1.5*IQR, upper=Q3+1.5*IQR
#   ) %>% filter(mean_task_duration<lower | mean_task_duration>upper) %>%
#   ungroup() %>% select(subject_id, instructional_strategy, task_condition, gaze_condition, mean_task_duration)

# Diagnostics
message("# outliers found: ", nrow(outliers_cond))
# print(outliers_cond)

# --- 3A) (Recommended) Drop outliers from the *summary* table ----------------
task_durations_clean <- task_durations_cond %>%
  anti_join(outliers_cond %>% select(subject_id, instructional_strategy, task_condition, gaze_condition) %>% distinct(),
            by = c("subject_id","instructional_strategy","task_condition","gaze_condition"))

# --- 3B) (Alternative) Drop from the *raw* data, then re-summarise -----------
# clean_data <- combined_segments_per_round_all_dt_merged %>%
#   anti_join(outliers_cond %>% select(subject_id, instructional_strategy, task_condition, gaze_condition) %>% distinct(),
#             by = c("subject_id","instructional_strategy","task_condition","gaze_condition"))
# task_durations_clean <- clean_data %>%
#   group_by(subject_id, instructional_strategy, task_condition, gaze_condition) %>%
#   summarise(mean_task_duration = median(task_duration, na.rm = TRUE), .groups = "drop")

# Sanity check to avoid empty data ‚Üí facet error
stopifnot(nrow(task_durations_clean) > 0)

# --- 4) Plot -----------------------------------------------------------------
ggplot(task_durations_clean,
       aes(x = task_condition, y = mean_task_duration, fill = instructional_strategy)) +
  geom_boxplot(position = position_dodge(width = 0.8), width = 0.6) +
  facet_wrap(~ gaze_condition, ncol = 2) +
  labs(
    title = "Median Task Duration by Strategy and Task Condition (Outliers Removed)",
    x = "Task Condition", y = "Median Duration (s)"
  ) +
  scale_fill_manual(values = c("Descriptive"="#FFD180","Spatial"="#81D4FA","Minimal"="#FF7043")) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
        legend.position = "bottom",
        axis.text.x = element_text(size = 12, face = "bold"),
        axis.text.y = element_text(size = 12))

```

```{r my_big_plot, fig.width=12, fig.height=6, dpi=300}
# Ensure desired ordering
task_durations_clean <- task_durations_clean %>%
  dplyr::mutate(
    gaze_condition = factor(gaze_condition, levels = c("Off","On")),
    task_condition = factor(task_condition, levels = c("Incorrect","Correct")),
    instructional_strategy = factor(instructional_strategy,
                                    levels = c("Descriptive","Spatial","Minimal"))
  )

# ‚îÄ‚îÄ Option A: SWAP positions ‚Üí x = Off/On; facet = Incorrect/Correct
# (keep fill = instructional_strategy)
p_swapped <- ggplot(task_durations_clean,
                    aes(x = gaze_condition,
                        y = mean_task_duration,
                        fill = instructional_strategy)) +
  geom_boxplot(position = position_dodge(width = 0.8), width = 0.6) +  # {‚û°Ô∏è} [position_dodge](https://www.google.com/search?q=ggplot2+position_dodge)
  facet_wrap(~ task_condition, ncol = 2) +                              # {üé®} [facet_wrap](https://www.google.com/search?q=ggplot2+facet_wrap)
  labs(
    title = "Median Task Duration by Strategy and Gaze, faceted by Correctness",
    x = "Gaze Condition",
    y = "Median Duration (s)",
    fill = "Instructional Strategy"
  ) +
  scale_fill_manual(values = c("Descriptive"="#FFD180","Spatial"="#81D4FA","Minimal"="#FF7043")) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text      = element_text(size = 12, face = "bold"),
    plot.title      = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.text.x     = element_text(size = 12, face = "bold"),
    axis.text.y     = element_text(size = 12)
  )

print(p_swapped)

# ‚îÄ‚îÄ Option B: SWAP and show Incorrect/Correct side-by-side inside each Off/On group
# (fill = task_condition), facet by instructional strategy
p_swapped_alt <- ggplot(task_durations_clean,
                        aes(x = task_condition,
                            y = mean_task_duration,
                            fill = gaze_condition)) +
  geom_boxplot(position = position_dodge(width = 0.75), width = 0.6) +
  facet_wrap(~ instructional_strategy, nrow = 1) +
  labs(
    title = "Median Task Duration by Gaze (x) and Correctness (fill), faceted by Strategy",
    x = "Gaze Condition",
    y = "Median Duration (s)",
    fill = "Correctness"
  ) +
  scale_fill_manual(values = c("Off" = "#F8766D", "On" = "#00BFC4")) +
  theme_minimal(base_size = 14) +
  theme(
    strip.text      = element_text(size = 12, face = "bold"),
    plot.title      = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    axis.text.x     = element_text(size = 12, face = "bold"),
    axis.text.y     = element_text(size = 12)
  )

print(p_swapped_alt)

```


Detect outliers per task_type + gaze_condition + instructional_strategy
```{r}
# Assuming you‚Äôve already built `task_durations_type2` that includes gaze_condition
outliers_type <- task_durations_type2 %>%
  group_by(
    gaze_condition,
    task_type,
    instructional_strategy
  ) %>%
  mutate(
    Q1    = quantile(mean_task_duration, 0.25),
    Q3    = quantile(mean_task_duration, 0.75),
    IQR   = Q3 - Q1,
    lower = Q1 - 1.5 * IQR,
    upper = Q3 + 1.5 * IQR
  ) %>%
  filter(
    mean_task_duration < lower |
    mean_task_duration > upper
  ) %>%
  ungroup() %>%
  select(
    subject_id,
    instructional_strategy,
    gaze_condition,
    task_type,
    mean_task_duration
  )

print(outliers_type)

```

Okay so a few things
- This only takes into account when the instructor started speaking and ended, not the actual duration for the task.
- Start and end are mixed up in the combined thing for some reason
- So to find the actual task times, we take the start of the task to the start of the next task except for the last one, which we try to use the round end time. Also figure out why round durations don't line up with the task durations. We can technically just look at the words files and calculate the correct time.
- Need to finish going through instructional strategies csv and segments csv to see for anomalies like wrong labels or wrong timings. 
- We only need to modify the non-classifying one, the classifying one is bad data.
- So you can use mean round durations, just keep in mind you will want to reword it later if you do change it and use this one where it would be 
- Sometimes takes into account either ratings but ALSO you need to make it not take into account the times someone spends just speaking out the prompt text as then you will actually see more extreme differences. So when you do that you should rename this to either the task durations or the duration instructor spent speaking in the TVCG paper.
Let's see it for just spatial and minimal
```{r}
# Filter for Spatial and Minimal instructional strategies
task_durations_subset <- task_durations %>%
  filter(instructional_strategy %in% c("Spatial", "Minimal"))

# calculate summary statistics (mean, Q1, Q3)
summary_data_subset <- task_durations_subset %>%
  group_by(gaze_condition, instructional_strategy) %>%
  summarise(
    mean = median(mean_task_duration),
    ymin = quantile(mean_task_duration, 0.25),  # Q1
    ymax = quantile(mean_task_duration, 0.75),  # Q3
    .groups = 'drop'
  )

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data_subset, aes(x = gaze_condition, y = mean, fill = instructional_strategy),
           stat = "identity", position = position_dodge(width = 0.9), width = 0.25, color = "black", alpha = 0.7) +
  
  # 2. Boxplot elements: IQR and median
  geom_errorbar(data = summary_data_subset, aes(x = gaze_condition, ymin = ymin, ymax = ymax, group = instructional_strategy),
                position = position_dodge(width = 0.9), width = 0.2, size = 1) +
  
  # 3. Violin plot
  geom_violin(data = task_durations_subset, aes(x = gaze_condition, y = mean_task_duration, fill = instructional_strategy),
              position = position_dodge(width = 0.9), trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
  geom_jitter(data = task_durations_subset, aes(x = gaze_condition, y = mean_task_duration, color = instructional_strategy),
              position = position_jitterdodge(jitter.width = 0.15, dodge.width = 0.9), size = 2.5, alpha = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  scale_color_manual(values = material_colors) +
  
  # Labels
  labs(title = "Task Duration by Instructional Strategy and Gaze Condition",
       x = "Gaze Condition", 
       y = "Mean Task Duration (s)") +
  
  # Theme
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    axis.text.x = element_text(size = 12, face = "bold"),
    axis.text.y = element_text(size = 12)
  )

# So now in the plot we want to swap instructional_strategy and gaze condition so that it's grouped by instructional strategy



```



```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
combined_segments_per_round_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/timings_updated.csv$") %>%
  map_dfr(.f = function(timings_updated_path) {
    print(timings_updated_path)
    segments_path <- str_replace(timings_updated_path, "timings_updated.csv", "segments.csv")
    timings <- read_csv(timings_updated_path, show_col_types = FALSE)
    segments <- read_csv(segments_path, show_col_types = FALSE)
    
    # Process segments and join with timings
    segments_per_round <- segments %>%
      mutate(text = tolower(text),  # Convert text to lower case
             text = str_trim(text),  # Trim whitespace
             text = replace_contraction(text),  # Replace contractions using textclean
             text = str_replace_all(text, "[[:punct:]]", "")) %>%  # Remove punctuation
      mutate(across(c(start, end), as.numeric))
      
    
    timings_dt <- setDT(timings)
    segments_per_round_dt <- setDT(segments_per_round) %>% .[timings_dt, on = .(start <= end, end >= start), allow.cartesian = TRUE]
    segments_per_round <- segments_per_round_dt %>% as.data.frame()
        
      #right_join(timings, by = character()) %>%
      #filter(start >= start.x & end <= end.x) %>%
      #select(round, task, start = start.x, end = end.x, text)
    segments_per_round <- segments_per_round %>% relocate(round, task, start, end, text)
    # Combine text for each round and task
    combined_text_per_round <- segments_per_round %>%
      group_by(round, task, start, end) %>%
      summarise(combined_text = paste(text, collapse = " "), .groups = 'drop')  # Combine text entries into a single string
    # Now we just want to add in the subject id to the round and task
    subject_id <- substr(basename(dirname(timings_updated_path)), 1, 7)
    combined_text_per_round <- combined_text_per_round %>% mutate(subject_id = subject_id)
    combined_text_per_round
  })
#move subject_id before round
combined_segments_per_round_all <- combined_segments_per_round_all %>% relocate(subject_id, .before = round)
combined_segments_per_round_all

# Save the result to a new CSV file
#write_csv(combined_segments_per_round_all, file.path(path_to_files, "combined_segments_per_round_all.csv"))
```


```{r}

subject_ratings 


aggregated_ratings <- subject_ratings %>%
  mutate(renamed_question = question) %>%  # Initialize with existing questions
  mutate(renamed_question = ifelse(grepl("Question - How confident were you in correctly guessing", question), "Confident", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How easy was it to guide", question), "Ease", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How well were you able to predict", question), "Predict", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your understanding", question), "Focus", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How would you rate your awareness", question), "Awareness", renamed_question)) %>%
  mutate(renamed_question = ifelse(grepl("Question - How complex were your instructions when guiding", question), "Complex", renamed_question))

# So now we want to keep this in a separate variable
processed_ratings <- aggregated_ratings


aggregated_ratings <- aggregated_ratings %>%
  group_by(subject_id, renamed_question, gaze_condition) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")
  
aggregated_ratings2 <- processed_ratings %>%
  group_by(subject_id, renamed_question, task_condition) %>%
  summarise(mean_rating = mean(rating, na.rm = TRUE), .groups = "drop")

  processed_ratings
  aggregated_ratings
  aggregated_ratings2
  # Split the data frame by question
  questions_list <- split(aggregated_ratings, aggregated_ratings$renamed_question)

  # Function to create and save a boxplot and scatter plot for each question
  plot_question <- function(data, question_name) {
    p <- ggplot(data, aes(x = gaze_condition, y = mean_rating, fill = gaze_condition)) +
      geom_boxplot(alpha = 0.5) +  # Set transparency to see scatter points clearly
      geom_point(position = position_jitter(width = 0.1), color = "black", size = 3, alpha = 0.6) +  # Add jitter to avoid overplotting
      labs(title = question_name, x = "Gaze Condition", y = "Mean Rating") +
      theme_classic() +
      scale_y_continuous(limits = c(1, 5), oob = scales::oob_squish)  # Force all data into the specified range

    # Save the plot
    ggsave(paste0("Boxplot_Scatter_", question_name, ".png"), plot = p, width = 10, height = 8, dpi = 300)
  }

```




We process the subject ratings and combine the graphs to create the complete ratings plot.
```{r}
# Reshape data from long to wide format for each question type and gaze condition
wide_ratings <- aggregated_ratings %>%
  pivot_wider(names_from = c(renamed_question, gaze_condition), 
              values_from = mean_rating,
              names_sep = "_")

# Function to perform Wilcoxon signed-rank test for each question type
perform_wilcox_test <- function(data, question_name) {
  data_on <- data[[paste0(question_name, "_On")]]
  data_off <- data[[paste0(question_name, "_Off")]]
  test_result <- wilcox.test(data_on, data_off, paired = TRUE)
  return(test_result)
}

# List of all question types
question_types <- unique(aggregated_ratings$renamed_question)


wide_ratings
# Pivot data for plotting
wide_ratings_for_plots <- aggregated_ratings %>%
  pivot_wider(names_from = renamed_question, values_from = mean_rating)
  
wide_ratings_for_plots <- wide_ratings_for_plots %>% mutate(gaze_condition = factor(gaze_condition, levels = c("Off", "On")))
  
  #Print out that this is results
  print("Results")
  
  awareness_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Awareness ~ gaze_condition, paired=T) 
  awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condition")
  awareness_wilcox
  print("Z for Awareness")
  compute_wilcoxon_signed_rank_summary(awareness_wilcox)
  
  
wide_ratings_for_plots

# Filter data for 'On' and 'Off' conditions for the 'Awareness' variable
x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Awareness)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Awareness)

# Calculate the Wilcoxon Z statistic for paired data
z_awareness <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Awareness:", z_awareness))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How aware were you of the student's attention?", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Confident
confident_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Confident ~ gaze_condition, paired=T)
confident_wilcox.p <- confident_wilcox |> add_x_position(x="gaze_condiition")
confident_wilcox
print("Z for Confident")
compute_wilcoxon_signed_rank_summary(confident_wilcox)

x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Confident)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Confident)

z_confident <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Confident:", z_confident))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Confident)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How confident did you feel when guiding or correcting the student?", x="", y="") +
  stat_pvalue_manual(confident_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Ease

ease_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Ease ~ gaze_condition, paired=T)
ease_wilcox.p <- ease_wilcox |> add_x_position(x="gaze_condiition")
ease_wilcox
print("Z for Ease")
compute_wilcoxon_signed_rank_summary(ease_wilcox)
x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Ease)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Ease)

z_ease <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Ease:", z_ease))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Ease)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How easy was it to guide and correct the student?", x="", y="") +
  stat_pvalue_manual(ease_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Focus

focus_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Focus ~ gaze_condition, paired=T)
focus_wilcox.p <- focus_wilcox |> add_x_position(x="gaze_condiition")
focus_wilcox
print("Z for Focus")
compute_wilcoxon_signed_rank_summary(focus_wilcox)

x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Focus)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Focus)

z_focus <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Focus:", z_focus))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Focus)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How well did you understand the student's focus?", x="", y="") +
  stat_pvalue_manual(focus_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

#Now do it for Predict

predict_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Predict ~ gaze_condition, paired=T)
predict_wilcox.p <- predict_wilcox |> add_x_position(x="gaze_condiition")
predict_wilcox
print("Z for Predict")
compute_wilcoxon_signed_rank_summary(predict_wilcox)

x_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "On") %>% 
  pull(Predict)

y_data <- wide_ratings_for_plots %>% 
  filter(gaze_condition == "Off") %>% 
  pull(Predict)

z_predict <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Predict:", z_predict))

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Predict)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="How well were you able to predict the student's actions?", x="", y="") +
  stat_pvalue_manual(predict_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))



test_result_awareness <- perform_wilcox_test(wide_ratings, "Complex")
test_result_awareness

test_result_confidence <- perform_wilcox_test(wide_ratings, "Confident")
test_result_confidence

test_result_ease <- perform_wilcox_test(wide_ratings, "Ease")
test_result_ease

test_result_focus <- perform_wilcox_test(wide_ratings, "Focus")
test_result_focus

test_result_predict <- perform_wilcox_test(wide_ratings, "Predict")
test_result_predict

awareness_wilcox <- wide_ratings_for_plots |> rstatix::wilcox_test( Awareness ~ gaze_condition, paired=T) 
awareness_wilcox.p <- awareness_wilcox |> add_x_position(x="gaze_condiition")
awareness_wilcox

wide_ratings_for_plots %>% ggplot(aes(x=gaze_condition, y=Awareness)) + 
  geom_count() +
  stat_summary(fun="median", geom="point", colour="orange") +
  coord_cartesian(ylim=c(1,6)) +
  scale_y_continuous(breaks=seq(1,5,1)) +
  theme_classic() +
  labs(title="Awareness", x="", y="") +
  stat_pvalue_manual(awareness_wilcox, label='p', hide.ns = T,
                     y.position = c(5.5))

# Plot w

# Apply the Wilcoxon test to each question type
test_results <- lapply(question_types, function(q) {
  # Perform the test
  test_result <- perform_wilcox_test(wide_ratings, q)
  list(question = q, wilcox_test_result = test_result)
})

# Output the test results
test_results
question_types
wide_ratings
#write_csv(wide_ratings, paste0(path_to_files, "wide_ratings_test.csv"))


# Get a total count of the number of ratings for each question broken down by 1, 2, 3, 4, 5
summed_ratings <- processed_ratings %>%
  group_by(gaze_condition, renamed_question, rating) %>%
  summarise(count = n(), .groups = "drop")

summed_ratings

# Now we want horizontal stacked bar charts for each question type with the legend on the top-center as discrete values

# Plot the stacked bar chart for each question type, so the x-axis is the count, y-axis is the gaze condition, and we have 'stacked' grouped bar charts for each rating to show distribution. Let's start with Awareness


# Define a darker grayscale theme for each rating number using material design shades
rating_colors <- c(
  "1" = "#E0E0E0",  # Material light gray (darker)
  "2" = "#BDBDBD",  # Material gray
  "3" = "#9E9E9E",  # Material medium gray
  "4" = "#757575",  # Material dark gray
  "5" = "#424242"   # Material very dark gray (darker)
)

rating_labels <- c("1 (worst)", "2", "3", "4", "5 (best)")

# Filter the data for Awareness
awareness_data <- summed_ratings %>% filter(renamed_question == "Awareness")
# Ensure the rating is a factor with levels ordered from 1 to 5
awareness_data$rating <- factor(awareness_data$rating, levels = c("1", "2", "3", "4", "5"))

# Create the plot for Awareness
awareness_plot <- ggplot(awareness_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.5) +  # Adjust bar width
  scale_fill_manual(values = rating_colors, labels = rating_labels) +
  labs(title = "Awareness Ratings by Gaze Condition", x = "", y = "Awareness") +
  scale_y_discrete(labels = c("off" = "Gaze Off", "on" = "Gaze On")) +
  theme_minimal() +
  theme(legend.position = "top", legend.justification = "center", 
        axis.text.y = element_text(size = 8),  # Reduce font size
        axis.title.y = element_text(size = 10))

# Plot the Awareness data
awareness_plot


# Filter the data for Awareness and ensure the rating is a factor
awareness_data <- summed_ratings %>% filter(renamed_question == "Awareness")
awareness_data$rating <- factor(awareness_data$rating, levels = c("1", "2", "3", "4", "5"))
awareness_plot <- ggplot(awareness_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.95, color="black") +
  scale_fill_manual(values = rating_colors, labels = rating_labels) +
  labs(title = "", x = "", y = "Awareness") +
  scale_y_discrete(labels = c("off" = "Gaze Off", "on" = "Gaze On")) +
  theme_minimal() +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = seq(0, 150, by = 25)) + # Fixed breaks set to every 20 units up to 150
  theme(legend.position = "top", legend.justification = "center", 
        axis.text.y = element_text(size = 8), 
        axis.title.y = element_text(size = 10))

# Repeat for other question types
# Confident
confident_data <- summed_ratings %>% filter(renamed_question == "Confident")
confident_data$rating <- factor(confident_data$rating, levels = c("1", "2", "3", "4", "5"))
confident_plot <- ggplot(confident_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.95, color="black") +
  scale_fill_manual(values = rating_colors, labels = rating_labels) +
  labs(title = "", x = "", y = "Confident") +
  theme_minimal() +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = seq(0, 150, by = 25)) + # Fixed breaks set to every 20 units up to 150
  theme(legend.position = "none")

# Ease
ease_data <- summed_ratings %>% filter(renamed_question == "Ease")
ease_data$rating <- factor(ease_data$rating, levels = c("1", "2", "3", "4", "5"))
ease_plot <- ggplot(ease_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.95, color="black") +
  scale_fill_manual(values = rating_colors, labels = rating_labels) +
  labs(title = "", x = "", y = "Ease") +
  theme_minimal() +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = seq(0, 150, by = 25)) + # Fixed breaks set to every 20 units up to 150
  theme(legend.position = "none")

# Focus
focus_data <- summed_ratings %>% filter(renamed_question == "Focus")
focus_data$rating <- factor(focus_data$rating, levels = c("1", "2", "3", "4", "5"))
focus_plot <- ggplot(focus_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.95, color="black") +
  scale_fill_manual(values = rating_colors, labels = rating_labels) +
  labs(title = "", x = "", y = "Focus") +
  theme_minimal() +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = seq(0, 150, by = 25)) + # Fixed breaks set to every 20 units up to 150
  theme(legend.position = "none")

# Predict
predict_data <- summed_ratings %>% filter(renamed_question == "Predict")
predict_data$rating <- factor(predict_data$rating, levels = c("1", "2", "3", "4", "5"))
predict_plot <- ggplot(predict_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.95, color="black") +
  scale_fill_manual(values = rating_colors, labels = rating_labels) +
  labs(title = "", x = "", y = "Predict") +
  theme_minimal() +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = seq(0, 150, by = 25)) + # Fixed breaks set to every 20 units up to 150
  theme(legend.position = "none")

# Combine plots using patchwork
combined_plot <- awareness_plot / confident_plot / ease_plot / focus_plot / predict_plot +
  plot_layout(heights = c(1, 1, 1, 1, 1))  # Equal heights for each plot
combined_plot

summed_ratings
# Sum ratings for off and on conditions for each renamed question
summed_ratings_by_condition <- summed_ratings %>%
  group_by(renamed_question, gaze_condition) %>%
  summarise(count = sum(count), .groups = "drop")
summed_ratings_by_condition

#So there seems to be one extra value for each gaze_condition off question. Which subject_id is this? Can we figure out which subject_id has no corresponding gaze_condition 'on'? If we count responses by each subject_id it should show 4 for each gaze condition
# Count the number of responses for each subject_id and gaze_condition
responses_by_subject <- processed_ratings %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(count = n(), .groups = "drop")

responses_by_subject
#What's the total number of responses overall for awareness?
total_responses <- summed_ratings %>% filter(renamed_question == "Awareness") %>% summarise(total_responses = sum(count))
total_responses
```
```{r awareness-plot, fig.height=2, fig.width=8}
# Filter the data for Awareness
awareness_data <- summed_ratings %>% filter(renamed_question == "Awareness")
# Ensure the rating is a factor with levels ordered from 1 to 5
awareness_data
awareness_data$rating <- factor(awareness_data$rating, levels = c("1", "2", "3", "4", "5"))
ggplot(awareness_data, aes(x = count, y = gaze_condition, fill = rating)) +
  geom_bar(stat = "identity", position = position_stack(reverse = TRUE), width = 0.95, color="black") +  # Adjust bar width
  scale_fill_manual(values = rating_colors) +
  labs(title = "", x = "", y = "Awareness") +
  scale_y_discrete(labels = c("off" = "Gaze Off", "on" = "Gaze On")) +
  theme_minimal() +
  scale_x_continuous(expand = expansion(mult = c(0, 0.05)), breaks = seq(0, 150, by = 25)) + # Fixed breaks set to every 20 units up to 150
  theme(legend.position = "top", legend.justification = "center", 
        axis.text.y = element_text(size = 8),  # Reduce font size
        axis.title.y = element_text(size = 10))
```

TODO: The combined plot needs to have the legend labelled from 1 being the lowest to 5 being the highest.

```{r combined-plot, fig.height=8, fig.width=8}
combined_plot <- awareness_plot / confident_plot / ease_plot / focus_plot / predict_plot
combined_plot
```


Here we will want to calculate the mean duration of each round for each subject and gaze condition. We will then perform a Wilcoxon signed-rank test to compare the mean durations between the 'Off' and 'On' gaze conditions.


--Mean Duration of Each Round by Gaze Condition

```{r}

# Step 1: Group by subject_id and round_id to get the minimum unity_log_time along with gaze_condition
round_times <- subject_ratings %>%
  group_by(subject_id, round_id) %>%
  summarise(min_unity_log_time = min(unity_log_time, na.rm = TRUE),
            gaze_condition = first(gaze_condition)) %>%
  ungroup()

# Step 2: Calculate the duration of each round
# We use the `lag` function to get the previous round's min_unity_log_time
round_durations <- round_times %>%
  arrange(subject_id, round_id) %>%
  group_by(subject_id) %>%
  mutate(previous_time = lag(min_unity_log_time),
         round_duration = if_else(is.na(previous_time), min_unity_log_time, min_unity_log_time - previous_time))

round_durations

mean_durations <- round_durations %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE))

mean_durations

# Step 4: Reshape data from long to wide format
wide_mean_durations <- mean_durations %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = mean_duration,
              names_prefix = "mean_duration_")

wide_mean_durations


# Assuming wide_mean_durations is already loaded with your data
# Ensure it's in the correct format
wide_mean_durations <- wide_mean_durations %>%
  pivot_longer(cols = starts_with("mean_duration"), names_to = "gaze_condition", values_to = "mean_duration") %>%
  mutate(gaze_condition = sub("mean_duration_", "", gaze_condition))

# Make it a data frame
wide_mean_durations <- wide_mean_durations %>% as.data.frame()
wide_mean_durations
# Perform Wilcoxon signed-rank test using rstatix
duration_wilcox <- wide_mean_durations %>%
  rstatix::wilcox_test(mean_duration ~ gaze_condition, paired = TRUE)
duration_wilcox.p <- duration_wilcox |> add_x_position(x="gaze_condition")
duration_wilcox 
print("Z for Duration")
compute_wilcox_summary(duration_wilcox)
x_data <- wide_mean_durations %>% 
  filter(gaze_condition == "On") %>% 
  pull(mean_duration)
y_data <- wide_mean_durations %>% 
  filter(gaze_condition == "Off") %>% 
  pull(mean_duration)

z_duration <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
z_duration
print(paste("Z for Duration:", z_duration))


```


So now we want similar to wide_mean_durations but we want to use instructional_strategies_data_dt_merged_tt_tc to get the mean duration of each round for each subject and gaze condition and the instructional strategy used.
```{r}

instructional_strategies_data_dt_merged_tt_tc
round_durations
mean_durations
```




```{r}


# Perform Wilcoxon signed-rank test
wilcox_test_result <- wilcox.test(
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "Off"],
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "On"],
  paired = TRUE
)

# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 350, 
  label = paste("p-value:", formatC(wilcox_test_result$p.value, format = "e", digits = 2))
)

# Plotting the results
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 50)) +
  labs(title = "Comparison of Mean Round (3 Tasks) Durations by Gaze Condition", x = "Gaze Condition", y = "Mean Duration") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```



```{r}
summary_data <- wide_mean_durations %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = mean(mean_duration),
    se = sd(mean_duration) / sqrt(n())
  )

# Define p-value annotation position
p_value_label <- data.frame(
  x = 1.5, 
  y = max(summary_data$mean + summary_data$se) + 10, 
  label = "p = .004"  # Use formatted p-value manually or dynamically
)

summary_data

# Create the plot
ggplot(summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition)) +
  geom_bar(stat = "identity", width = 0.2, color = "black") +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.1) +
  scale_fill_manual(values = c("Off" = "blue", "On" = "orange")) +
  geom_text(data = p_value_label, aes(x = x, y = y, label = label), 
            color = "red", size = 5, fontface = "bold", inherit.aes = FALSE) +
  annotate("segment", x = 1, xend = 2, y = p_value_label$y - 5, 
           yend = p_value_label$y - 5, color = "red", size = 1) +
  labs(title = "Mean Round Durations by Gaze Condition",
       x = "Gaze Condition", 
       y = "Mean Duration",
       fill = NULL) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 12),
    legend.position = "none"
  )



```

We want to make a nicer plot version.

```{r}
library(ggplot2)
library(dplyr)

# Calculate summary statistics (mean and IQR)
summary_data <- wide_mean_durations %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = median(mean_duration),
    ymin = quantile(mean_duration, 0.25),  # Q1
    ymax = quantile(mean_duration, 0.75)   # Q3
  )

# Correct P-value annotation
wilcox_test_result <- wilcox.test(
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "Off"],
  wide_mean_durations$mean_duration[wide_mean_durations$gaze_condition == "On"],
  paired = TRUE
)
p_value_data <- data.frame(
  x = 1.5, 
  y = 350, 
  label = paste("p =", formatC(wilcox_test_result$p.value, format = "f", digits = 8))
)

p_value_data$label = "p < 0.001"

# Lighter material colors
material_colors <- c("Off" = "#FFD180", "On" = "#81D4FA")

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition),
           stat = "identity", width = 0.36, color = "black", alpha = 0.7) +
  
  # 2. Boxplot elements: IQR and median
stat_boxplot(
  data = wide_mean_durations, 
  aes(
    x = gaze_condition, 
    y = mean_duration,
    ymin = after_stat(ymin), 
    ymax = after_stat(ymax)
  ), 
  geom = "errorbar",
  linewidth = 1, 
  width = 0.15
) +
stat_boxplot(
  data = wide_mean_durations, 
  aes(
    x = gaze_condition, 
    y = mean_duration,
    ymin = ..middle.., 
    ymax = ..middle..
  ), 
  geom = "errorbar",
  linewidth = 2, 
  width = 0.25
) +
  # 3. Violin plot
  geom_violin(data = wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
geom_jitter(
  data = wide_mean_durations, 
  aes(x = gaze_condition, y = mean_duration, color = gaze_condition),
  width = 0.15, 
  size = 2.5, 
  alpha = 1  # Set to 1 for fully opaque points
) +
scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +  # Darker shades of orange and blue
  
  # 5. P-value annotation
  geom_text(data = p_value_data, aes(x = x, y = y+10, label = label), 
            inherit.aes = FALSE, color = "red", size = 5, fontface = "bold") +
  annotate("segment", x = 1, xend = 2, y = p_value_data$y - 10, 
           yend = p_value_data$y - 10, color = "red", size = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  
  # Labels
  labs(title = "",
       x = "Gaze Condition", 
       y = "Mean Duration (s)") +
  
  # Theme
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.x = element_text(size = 12, face = "bold")
  )

```



```{r duration-plot, fig.height=3, fig.width=8}
# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition),
           stat = "identity", width = 0.5, color = "black", alpha = 0.7, position = position_dodge2(preserve = "single", padding = 0)) +
  
  # 2. Boxplot elements: IQR and median

stat_boxplot(
  data = wide_mean_durations, 
  aes(
    x = gaze_condition, 
    y = mean_duration,
    ymin = ..middle.., 
    ymax = ..middle..
  ), 
  geom = "errorbar",
  linewidth = 2, 
  width = 0.75
) +
  # 3. Violin plot
  geom_violin(data = wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
geom_jitter(
  data = wide_mean_durations, 
  aes(x = gaze_condition, y = mean_duration, color = gaze_condition),
  width = 0.15, 
  size = 2.5, 
  alpha = 1  # Set to 1 for fully opaque points
) +
  stat_boxplot(
  data = wide_mean_durations, 
  aes(
    x = gaze_condition, 
    y = mean_duration,
    ymin = after_stat(ymin), 
    ymax = after_stat(ymax)
  ), 
  geom = "errorbar",
  linewidth = 1, 
  width = 0.15
) +
scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +  # Darker shades of orange and blue
  
  # 5. P-value annotation
  geom_text(data = p_value_data, aes(x = x+0.75, y = y-30, label = label), 
            inherit.aes = FALSE, color = "red", size = 5, fontface = "bold") +
  annotate("segment", x = 1, xend = 2, y = p_value_data$y - 10, 
           yend = p_value_data$y - 10, color = "red", size = 1) +
  
  # Scaling
  scale_y_continuous(breaks = seq(0, 350, by = 50)) +


  # Custom colors
  scale_fill_manual(values = material_colors) +
  # Labels
  labs(title = "",
       x = "Gaze Condition", 
       y = "Mean Duration (s)") +
  
  # Theme
  theme_minimal(base_size = 14) +
  coord_flip() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.x = element_text(size = 12, face = "bold")
  )
```

```{r}
library(ggplot2)

# Example with flipped axes
ggplot() +
  # Horizontal bar elements; flip x and y in aes
  geom_bar(data = summary_data, aes(y = gaze_condition, x = mean, fill = gaze_condition),
           stat = "identity", width = 0.5, color = "black", alpha = 0.7, position = position_dodge2(preserve = "single", padding = 0)) +
  
  # Horizontal violin plot
  geom_violin(data = wide_mean_durations, aes(y = gaze_condition, x = mean_duration, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +

  # Horizontal jittered dots for individual points
  geom_jitter(
    data = wide_mean_durations, 
    aes(y = gaze_condition, x = mean_duration, color = gaze_condition),
    width = 0.15, 
    size = 2.5, 
    alpha = 1
  ) +

  # Horizontal boxplot elements: error bars
  geom_errorbar(
    data = wide_mean_durations, 
    aes(y = gaze_condition, 
        xmin = after_stat(ymin), 
        xmax = after_stat(ymax),
        x = after_stat(middle)),
    width = 0.5, 
    size = 2
  ) +

  # Additional error bars for IQR
  geom_errorbar(
    data = wide_mean_durations, 
    aes(y = gaze_condition, 
        xmin = after_stat(ymin), 
        xmax = after_stat(ymax),
        x = after_stat(middle)),
    width = 0.15, 
    size = 1
  ) +
  
  scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +
  scale_fill_manual(values = material_colors) +
  
  # Adjust y-axis scaling to be horizontal
  scale_x_continuous(breaks = seq(0, 350, by = 50)) +

  # Labels for horizontal layout
  labs(title = "",
       y = "Gaze Condition", 
       x = "Mean Duration (s)") +

  # Apply minimal theme and adjust text elements
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.y = element_text(size = 12, face = "bold")
  )

```



```{r}
library(ggplot2)
data(mtcars)
ggplot(wide_mean_durations, aes(x=as.factor(gaze_condition), y=mean_duration)) + 
  stat_boxplot(aes(ymin = after_stat(ymin), ymax = after_stat(ymax)), geom="errorbar", width=.25) + 
  stat_boxplot(aes(ymin = after_stat(middle), ymax = after_stat(middle)), geom="errorbar", linewidth=2, width=.25)  + 
  geom_point(aes(colour=as.factor(gaze_condition)), position=position_jitter(width=.25)) + 
  theme_classic()
```


```{r}
summary_data <- wide_mean_durations %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = mean(mean_duration),
    sd = sd(mean_duration)  # Standard deviation
  )

# Adjust error bars for SD
ggplot(summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition)) +
  geom_bar(stat = "identity", width = 0.8, color = "black") +
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.2) +
  scale_fill_manual(values = c("Off" = "blue", "On" = "orange")) +
  geom_text(data = p_value_label, aes(x = x, y = y, label = label), 
            color = "red", size = 5, fontface = "bold", inherit.aes = FALSE) +
  annotate("segment", x = 1, xend = 2, y = p_value_label$y - 5, 
           yend = p_value_label$y - 5, color = "red", size = 1) +
  labs(title = "Mean Round Durations by Gaze Condition",
       x = "Gaze Condition", 
       y = "Mean Duration",
       fill = NULL) +
  theme_minimal()
```

Paper-friendly version of the plot that can be customized.
```{r duration-plot, fig.height=3, fig.width=3}
# Plotting the results. We don't want the p-value label. We don't want to have the x-axis labeled as Gaze Condition. The x-axis labels should instead be "Gaze Off" and "Gaze On". We do not want the legend. We do not want the title. Finally, we want to have error bars show up as well.

wide_mean_durations
# Calculate summary statistics
summary_round_durations <- round_durations %>%
  group_by(gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE),
            sd_duration = sd(round_duration, na.rm = TRUE))

summary_round_durations
# Plot using wide_mean_durations for the boxplot and jitter, and summary_mean_durations for error bars with horizontal caps
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot(width = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5) +
  geom_errorbar(data = summary_round_durations, aes(ymin = mean_duration - sd_duration, ymax = mean_duration + sd_duration, x = gaze_condition), width = 0.6, color = "black") +
  # Adding horizontal end caps
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration - sd_duration, yend = mean_duration - sd_duration), color = "black") +
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration + sd_duration, yend = mean_duration + sd_duration), color = "black") +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 100)) +
  labs(title = "", x = "", y = "Mean Duration (in seconds)") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))
```





```{r}
# UHH don't run this unless you need to recalculate timings
# Now we want to compute end_time for timings.csv files that do not have that column
# Define the path to the root directory containing the subdirectories

find_end_time <- function(start_time, round, task, timings, segments) {
  print(paste("Processing task:", task, "in round:", round, "with start time:", start_time))
  
  # Find the start time of the next task in the same round
  next_task_start_times <- timings %>%
    filter(round == round, task == task, start > start_time) %>%
    arrange(task) %>%
    pull(start)

  print("Next task start times:")
  print(next_task_start_times)

  # Determine the next task start time or use a large number if it's the last task
  next_task_start_time <- if (length(next_task_start_times) > 0) next_task_start_times[1] else Inf

  print("Next task start time:")
  print(next_task_start_time)

  # Find the last segment that starts before the next task starts and ends after the current task's start time
  valid_segments <- segments %>%
    filter(start >= start_time, start < next_task_start_time)

  print("Valid segments:")
  print(valid_segments)

  if (nrow(valid_segments) > 0) {
    # Find the maximum end time among segments that start before the next task
    end_time <- max(valid_segments$end)
  } else {
    # If no valid segments, use the maximum end time of any segment that starts after this task's start time
    end_time <- max(segments %>% filter(start >= start_time) %>% pull(end), na.rm = TRUE)
  }

  print("Calculated end time:")
  print(end_time)

  return(end_time)
}

# Function to process each directory
process_directory <- function(directory_path) {
  segments_path <- file.path(directory_path, "segments.csv")
  timings_path <- file.path(directory_path, "timings.csv")
  
  if (file.exists(segments_path) && file.exists(timings_path)) {
    segments <- read_csv(segments_path)
    timings <- read_csv(timings_path)

    if (!"end" %in% names(timings)) {
      print("Processing timings file without end time for directory:")
      print(directory_path)
      
      # Calculate end times for each row in timings
      timings$end <- mapply(function(start, round, task) {
        find_end_time(start, round, task, timings, segments)
      }, timings$start, timings$round, timings$task)

      # Save the updated timings file in the same directory
      print("Writing updated timings file...")
      print(file.path(directory_path, "timings_updated.csv"))
      
      #write_csv(timings, file.path(directory_path, "timings_updated.csv"))
    }
  }
}
# List all directories under the root path and process each one
directories <- fs::dir_ls(path_to_files, recurse = TRUE, type = "directory")
walk(directories, process_directory)

```


```{r}
directory_path <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\QWE-042"
timings_path <- file.path(directory_path, "timings_updated.csv")
words_path <- file.path(directory_path, "words.csv")
participant_data_path <- file.path(directory_path, "participantData.csv")
print(directory_path)
  
timings <- read_csv(timings_path, na = na_strings, show_col_types = FALSE)
words <- read_csv(words_path, na = na_strings, show_col_types = FALSE)
participant_data <- read_csv(participant_data_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
print(timings_path)
    #timings
    #words
    #participant_data

    # Extract subject_id
    subject_id <- participant_data$subject_id

    
    # Clean and standardize words
    words <- words %>%
      mutate(word = tolower(word),  # Convert to lower case
             word = str_trim(word),  # Trim whitespace
             word = replace_contraction(word),  # Standardize contractions
             word = str_replace_all(word, "[[:punct:]]", ""))  # Remove punctuation using stringr

    #words
    #print("Words data after cleaning:")
    #print(head(words))

    # Merge words data with timings based on overlapping intervals
    words_timings <- words %>%
      mutate(across(c(start, end), as.numeric)) %>%
      full_join(timings, by = character()) %>%
      rename(start_word = start.x, end_word = end.x, start_timing = start.y, end_timing = end.y)
    words_timings


    # Apply filter to match words within the timings intervals
    words_timings <- words_timings %>%
      filter(start_word >= start_timing & end_word <= end_timing)
  
    # Count words for each round and task
    word_counts <- words_timings %>%
      group_by(round, task, word) %>%
      summarise(count = n(), .groups = 'drop')
    
    word_counts
      
    # Add subject_id to each row
    word_counts <- word_counts %>%
      mutate(subject_id = subject_id) %>%
      rename(round_id = round, task_id = task) %>%
      relocate(subject_id, .before = round_id) %>%
      arrange(round_id, task_id, desc(count))
    
    word_counts
    participant_data
    subject_rounds

    subject_rounds <- subject_rounds %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    word_counts <- word_counts %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining
    #display the types of subject_rounds and word_counts
    print("Subject Rounds")
    
    

    # Perform the join
    word_counts <- inner_join(subject_rounds, word_counts, by = c("subject_id", "round_id", "task_id"))
    word_counts

    # Optionally, save the result in the same directory
    write_csv(word_counts, file.path(directory_path, "word_counts.csv"))


```



```{r}


process_directory <- function(directory_path) {
  timings_path <- file.path(directory_path, "timings_updated.csv")
  words_path <- file.path(directory_path, "words.csv")
  participant_data_path <- file.path(directory_path, "participantData.csv")
  print(directory_path)
  
  if (file.exists(timings_path) && file.exists(words_path)) {
    timings <- read_csv(timings_path, na = na_strings, show_col_types = FALSE)
    words <- read_csv(words_path, na = na_strings, show_col_types = FALSE)
    participant_data <- read_csv(participant_data_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
    print(timings_path)
    timings
    words
    participant_data

    # Extract subject_id
    subject_id <- participant_data$subject_id

    
    # Clean and standardize words
    words <- words %>%
      mutate(word = tolower(word),  # Convert to lower case
             word = str_trim(word),  # Trim whitespace
             word = replace_contraction(word),  # Standardize contractions
             word = str_replace_all(word, "[[:punct:]]", ""))  # Remove punctuation using stringr

    words
    print("Words data after cleaning:")
    print(head(words))

    # Merge words data with timings based on overlapping intervals
    words_timings <- words %>%
      mutate(across(c(start, end), as.numeric)) %>%
      full_join(timings, by = character()) %>%
      rename(start_word = start.x, end_word = end.x, start_timing = start.y, end_timing = end.y)

    print("Data after merging and renaming:")
    print(head(words_timings))

    # Apply filter to match words within the timings intervals
    words_timings <- words_timings %>%
      filter(start_word >= start_timing & end_word <= end_timing)

    print("Data after filtering:")
    print(head(words_timings))

    # Count words for each round and task
    word_counts <- words_timings %>%
      group_by(round, task, word) %>%
      summarise(count = n(), .groups = 'drop')
      
    # Add subject_id to each row
    word_counts <- word_counts %>%
      mutate(subject_id = subject_id) %>%
      rename(round_id = round, task_id = task) %>%
      relocate(subject_id, .before = round_id) %>%
      arrange(round_id, task_id, desc(count))


    print("Aggregated word counts:")
    print(head(word_counts))


    subject_rounds <- subject_rounds %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    word_counts <- word_counts %>%
      mutate(across(c(subject_id, round_id, task_id), as.character))  # Convert keys to character for consistent joining

    

    # Perform the join
    word_counts <- inner_join(subject_rounds, word_counts, by = c("subject_id", "round_id", "task_id"))
    word_counts
    # Optionally, save the result in the same directory
    write_csv(word_counts, file.path(directory_path, "word_counts.csv"))
    
    return(word_counts)
  } else {
    warning("Timings or words file missing in ", directory_path)
  }
}

# List all directories under the root path and process each one
directories <- dir_ls(path_to_files, recurse = TRUE, type = "directory")
results <- map(directories, process_directory)

# Combine all results into a single data frame
#final_results <- bind_rows(results)

```

TODO: Plot the word counts for each condition and round.

Start off by figuring out % of time they spent looking at the terrain in each condition/task.

Run this code if you are having errors about incompatible join types.
```{r}
subject_rounds <- subject_rounds %>%
  mutate(across(c(round_id, task_id), as.double))  # Convert keys to double
```

First we do a test with one file

```{r}
#subject_rounds
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")

#Change the round_id type to character

terrain_path <- paste0(path_to_files, "DIB-019\\DIB-019_terrain_fixed.csv")
terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))
terrain_dt <- terrain %>% setDT() %>%
  .[round_id != 0]

#terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
terrain_dt

# Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
terrain_dt <- terrain_dt %>% 
  mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
  filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
  select(-task_id_mod3)  # Remove the helper column

terrain_dt_grouped <- terrain_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_syncs = n(),  # Total number of sync_id entries
    valid_hits = sum(hit_valid, na.rm = TRUE),  # Count of TRUE in hit_valid
    percent_looking_at_terrain = (valid_hits / total_syncs) * 100  # Calculate percentage
  ) %>%
  ungroup()

terrain_dt_grouped
# Calculate the average percentage of time looking at the terrain for each gaze condition
average_percent_by_condition <- terrain_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_percent_looking_at_terrain = mean(percent_looking_at_terrain, na.rm = TRUE)
  ) %>% ungroup()

average_percent_by_condition

```


Mean head_angle_using_dir and eye_angle_using_dir for each condition

```{r}
# Calculate the mean head_angle_using_dir and eye_angle_using_dir for each condition
terrain_dt
# For head_angle_using_dir and eye_angle_using_dir, find the maximum value that is not -10000 and then use that instead of -10000

#First get the maximum for each and add a new column with those, PMAX DOES NOT WORK


max_head_angle <- terrain_dt %>% filter(head_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle = max(head_angle_using_dir, na.rm = TRUE)) %>% ungroup()

max_eye_angle <- terrain_dt %>% filter(eye_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_eye_angle = max(eye_angle_using_dir, na.rm = TRUE)) %>% ungroup()

max_distance <- terrain_dt %>% filter(distance != 10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_distance = max(distance, na.rm = TRUE)) %>% ungroup()

angles_terrain_dt <- merge(terrain_dt, max_head_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
angles_terrain_dt <- merge(angles_terrain_dt, max_eye_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
angles_terrain_dt <- merge(angles_terrain_dt, max_distance, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)

# Now find all -10000 values in eye_angle_using_dir and head_angle_using_dir and replace them with the max values

angles_terrain_dt <- angles_terrain_dt %>% 
  mutate(head_angle_using_dir = ifelse(head_angle_using_dir == -10000, max_head_angle, head_angle_using_dir),
         eye_angle_using_dir = ifelse(eye_angle_using_dir == -10000, max_eye_angle, eye_angle_using_dir),
         distance = ifelse(distance == 10000, max_distance, distance))

  
average_angles_by_condition <- angles_terrain_dt %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    mean_head_angle_using_dir = mean(head_angle_using_dir, na.rm = TRUE),
    mean_eye_angle_using_dir = mean(eye_angle_using_dir, na.rm = TRUE),
    mean_distance = mean(distance, na.rm = TRUE)
  ) %>% ungroup()

average_angles_by_condition


```


#TODO: Some other things to investigate - use head_angle_using_dir and eye_angle_using_dir for angles
Can check the distance fields to see how much movement their eyes did on the terrain
Mean head_angle_using_dir and eye_angle_using_dir for each condition
delta angles should be in angle per second, so we can calculate the average delta angle per second for each condition.


The all files version.

```{r}

# Process each terrain file and calculate the average percentage of time looking at the terrain
subject_rounds_dt <- subject_rounds %>%
  mutate(across(c(round_id, task_id), as.double)) %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_percent_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_terrain_fixed\\.csv$") %>%
  map_dfr(.f = function(terrain_path) {
    print(terrain_path)
    terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
          mutate(subject_id = substr(subject_id, 1, 7))
    terrain_dt <- terrain %>% setDT() %>%
      .[round_id != 0]

    #terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]
    
    terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
        
    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    terrain_dt <- terrain_dt %>% 
      mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
      filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      select(-task_id_mod3)  # Remove the helper column    
    
    
    terrain_dt <- terrain_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_syncs = n(),  # Total number of sync_id entries
        valid_hits = sum(hit_valid, na.rm = TRUE),  # Count of TRUE in hit_valid
        percent_looking_at_terrain = (valid_hits / total_syncs) * 100  # Calculate percentage
      ) %>%
      ungroup()
    terrain_dt

    # Calculate the average percentage of time looking at the terrain for each gaze condition
    average_percent_by_condition <- terrain_dt %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_percent_looking_at_terrain = median(percent_looking_at_terrain, na.rm = TRUE)
      ) %>% ungroup()

    average_percent_by_condition
  })
average_percent_by_condition_all

# Process each terrain file and calculate the average head_angle_using_dir and eye_angle_using_dir for each condition
average_angles_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_terrain_fixed\\.csv$") %>%
  map_dfr(.f = function(terrain_path) {
    print(terrain_path)
    terrain <- read_csv(terrain_path, na = na_strings, show_col_types = FALSE) %>% clean_names() %>%
          mutate(subject_id = substr(subject_id, 1, 7))
    terrain_dt <- terrain %>% setDT() %>%
      .[round_id != 0]

    #terrain_dt <- terrain_dt[subject_rounds, on = c("subject_id", "round_id", "task_id")]

    terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    terrain_dt <- terrain_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>% .[, gaze_condition.x := NULL] %>% .[, task_condition.x := NULL]  %>% .[, gaze_condition.y := NULL] %>% .[, task_condition.y := NULL] %>% as.data.frame()
    terrain_dt
    
    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    terrain_dt <- terrain_dt %>% 
      mutate(task_id_mod3 = task_id %% 3 == 0) %>%  # Identify task_ids that are multiples of 3
      filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      select(-task_id_mod3)  # Remove the helper column
    
    # Calculate the mean head_angle_using_dir and eye_angle_using_dir for each condition
    max_head_angle <- terrain_dt %>% filter(head_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle = max(head_angle_using_dir, na.rm = TRUE)) %>% ungroup()
    
    max_eye_angle <- terrain_dt %>% filter(eye_angle_using_dir != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_eye_angle = max(eye_angle_using_dir, na.rm = TRUE)) %>% ungroup()

    max_distance <- terrain_dt %>% filter(distance != 10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_distance = max(distance, na.rm = TRUE)) %>% ungroup()
    
    max_head_angle2 <- terrain_dt %>% filter(head_angle != -10000) %>% group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>% summarise(max_head_angle2 = max(head_angle, na.rm = TRUE)) %>% ungroup()
    
    
    angles_terrain_dt <- merge(terrain_dt, max_head_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_eye_angle, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_distance, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    angles_terrain_dt <- merge(angles_terrain_dt, max_head_angle2, by = c("subject_id", "round_id", "task_id", "task_type", "gaze_condition", "task_condition"), all.x = TRUE)
    
    
    # Now find all -10000 values in eye_angle_using_dir and head_angle_using_dir and replace them with the max values
    
    angles_terrain_dt <- angles_terrain_dt %>% 
      mutate(head_angle_using_dir = ifelse(head_angle_using_dir == -10000, max_head_angle, head_angle_using_dir),
             eye_angle_using_dir = ifelse(eye_angle_using_dir == -10000, max_eye_angle, eye_angle_using_dir),
             distance = ifelse(distance == 10000, max_distance, distance))
    
      
    average_angles_by_condition <- angles_terrain_dt %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        mean_head_angle_using_dir = median(head_angle_using_dir, na.rm = TRUE),
        mean_eye_angle_using_dir = median(eye_angle_using_dir, na.rm = TRUE),
        mean_distance = median(distance, na.rm = TRUE),
        mean_head_angle2 = median(head_angle, na.rm = TRUE)
      ) %>% ungroup()
    
    average_angles_by_condition
  })



```

Percentage for all conditions
```{r}
# Now pivot it wider
average_percent_by_condition_all <- average_percent_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_percent_looking_at_terrain,
              names_prefix = "average_percent_looking_at_terrain_") %>%
  as.data.frame()


average_percent_by_condition_all

#Now we can plot the results using box and whiskers

average_percent_by_condition_all %>%
  pivot_longer(cols = starts_with("average_percent_looking_at_terrain"), names_to = "gaze_condition", values_to = "average_percent_looking_at_terrain") %>%
  ggplot(aes(x = gaze_condition, y = average_percent_looking_at_terrain, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "Comparison of Average Percentage of Time Looking at Terrain by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Time Looking at Terrain") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```

```{r}
average_angles_by_condition_all

# Assuming average_angles_by_condition_all is a data frame
# Convert to data.table and reshape the data from long to wide format using dcast
average_angles_by_condition_all_dt <- average_angles_by_condition_all %>%
  data.table::as.data.table() %>%
  data.table::dcast(subject_id ~ gaze_condition, 
                    value.var = c("mean_head_angle_using_dir", "mean_eye_angle_using_dir", "mean_distance", "mean_head_angle2"),
                    fun.aggregate = mean)
names(average_angles_by_condition_all_dt)
average_angles_by_condition_all_dt

#Now plot each of the angles and distance
# Box and whiskers for head angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_head_angle_using_dir"), names_to = "gaze_condition", values_to = "mean_head_angle_using_dir") %>%
  ggplot(aes(x = gaze_condition, y = mean_head_angle_using_dir, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Head Angle Using Direction by Gaze Condition", x = "Gaze Condition", y = "Mean Head Angle Using Direction") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for eye angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_eye_angle_using_dir"), names_to = "gaze_condition", values_to = "mean_eye_angle_using_dir") %>%
  ggplot(aes(x = gaze_condition, y = mean_eye_angle_using_dir, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Eye Angle Using Direction by Gaze Condition", x = "Gaze Condition", y = "Mean Eye Angle Using Direction") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for distance
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_distance"), names_to = "gaze_condition", values_to = "mean_distance") %>%
  ggplot(aes(x = gaze_condition, y = mean_distance, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +
  labs(title = "Comparison of Mean Distance by Gaze Condition", x = "Gaze Condition", y = "Mean Distance") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

# Box and whiskers for head angle
average_angles_by_condition_all_dt %>%
  pivot_longer(cols = starts_with("mean_head_angle2"), names_to = "gaze_condition", values_to = "mean_head_angle2") %>%
  ggplot(aes(x = gaze_condition, y = mean_head_angle2, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 360), breaks = seq(0, 360, 45)) +
  labs(title = "Comparison of Mean Head Angle by Gaze Condition", x = "Gaze Condition", y = "Mean Head Angle") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))
```

TODO: I found no differences but we can further group it by task condition and task type if we want.


Now we do it for the student


Single file version to test % of time looking at object

```{r}

#C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/DIB-019_object.csv
object_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/DIB-019_object.csv"
print(object_path)
object_data <- read_csv(object_path, na = na_strings, show_col_types = FALSE) %>%
  clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))
object_dt <- object_data %>% setDT() %>%
  .[round_id != 0]

subject_rounds_dt <- subject_rounds %>%
  mutate(across(c(round_id, task_id), as.double)) %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
# join round_durations on subject_rounds so we can then later convert the percent into an actual duration
subject_rounds_and_durations_dt <- merge(subject_rounds_dt, round_durations, by = c("subject_id", "round_id"), all.x = TRUE)

object_dt <- merge(object_dt, subject_rounds_and_durations_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
object_dt <- object_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

# Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
object_dt <- object_dt %>%
  mutate(task_id_mod3 = task_id %% 3 == 0) %>%
  filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
  select(-task_id_mod3)  # Remove the helper column

# Calculate looking at object based on eye_angle threshold
object_dt <- object_dt %>%
  mutate(looking_at_object = eye_angle_using_pos < 25)  # True if eye_angle is under 25, indicating looking at the object
object_dt

object_dt_grouped <- object_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_syncs = n(),  # Total number of sync_id entries
    valid_hits = sum(looking_at_object, na.rm = TRUE),   # Count of entries where looking at object
    percent_looking_at_object = (valid_hits / total_syncs) * 100,  # Calculate percentage
    time_spent_looking_at_object = round_duration * percent_looking_at_object / 100
  ) %>%
  ungroup()

# Calculate the average percentage of time looking at the object for each gaze condition
average_percent_by_condition <- object_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_percent_looking_at_object = median(percent_looking_at_object, na.rm = TRUE),
    average_time_spent_looking_at_object = median(time_spent_looking_at_object, na.rm = TRUE)
  ) %>%
  ungroup()

average_percent_by_condition
```

Now we do it for all files

```{r}

# Process each object file and calculate the average percentage of time looking at the object with eye_angle over 25
subject_rounds_dt <- subject_rounds %>%
  mutate(across(c(round_id, task_id), as.double)) %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_object_percent_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/[A-Z]{3}-\\d{3}_object\\.csv$") %>%
  map_dfr(.f = function(object_path) {
    print(object_path)
    object_data <- read_csv(object_path, na = na_strings, show_col_types = FALSE) %>%
      clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))
    object_dt <- object_data %>% setDT() %>%
      .[round_id != 0]
    # join round_durations on subject_rounds so we can then later convert the percent into an actual duration
    subject_rounds_and_durations_dt <- merge(subject_rounds_dt, round_durations, by = c("subject_id", "round_id"), all.x = TRUE)

    object_dt <- merge(object_dt, subject_rounds_and_durations_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    object_dt <- object_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    # Filter out entries for task_ids that are multiples of 3 and have unity_log_time greater than round_end
    object_dt <- object_dt %>%
      mutate(task_id_mod3 = task_id %% 3 == 0) %>%
      filter(!(task_id_mod3 & unity_log_time > round_end)) %>%
      select(-task_id_mod3)  # Remove the helper column
    
    # Calculate looking at object based on eye_angle threshold
    object_dt <- object_dt %>%
      mutate(looking_at_object = eye_angle_using_pos < 25)  # True if eye_angle is under 25, indicating looking at the object


    object_dt_grouped <- object_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_syncs = n(),  # Total number of sync_id entries
        valid_hits = sum(looking_at_object, na.rm = TRUE),   # Count of entries where looking at object
        percent_looking_at_object = (valid_hits / total_syncs) * 100,  # Calculate percentage
        time_spent_looking_at_object = round_duration * percent_looking_at_object / 100
      ) %>%
      ungroup()

    # Calculate the average percentage of time looking at the object for each gaze condition
    average_percent_by_condition <- object_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_percent_looking_at_object = median(percent_looking_at_object, na.rm = TRUE),
        average_time_spent_looking_at_object = median(time_spent_looking_at_object, na.rm = TRUE)
      ) %>%
      ungroup()

    average_percent_by_condition
  })
average_object_percent_by_condition_all
```

```{r}
# Now pivot it wider
# TODO: Use the average time spent instead of the percentage
average_object_percent_by_condition_all_wide <- average_object_percent_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_percent_looking_at_object,
              names_prefix = "average_percent_looking_at_object_") %>%
  as.data.frame()

average_object_percent_by_condition_all_wide

#Now we can plot the results using box and whiskers
average_object_percent_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_percent_looking_at_object"), names_to = "gaze_condition", values_to = "average_percent_looking_at_object") %>%
  ggplot(aes(x = gaze_condition, y = average_percent_looking_at_object, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 60), breaks = seq(0, 60, 5)) +
  labs(title = "Comparison of Average Percentage of Time Looking at Student by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Time Looking at Student") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```
There might be a significant difference here so let's test it with wilcoxon.

```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_object_percent_by_condition_all
average_object_percent_by_condition_all_wide

#So for future reference wilcoxon wants to see subject_id, gaze_condition, and average_percent_looking_at_object in that format, so we have that in average_object_percent_by_condition_all

#Now we can do the wilcoxon test
looking_at_object_wilcox <- average_object_percent_by_condition_all %>%
  rstatix::wilcox_test(average_percent_looking_at_object ~ gaze_condition, paired = TRUE)
looking_at_object_wilcox.p <- looking_at_object_wilcox %>%
  add_x_position(x = "gaze_condition")
looking_at_object_wilcox


#Okay there is a sig diff, now we do it the wilcox.test way so we can plot it

looking_at_object_wilcox_test <- wilcox.test(average_object_percent_by_condition_all$average_percent_looking_at_object[average_object_percent_by_condition_all$gaze_condition == "Off"], average_object_percent_by_condition_all$average_percent_looking_at_object[average_object_percent_by_condition_all$gaze_condition == "On"], paired = TRUE)
looking_at_object_wilcox_test
print("Z for Looking at Object")
#compute_wilcox_summary(looking_at_object_wilcox_test)

x_data <- average_object_percent_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_percent_looking_at_object)
y_data <- average_object_percent_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_percent_looking_at_object)
z_object <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
print(paste("Z for Object:", z_object))

# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 50, 
  label = paste("p-value:", formatC(looking_at_object_wilcox_test$p.value, format = "f", digits = 6))
)

average_object_percent_by_condition_all
ggplot(average_object_percent_by_condition_all, aes(x = gaze_condition, y = average_percent_looking_at_object, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 50), breaks = seq(0, 50, 5)) +
  labs(title = "Comparison of Average Percentage of Time Looking at Student by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Time Looking at Student") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)


```
Defined as anytime the eye gaze ray hit the student collider (capsule collider containing student).

Nicer plot version of percentage looking at student.

```{r duration-plot, fig.height=3, fig.width=8}
library(ggplot2)
library(dplyr)

# Calculate summary statistics (mean and IQR)
summary_data <- average_object_percent_by_condition_all %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = median(average_percent_looking_at_object),
    ymin = quantile(average_percent_looking_at_object, 0.25),  # Q1
    ymax = quantile(average_percent_looking_at_object, 0.75)   # Q3
  )

# Correct P-value annotation
wilcox_test_result <- wilcox.test(
  average_object_percent_by_condition_all$average_percent_looking_at_object[average_object_percent_by_condition_all$gaze_condition == "Off"],
  average_object_percent_by_condition_all$average_percent_looking_at_object[average_object_percent_by_condition_all$gaze_condition == "On"],
  paired = TRUE
)
p_value_data <- data.frame(
  x = 1.5, 
  y = 100, 
  label = paste("p =", formatC(wilcox_test_result$p.value, format = "f", digits = 8))
)

p_value_data$label = "p < 0.001"

# Lighter material colors
material_colors <- c("Off" = "#FFD180", "On" = "#81D4FA")

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition),
           stat = "identity", width = 0.0, alpha = 0.0) +
  
  # 2. Boxplot elements: IQR and median
stat_boxplot(
  data = average_object_percent_by_condition_all, 
  aes(
    x = gaze_condition, 
    y = average_percent_looking_at_object,
    ymin = after_stat(ymin), 
    ymax = after_stat(ymax)
  ), 
  geom = "errorbar",
  linewidth = 1, 
  width = 0.15
) +
stat_boxplot(
  data = average_object_percent_by_condition_all, 
  aes(
    x = gaze_condition, 
    y = average_percent_looking_at_object,
    ymin = ..middle.., 
    ymax = ..middle..
  ), 
  geom = "errorbar",
  linewidth = 2, 
  width = 0.5
) +
  # 3. Violin plot
  geom_violin(data = average_object_percent_by_condition_all, aes(x = gaze_condition, y = average_percent_looking_at_object, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
geom_jitter(
  data = average_object_percent_by_condition_all, 
  aes(x = gaze_condition, y = average_percent_looking_at_object, color = gaze_condition),
  width = 0.15, 
  size = 2.5, 
  alpha = 1  # Set to 1 for fully opaque points
) +
scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +  # Darker shades of orange and blue
  
  # 5. P-value annotation
  geom_text(data = p_value_data, aes(x = x, y = y-2, label = label), 
            inherit.aes = FALSE, color = "red", size = 5, fontface = "bold") +
  annotate("segment", x = 1, xend = 2, y = p_value_data$y - 10, 
           yend = p_value_data$y - 10, color = "red", size = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  
  # Labels
  labs(title = "",
       x = "Gaze Condition", 
       y = "Average Percentage of Time Looking at Student") +
  
  # Theme
  theme_minimal(base_size = 14) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  coord_flip() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.x = element_text(size = 12, face = "bold")
  )

```



We'll check annotations later but now let's check the word count distrbution.
First just check total words per condition

Single file test
```{r}
subject_rounds_dt <- subject_rounds %>%
  mutate(across(c(round_id, task_id), as.double)) %>% setDT() %>% setkey("subject_id", "round_id", "task_id")

word_count_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/word_counts.csv"
print(word_count_path)

word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))

# Remove start_round and end_round columns
subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))
word_count_data

word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
word_count_dt

# Need to compute the task_type column based on mod
# mod 3 == 1 -> "Locate"
# mod 3 == 2 -> "Puzzle"
# mod 3 == 0 -> "Remaining"

word_count_dt <- word_count_dt %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
word_count_dt

# Calculate the total word count for each condition
word_count_dt_grouped <- word_count_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_words = sum(count, na.rm = TRUE)  # Total word count
  ) %>%
  ungroup()

# Now calculate the mean word count for each condition
average_word_count_by_condition <- word_count_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_word_count = mean(total_words, na.rm = TRUE)
  ) %>%
  ungroup()
average_word_count_by_condition

```


All files version

```{r}
# Process each word count file and calculate the average word count for each condition
subject_rounds_dt <- subject_rounds %>%
  mutate(across(c(round_id, task_id), as.double)) %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_word_count_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)

    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"

    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    
    # Calculate the total word count for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_words = sum(count, na.rm = TRUE)  # Total word count
      ) %>%
      ungroup()
    
    # Now calculate the mean word count for each condition
    average_word_count_by_condition <- word_count_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_word_count = mean(total_words, na.rm = TRUE)
      ) %>%
      ungroup()
    
    average_word_count_by_condition
  })
average_word_count_by_condition_all

word_count_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)

    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"

    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    
    # Calculate the total word count for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_words = sum(count, na.rm = TRUE)  # Total word count
      ) %>%
      ungroup()
    word_count_dt_grouped
  })

word_count_by_condition_all
```

Now we plot it

```{r}
# Now pivot it wider
average_word_count_by_condition_all_wide <- average_word_count_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_word_count,
              names_prefix = "average_word_count_") %>%
  as.data.frame()

average_word_count_by_condition_all_wide

#Now we can plot the results using box and whiskers
average_word_count_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_word_count"), names_to = "gaze_condition", values_to = "average_word_count") %>%
  ggplot(aes(x = gaze_condition, y = average_word_count, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "Comparison of Average Word Count by Gaze Condition", x = "Gaze Condition", y = "Average Word Count") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```

We may have a significant difference so we should test and see.

```{r}
# Now we can do the wilcoxon test
word_count_wilcox <- average_word_count_by_condition_all %>%
  rstatix::wilcox_test(average_word_count ~ gaze_condition, paired = TRUE)
word_count_wilcox.p <- word_count_wilcox %>%
  add_x_position(x = "gaze_condition")
word_count_wilcox
print("Z for Word Count")
compute_wilcox_summary(word_count_wilcox)

x_data <- average_word_count_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_word_count)
y_data <- average_word_count_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_word_count)
z_word_count <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Word Count:", z_word_count))
```
Looks like we have a significant difference, let's plot it.

```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 100, 
  label = paste("p-value:", formatC(word_count_wilcox$p, format = "f", digits = 6))
)

average_word_count_by_condition_all
ggplot(average_word_count_by_condition_all, aes(x = gaze_condition, y = average_word_count, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "Comparison of Average Words Spoken Per Target Set by Gaze Condition", x = "Gaze Condition", y = "Average Word Count") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```

Now we want nicer plot version

```{r duration-plot, fig.height=3, fig.width=8}
library(ggplot2)
library(dplyr)

# Calculate summary statistics (mean and IQR)
summary_data <- average_word_count_by_condition_all %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = median(average_word_count),
    ymin = quantile(average_word_count, 0.25),  # Q1
    ymax = quantile(average_word_count, 0.75)   # Q3
  )

# Correct P-value annotation
wilcox_test_result <- wilcox.test(
  average_word_count_by_condition_all$average_word_count[average_word_count_by_condition_all$gaze_condition == "Off"],
  average_word_count_by_condition_all$average_word_count[average_word_count_by_condition_all$gaze_condition == "On"],
  paired = TRUE
)
p_value_data <- data.frame(
  x = 1.5, 
  y = 100, 
  label = paste("p =", formatC(wilcox_test_result$p.value, format = "f", digits = 8))
)

p_value_data$label = "p < 0.001"

# Lighter material colors
material_colors <- c("Off" = "#FFD180", "On" = "#81D4FA")

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition),
           stat = "identity", width = 0.0, alpha = 0.0) +
  
  # 2. Boxplot elements: IQR and median
stat_boxplot(
  data = average_word_count_by_condition_all, 
  aes(
    x = gaze_condition, 
    y = average_word_count,
    ymin = after_stat(ymin), 
    ymax = after_stat(ymax)
  ), 
  geom = "errorbar",
  linewidth = 1, 
  width = 0.15
) +
stat_boxplot(
  data = average_word_count_by_condition_all, 
  aes(
    x = gaze_condition, 
    y = average_word_count,
    ymin = ..middle.., 
    ymax = ..middle..
  ), 
  geom = "errorbar",
  linewidth = 2, 
  width = 0.5
) +
  # 3. Violin plot
  geom_violin(data = average_word_count_by_condition_all, aes(x = gaze_condition, y = average_word_count, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
geom_jitter(
  data = average_word_count_by_condition_all, 
  aes(x = gaze_condition, y = average_word_count, color = gaze_condition),
  width = 0.15, 
  size = 2.5, 
  alpha = 1  # Set to 1 for fully opaque points
) +
scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +  # Darker shades of orange and blue
  
  # 5. P-value annotation
  geom_text(data = p_value_data, aes(x = x, y = y-2, label = label), 
            inherit.aes = FALSE, color = "red", size = 5, fontface = "bold") +
  annotate("segment", x = 1, xend = 2, y = p_value_data$y - 10, 
           yend = p_value_data$y - 10, color = "red", size = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  
  # Labels
  labs(title = "",
       x = "Gaze Condition", 
       y = "Average Word Count") +
  
  # Theme
  theme_minimal(base_size = 14) +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  coord_flip() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.x = element_text(size = 12, face = "bold")
  )

```

So people spoke less when they saw the student's gaze. Let's see whether this made them give more 'directional' instructions. Let's first check the percentage of directional instructions.

Directional words:
- right
- left
- down
- up
- straight
- above
- below
- forward
- behind
- back
- between
- over
- under
- towards

Okay so single file test first.

```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
directional_words <- c("right", "left", "down", "up", "straight", "above", "below", "forward", "behind", "back", "between", "over", "under", "towards")


word_count_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/word_counts.csv"
print(word_count_path)

word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))

# Remove start_round and end_round columns
subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))
word_count_data

word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
word_count_dt

# Need to compute the task_type column based on mod
# mod 3 == 1 -> "Locate"
# mod 3 == 2 -> "Puzzle"
# mod 3 == 0 -> "Remaining"

word_count_dt <- word_count_dt %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
word_count_dt

# Now we calculate the total number of directional words for each condition
word_count_dt_grouped <- word_count_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_directional_words = sum(count[word %in% directional_words], na.rm = TRUE)  # Total number of directional words
  ) %>%
  ungroup()
word_count_dt_grouped
# Now calculate the percentage of directional words for each condition
average_directional_words_by_condition <- word_count_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_directional_words = mean(total_directional_words, na.rm = TRUE)
  ) %>%
  ungroup()
average_directional_words_by_condition


```

Now we do it for all files

```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
#directional_words <- c("right", "left", "down", "up", "straight", "above", "below", "forward", "behind", "back", "between", "over", "under", "towards")
directional_words <- c("right", "left", "down", "up")
# Process each word count file and calculate the average percentage of directional words for each condition
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_directional_words_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
    
    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"
    
    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    word_count_dt
    
    # Now we calculate the total number of directional words for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_directional_words = sum(count[word %in% directional_words], na.rm = TRUE)  # Total number of directional words
      ) %>%
      ungroup()
    word_count_dt_grouped
    # Now calculate the percentage of directional words for each condition
    average_directional_words_by_condition <- word_count_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_directional_words = mean(total_directional_words, na.rm = TRUE)
      ) %>%
      ungroup()
    average_directional_words_by_condition
})

average_directional_words_by_condition_all
```
Now we plot it

```{r}

# Now pivot it wider
average_directional_words_by_condition_all_wide <- average_directional_words_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_directional_words,
              names_prefix = "average_directional_words_") %>%
  as.data.frame()

average_directional_words_by_condition_all_wide

#Now we can plot the results using box and whiskers

average_directional_words_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_directional_words"), names_to = "gaze_condition", values_to = "average_directional_words") %>%
  ggplot(aes(x = gaze_condition, y = average_directional_words, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +
  labs(title = "Comparison of Average Percentage of Directional Words by Gaze Condition", x = "Gaze Condition", y = "Average Percentage of Directional Words") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))


```

We may have a significant difference so we should test and see.

```{r}

# Now we can do the wilcoxon test
directional_words_wilcox <- average_directional_words_by_condition_all %>%
  rstatix::wilcox_test(average_directional_words ~ gaze_condition, paired = TRUE)
directional_words_wilcox.p <- directional_words_wilcox %>%
  add_x_position(x = "gaze_condition")
directional_words_wilcox

print("Z for Directional Words")
compute_wilcox_summary(directional_words_wilcox)
compute_wilcoxon_signed_rank_summary(directional_words_wilcox)
x_data <- average_directional_words_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_directional_words)
y_data <- average_directional_words_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_directional_words)

z_directional_words <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
print(paste("Z for Directional Words:", z_directional_words))
```

Looks like we have a significant difference, let's plot it.

```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 5, 
  label = paste("p-value:", formatC(directional_words_wilcox$p, format = "f", digits = 6))
)

average_directional_words_by_condition_all
ggplot(average_directional_words_by_condition_all, aes(x = gaze_condition, y = average_directional_words, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 6), breaks = seq(0, 6, 1)) +
  labs(title = "Comparison of Average Number of Directional Words Per TS by Gaze Condition", x = "Gaze Condition", y="Average Number of Directional Words") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)
```

Now we want nicer plot version

```{r duration-plot, fig.height=3, fig.width=8}
library(ggplot2)
library(dplyr)

# Calculate summary statistics (mean and IQR)
summary_data <- average_directional_words_by_condition_all %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = median(average_directional_words),
    ymin = quantile(average_directional_words, 0.25),  # Q1
    ymax = quantile(average_directional_words, 0.75)   # Q3
  )

# Correct P-value annotation
wilcox_test_result <- wilcox.test(
  average_directional_words_by_condition_all$average_directional_words[average_directional_words_by_condition_all$gaze_condition == "Off"],
  average_directional_words_by_condition_all$average_directional_words[average_directional_words_by_condition_all$gaze_condition == "On"],
  paired = TRUE
)
p_value_data <- data.frame(
  x = 1.5, 
  y = 5, 
  label = paste("p =", formatC(wilcox_test_result$p.value, format = "f", digits = 8))
)

# Use the actual p-value from your existing test
p_value_data$label = paste("p =", formatC(directional_words_wilcox$p, format = "f", digits = 6))

# Lighter material colors
material_colors <- c("Off" = "#FFD180", "On" = "#81D4FA")

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition),
           stat = "identity", width = 0.0, alpha = 0.0) +
  
  # 2. Boxplot elements: IQR and median
  stat_boxplot(
    data = average_directional_words_by_condition_all, 
    aes(
      x = gaze_condition, 
      y = average_directional_words,
      ymin = after_stat(ymin), 
      ymax = after_stat(ymax)
    ), 
    geom = "errorbar",
    linewidth = 1, 
    width = 0.15
  ) +
  stat_boxplot(
    data = average_directional_words_by_condition_all, 
    aes(
      x = gaze_condition, 
      y = average_directional_words,
      ymin = ..middle.., 
      ymax = ..middle..
    ), 
    geom = "errorbar",
    linewidth = 2, 
    width = 0.5
  ) +
  # 3. Violin plot
  geom_violin(data = average_directional_words_by_condition_all, aes(x = gaze_condition, y = average_directional_words, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
  geom_jitter(
    data = average_directional_words_by_condition_all, 
    aes(x = gaze_condition, y = average_directional_words, color = gaze_condition),
    width = 0.15, 
    size = 2.5, 
    alpha = 1  # Set to 1 for fully opaque points
  ) +
  scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +  # Darker shades of orange and blue
  
  # 5. P-value annotation
  geom_text(data = p_value_data, aes(x = x, y = y-0.2, label = label), 
            inherit.aes = FALSE, color = "red", size = 5, fontface = "bold") +
  annotate("segment", x = 1, xend = 2, y = p_value_data$y - 0.5, 
           yend = p_value_data$y - 0.5, color = "red", size = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  
  # Labels
  labs(title = "",
       x = "Gaze Condition", 
       y = "Average Number of Directional Words") +
  
  # Theme
  theme_minimal(base_size = 14) +
  scale_y_continuous(limits = c(0, 6), breaks = seq(0, 6, 1)) +
  coord_flip() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.x = element_text(size = 12, face = "bold")
  )


```


We want to do similar things for simple affirmations and negations. Let's first check the percentage of affirmations and negations.

Affirmations:
- yes
- correct
- right
- true
- sure
- okay
- yep
- yeah
- affirmative
- indeed
- absolutely
- agreed
- confirmed
- fine
- good
- great
- right
- understood
- OK
- agreed
- fine
- good

Negations:
- no
- incorrect
- wrong
- false
- not
- never
- nope
- nah

Single file test first

```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
affirmations <- c("yes", "correct", "right", "true", "sure", "okay", "yep", "yeah", "affirmative", "indeed", "absolutely", "agreed", "confirmed", "fine", "good", "great", "right", "understood", "OK", "agreed", "fine", "good")
negations <- c("no", "incorrect", "wrong", "false", "not", "never", "nope", "nah", "but", "although", "however")

word_count_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/DIB-019/word_counts.csv"
print(word_count_path)

word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
  mutate(subject_id = substr(subject_id, 1, 7))

# Remove start_round and end_round columns
subject_rounds_dt_word_count <- subject_rounds_dt %>% dplyr::select(-c(round_start, round_end))
word_count_data

word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

# Rename some columns back to where they were before
word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
  .[, gaze_condition.x := NULL] %>%
  .[, task_condition.x := NULL] %>%
  .[, gaze_condition.y := NULL] %>%
  .[, task_condition.y := NULL] %>%
  as.data.frame()

word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
word_count_dt

# Need to compute the task_type column based on mod
# mod 3 == 1 -> "Locate"
# mod 3 == 2 -> "Puzzle"
# mod 3 == 0 -> "Remaining"

word_count_dt <- word_count_dt %>%
  mutate(task_id = as.numeric(task_id)) %>%
  setDT() %>% 
  .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
word_count_dt

# Now we calculate the total number of affirmations and negations for each condition
word_count_dt_grouped <- word_count_dt %>%
  group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
  summarise(
    total_affirmations = sum(count[word %in% affirmations], na.rm = TRUE),  # Total number of affirmations
    total_negations = sum(count[word %in% negations], na.rm = TRUE)  # Total number of negations
  ) %>%
  ungroup()
word_count_dt_grouped
# Now calculate the percentage of affirmations and negations for each condition
average_affirmations_negations_by_condition <- word_count_dt_grouped %>%
  group_by(subject_id, gaze_condition) %>%
  summarise(
    average_affirmations = mean(total_affirmations, na.rm = TRUE),
    average_negations = mean(total_negations, na.rm = TRUE)
  ) %>%
  ungroup()
average_affirmations_negations_by_condition

```

Now we do it for all files


```{r}

subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
affirmations <- c("yes", "correct", "right", "true", "sure", "okay", "yep", "yeah", "affirmative", "indeed", "absolutely", "agreed", "confirmed", "fine", "good", "great", "right", "understood", "OK", "agreed", "fine", "good")
negations <- c("no", "incorrect", "wrong", "false", "not", "never", "nope", "nah", "but", "although", "however")
# Process each word count file and calculate the average percentage of directional words for each condition
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
average_affirmations_negations_by_condition_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/word_counts\\.csv$") %>%
  map_dfr(.f = function(word_count_path) {
    print(word_count_path)
    word_count_data <- read_csv(word_count_path, na = na_strings, show_col_types = FALSE, col_types = cols(round_id = col_double(), task_id = col_double())) %>% clean_names() %>%
      mutate(subject_id = substr(subject_id, 1, 7))

    # Remove start_round and end_round columns
    subject_rounds_dt_word_count <- subject_rounds_dt %>% dplyr::select(-c(round_start, round_end))

    word_count_dt <- word_count_data %>% setDT() %>% .[round_id != 0]
    word_count_dt <- merge(word_count_dt, subject_rounds_dt_word_count, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)

    # Rename some columns back to where they were before
    word_count_dt <- word_count_dt %>% .[, c("gaze_condition", "task_condition") := .(gaze_condition.x, task_condition.x)] %>%
      .[, gaze_condition.x := NULL] %>%
      .[, task_condition.x := NULL] %>%
      .[, gaze_condition.y := NULL] %>%
      .[, task_condition.y := NULL] %>%
      as.data.frame()

    word_count_dt <- word_count_dt %>% relocate(gaze_condition, .after = subject_id) %>% relocate(task_condition, .after = gaze_condition) %>% relocate(round_id, .after = task_id)
    
    # Need to compute the task_type column based on mod
    # mod 3 == 1 -> "Locate"
    # mod 3 == 2 -> "Puzzle"
    # mod 3 == 0 -> "Remaining"
    
    word_count_dt <- word_count_dt %>%
      mutate(task_id = as.numeric(task_id)) %>%
      setDT() %>% 
      .[task_id %% 3 == 1, task_type := "Locate"] %>% .[task_id %% 3 == 2, task_type := "Puzzle"] %>% .[task_id %% 3 == 0, task_type := "Remaining"] %>% as.data.frame()
    word_count_dt
    
    # Now we calculate the total number of affirmations and negations for each condition
    word_count_dt_grouped <- word_count_dt %>%
      group_by(subject_id, round_id, task_id, task_type, gaze_condition, task_condition) %>%
      summarise(
        total_affirmations = sum(count[word %in% affirmations], na.rm = TRUE),  # Total number of affirmations
        total_negations = sum(count[word %in% negations], na.rm = TRUE)  # Total number of negations
      ) %>%
      ungroup()
    
    # Now calculate the percentage of affirmations and negations for each condition
    average_affirmations_negations_by_condition <- word_count_dt_grouped %>%
      group_by(subject_id, gaze_condition) %>%
      summarise(
        average_affirmations = mean(total_affirmations, na.rm = TRUE),
        average_negations = mean(total_negations, na.rm = TRUE)
      ) %>%
      ungroup()
    
    average_affirmations_negations_by_condition
  })
average_affirmations_negations_by_condition_all
```

Now we plot it (we will want to plot affirmations and negations separately)
Let's start with affirmations

```{r}

# Now pivot it wider
average_affirmations_negations_by_condition_all_wide <- average_affirmations_negations_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_affirmations,
              names_prefix = "average_affirmations_") %>%
  as.data.frame()

average_affirmations_negations_by_condition_all_wide

#Now we can plot the results using box and whiskers

average_affirmations_negations_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_affirmations"), names_to = "gaze_condition", values_to = "average_affirmations") %>%
  ggplot(aes(x = gaze_condition, y = average_affirmations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +
  labs(title = "Comparison of Average Percentage of Affirmations by Gaze Condition", x = "Gaze Condition", y="Average Number of Affirmations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))

```

We may have a significant difference so we should test and see.


```{r}

# Now we can do the wilcoxon test
affirmations_wilcox <- average_affirmations_negations_by_condition_all %>%
  rstatix::wilcox_test(average_affirmations ~ gaze_condition, paired = TRUE)
affirmations_wilcox.p <- affirmations_wilcox %>%
  add_x_position(x = "gaze_condition")
affirmations_wilcox

print("Z for Affirmations")
compute_wilcox_summary(affirmations_wilcox)

average_affirmations_negations_by_condition_all

x_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_affirmations)
y_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_affirmations)

z_affirmations <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 5)
print(paste("Z for Affirmations:", z_affirmations))

```

Looks like we have a significant difference, let's plot it.


```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 8, 
  label = paste("p-value:", formatC(affirmations_wilcox$p, format = "f", digits = 6))
)

average_affirmations_negations_by_condition_all

ggplot(average_affirmations_negations_by_condition_all, aes(x = gaze_condition, y = average_affirmations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 1)) +
  labs(title = "Comparison of Average Number of Affirmations Per Target Set by Gaze Condition", x = "Gaze Condition", y= "Average Number of Affirmations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)

```

Nicer plot version

```{r duration-plot, fig.height=3, fig.width=8}
# Calculate summary statistics (mean and IQR)
summary_data <- average_affirmations_negations_by_condition_all %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = median(average_affirmations),
    ymin = quantile(average_affirmations, 0.25),  # Q1
    ymax = quantile(average_affirmations, 0.75)   # Q3
  )

# Correct P-value annotation
wilcox_test_result <- wilcox.test(
  average_affirmations_negations_by_condition_all$average_affirmations[average_affirmations_negations_by_condition_all$gaze_condition == "Off"],
  average_affirmations_negations_by_condition_all$average_affirmations[average_affirmations_negations_by_condition_all$gaze_condition == "On"],
  paired = TRUE
)
p_value_data <- data.frame(
  x = 1.5, 
  y = 7, 
  label = paste("p =", formatC(wilcox_test_result$p.value, format = "f", digits = 8))
)

# Use the actual p-value from your existing test
p_value_data$label = paste("p =", formatC(affirmations_wilcox$p, format = "f", digits = 6))

# Lighter material colors
material_colors <- c("Off" = "#FFD180", "On" = "#81D4FA")

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition),
           stat = "identity", width = 0.0, alpha = 0.0) +
  
  # 2. Boxplot elements: IQR and median
  stat_boxplot(
    data = average_affirmations_negations_by_condition_all, 
    aes(
      x = gaze_condition, 
      y = average_affirmations,
      ymin = after_stat(ymin), 
      ymax = after_stat(ymax)
    ), 
    geom = "errorbar",
    linewidth = 1, 
    width = 0.15
  ) +
  stat_boxplot(
    data = average_affirmations_negations_by_condition_all, 
    aes(
      x = gaze_condition, 
      y = average_affirmations,
      ymin = ..middle.., 
      ymax = ..middle..
    ), 
    geom = "errorbar",
    linewidth = 2, 
    width = 0.5
  ) +
  # 3. Violin plot
  geom_violin(data = average_affirmations_negations_by_condition_all, aes(x = gaze_condition, y = average_affirmations, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
  geom_jitter(
    data = average_affirmations_negations_by_condition_all, 
    aes(x = gaze_condition, y = average_affirmations, color = gaze_condition),
    width = 0.15, 
    size = 2.5, 
    alpha = 1  # Set to 1 for fully opaque points
  ) +
  scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +  # Darker shades of orange and blue
  
  # 5. P-value annotation
  geom_text(data = p_value_data, aes(x = x, y = y-0.2, label = label), 
            inherit.aes = FALSE, color = "red", size = 5, fontface = "bold") +
  annotate("segment", x = 1, xend = 2, y = p_value_data$y - 0.5, 
           yend = p_value_data$y - 0.5, color = "red", size = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  
  # Labels
  labs(title = "",
       x = "Gaze Condition", 
       y = "Average Number of Affirmations") +
  
  # Theme
  theme_minimal(base_size = 14) +
  scale_y_continuous(limits = c(0, 8), breaks = seq(0, 8, 1)) +
  coord_flip() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.x = element_text(size = 12, face = "bold")
  )

```

Now we do the same for negations

```{r}

# Now pivot it wider
average_affirmations_negations_by_condition_all_wide <- average_affirmations_negations_by_condition_all %>%
  pivot_wider(names_from = gaze_condition, 
              values_from = average_negations,
              names_prefix = "average_negations_") %>%
  as.data.frame()

average_affirmations_negations_by_condition_all_wide

#Now we can plot the results using box and whiskers

average_affirmations_negations_by_condition_all_wide %>%
  pivot_longer(cols = starts_with("average_negations"), names_to = "gaze_condition", values_to = "average_negations") %>%
  ggplot(aes(x = gaze_condition, y = average_negations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 5), breaks = seq(0, 5, 1)) +
  labs(title = "Comparison of Average Number of Corrections by Gaze Condition", x = "Gaze Condition", y= "Average Number of Negations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red"))


```

We may have a significant difference so we should test and see.

```{r}

# Now we can do the wilcoxon test

negations_wilcox <- average_affirmations_negations_by_condition_all %>%
  rstatix::wilcox_test(average_negations ~ gaze_condition, paired = TRUE)

negations_wilcox.p <- negations_wilcox %>%
  add_x_position(x = "gaze_condition")
negations_wilcox

print("Z for Negations")
compute_wilcox_summary(negations_wilcox)

x_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "On") %>% 
  pull(average_negations)
y_data <- average_affirmations_negations_by_condition_all %>% 
  filter(gaze_condition == "Off") %>% 
  pull(average_negations)

z_negations <- wilcoxonZ(x = x_data, y = y_data, paired = TRUE, correct = TRUE, digits = 3)
print(paste("Z for Negations:", z_negations))

```

Looks like we have a significant difference, let's plot it.

```{r}
# Create a manual p-value annotation data frame
p_value_data <- data.frame(
  x = 1.5, 
  y = 4, 
  label = paste("p-value:", formatC(negations_wilcox$p, format = "f", digits = 6))
)

average_affirmations_negations_by_condition_all

ggplot(average_affirmations_negations_by_condition_all, aes(x = gaze_condition, y = average_negations, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  scale_y_continuous(limits = c(0, 4), breaks = seq(0, 4, 1)) +
  labs(title = "Comparison of Average Number of Corrections Per Target Set by Gaze Condition", x = "Gaze Condition", y= "Average Number of Negations") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  geom_text(data = p_value_data, aes(x = x, y = y, label = label), inherit.aes = FALSE)

```


Now do nicer plot version.

```{r duration-plot, fig.height=3, fig.width=8}
# Calculate summary statistics (mean and IQR)
summary_data <- average_affirmations_negations_by_condition_all %>%
  group_by(gaze_condition) %>%
  summarise(
    mean = median(average_negations),
    ymin = quantile(average_negations, 0.25),  # Q1
    ymax = quantile(average_negations, 0.75)   # Q3
  )

# Correct P-value annotation
wilcox_test_result <- wilcox.test(
  average_affirmations_negations_by_condition_all$average_negations[average_affirmations_negations_by_condition_all$gaze_condition == "Off"],
  average_affirmations_negations_by_condition_all$average_negations[average_affirmations_negations_by_condition_all$gaze_condition == "On"],
  paired = TRUE
)
p_value_data <- data.frame(
  x = 1.5, 
  y = 3.5, 
  label = paste("p =", formatC(wilcox_test_result$p.value, format = "f", digits = 8))
)

# Use the actual p-value from your existing test
p_value_data$label = paste("p =", formatC(negations_wilcox$p, format = "f", digits = 6))

# Lighter material colors
material_colors <- c("Off" = "#FFD180", "On" = "#81D4FA")

# Combined Plot
ggplot() +
  # 1. Bar plot for means
  geom_bar(data = summary_data, aes(x = gaze_condition, y = mean, fill = gaze_condition),
           stat = "identity", width = 0.0, alpha = 0.0) +
  
  # 2. Boxplot elements: IQR and median
  stat_boxplot(
    data = average_affirmations_negations_by_condition_all, 
    aes(
      x = gaze_condition, 
      y = average_negations,
      ymin = after_stat(ymin), 
      ymax = after_stat(ymax)
    ), 
    geom = "errorbar",
    linewidth = 1, 
    width = 0.15
  ) +
  stat_boxplot(
    data = average_affirmations_negations_by_condition_all, 
    aes(
      x = gaze_condition, 
      y = average_negations,
      ymin = ..middle.., 
      ymax = ..middle..
    ), 
    geom = "errorbar",
    linewidth = 2, 
    width = 0.5
  ) +
  # 3. Violin plot
  geom_violin(data = average_affirmations_negations_by_condition_all, aes(x = gaze_condition, y = average_negations, fill = gaze_condition),
              trim = TRUE, alpha = 0.3, color = NA) +
  
  # 4. Jittered dots for individual points
  geom_jitter(
    data = average_affirmations_negations_by_condition_all, 
    aes(x = gaze_condition, y = average_negations, color = gaze_condition),
    width = 0.15, 
    size = 2.5, 
    alpha = 1  # Set to 1 for fully opaque points
  ) +
  scale_color_manual(values = c("Off" = "#FF8C00", "On" = "#0288D1")) +  # Darker shades of orange and blue
  
  # 5. P-value annotation
  geom_text(data = p_value_data, aes(x = x, y = y-0.1, label = label), 
            inherit.aes = FALSE, color = "red", size = 5, fontface = "bold") +
  annotate("segment", x = 1, xend = 2, y = p_value_data$y - 0.3, 
           yend = p_value_data$y - 0.3, color = "red", size = 1) +
  
  # Custom colors
  scale_fill_manual(values = material_colors) +
  
  # Labels
  labs(title = "",
       x = "Gaze Condition", 
       y = "Average Number of Corrections") +
  
  # Theme
  theme_minimal(base_size = 14) +
  scale_y_continuous(limits = c(0, 4), breaks = seq(0, 4, 1)) +
  coord_flip() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    legend.position = "none",
    axis.text.x = element_text(size = 12, face = "bold")
  )

```

Key takeaways -
- People spoke less overall when they saw the student's gaze
- People needed to correct the student more when they did not see the student's gaze
- People needed to use more directional words to guide the student when they did not see the student's gaze
- People needed to use more affirmations when they did not see the student's gaze
- People looked at the student more when they did not see the student's gaze
- We found decreased usage of descriptive guidance strategies and increased usage of spatial-based and minimal guidance strategies when the student's gaze was visible.




Remove and split the next part if needed.


Load up the instructional strategies data
```{r}
# Load the instructional strategies data
instructional_strategies_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/instructional_strategies.csv"

instructional_strategies_data <- read_csv(instructional_strategies_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
# Rename round to round_id and task to task_id
instructional_strategies_data
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
instructional_strategies_data_dt <- instructional_strategies_data %>% setDT() %>% setkey("subject_id", "round_id", "task_id")


terrain_dt <- merge(terrain_dt, subject_rounds_dt, by = c("subject_id", "round_id", "task_id"), all.x = TRUE)



```


Now we want to just do the background questionnaire data

```{r}
# Load the background questionnaire data
background_questionnaire_path <- "C:/Users/Kit/OneDrive/Communication Experiment Data/Communication Analysis/_raw data/questionnaireData.csv"

background_questionnaire_data <- read_csv(background_questionnaire_path, na = na_strings, show_col_types = FALSE) %>% clean_names()
background_questionnaire_data

# Now we just want to summarize the median age and get the count of each gender (female, male, non-binary), we also need a count of vr_experience, tutor_experience, and geology_experience, we also need the min and max age
background_questionnaire_summary <- background_questionnaire_data %>%
  summarise(
    median_age = median(age, na.rm = TRUE),
    min_age = min(age, na.rm = TRUE),
    max_age = max(age, na.rm = TRUE),
    count_female = sum(gender == "Female", na.rm = TRUE),
    count_male = sum(gender == "Male", na.rm = TRUE),
    count_non_binary = sum(gender == "Non-Binary", na.rm = TRUE),
    count_vr_experience = sum(vr_experience, na.rm = TRUE),
    count_tutor_experience = sum(tutor_experience, na.rm = TRUE),
    count_geology_experience = sum(geology_experience, na.rm = TRUE)
  )
background_questionnaire_summary

```




We now want alllllllll the plots in one place with patchwork that are in paper-friendly format so for example:
```{r duration-plot, fig.height=3, fig.width=3}
# Plotting the results. We don't want the p-value label. We don't want to have the x-axis labeled as Gaze Condition. The x-axis labels should instead be "Gaze Off" and "Gaze On". We do not want the legend. We do not want the title. Finally, we want to have error bars show up as well.

wide_mean_durations
# Calculate summary statistics
summary_round_durations <- round_durations %>%
  group_by(gaze_condition) %>%
  summarise(mean_duration = mean(round_duration, na.rm = TRUE),
            sd_duration = sd(round_duration, na.rm = TRUE))

summary_round_durations
# Plot using wide_mean_durations for the boxplot and jitter, and summary_mean_durations for error bars with horizontal caps
ggplot(wide_mean_durations, aes(x = gaze_condition, y = mean_duration, fill = gaze_condition)) +
  geom_boxplot(width = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5) +
  geom_errorbar(data = summary_round_durations, aes(ymin = mean_duration - sd_duration, ymax = mean_duration + sd_duration, x = gaze_condition), width = 0.6, color = "black") +
  # Adding horizontal end caps
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration - sd_duration, yend = mean_duration - sd_duration), color = "black") +
  #geom_segment(data = summary_round_durations, aes(x = as.numeric(as.factor(gaze_condition)) - 0.1, xend = as.numeric(as.factor(gaze_condition)) + 0.1, y = mean_duration + sd_duration, yend = mean_duration + sd_duration), color = "black") +
  scale_y_continuous(limits = c(0, 350), breaks = seq(0, 350, 100)) +
  labs(title = "", x = "", y = "Mean Duration (in seconds)") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))
```

```{r word-count-plot, fig.height=3, fig.width=3}
# Summary statistics
word_count_by_condition_all
summary_word_count <- word_count_by_condition_all %>%
  group_by(gaze_condition) %>%
  summarise(mean_word_count = mean(total_words, na.rm = TRUE),
            sd_word_count = sd(total_words, na.rm = TRUE))
average_word_count_by_condition_all
summary_word_count
word_count_by_condition_all
# Plot with errors bars
ggplot(word_count_by_condition_all, aes(x = gaze_condition, y = average_word_count, fill = gaze_condition)) +
  geom_boxplot() +
  geom_jitter(width = 0.1, alpha = 0.5) +
  #geom_errorbar(data = summary_word_count, aes(ymin = mean_word_count - sd_word_count, ymax = mean_word_count + sd_word_count, x = gaze_condition), width = 0.6, color = "black") +
  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 10)) +
  labs(title = "", x = "", y = "Average Word Count") +
  theme_minimal() +
  scale_fill_manual(values = c("Off" = "blue", "On" = "red")) +
  theme(legend.position = "none") +
  scale_x_discrete(labels = c("Off" = "Gaze Off", "On" = "Gaze On"))
```


Finally, combine with patchwork.
```{r combined-all-plots, fig.height=3, fig.width=3}
```


Load all the data and combine the segments and timings into one data frame.
```{r}

# # Load the data
# directory_path <- "C:\\Users\\Kit\\OneDrive\\Communication Experiment Data\\Communication Analysis\\_raw data\\QWE-042"
# timings_path <- file.path(directory_path, "timings_updated.csv")
# segments_path <- file.path(directory_path, "segments.csv")

# timings <- read_csv(timings_path, show_col_types = FALSE)
# segments <- read_csv(segments_path, show_col_types = FALSE)

# # Process segments and join with timings
# segments_per_round <- segments %>%
#   mutate(text = tolower(text),  # Convert text to lower case
#          text = str_trim(text),  # Trim whitespace
#          text = replace_contraction(text),  # Replace contractions using textclean
#          text = str_replace_all(text, "[[:punct:]]", "")) %>%  # Remove punctuation
#   mutate(across(c(start, end), as.numeric))
  

# timings_dt <- setDT(timings)
# segments_per_round_dt <- setDT(segments_per_round) %>% .[timings_dt, on = .(start <= end, end >= start), allow.cartesian = TRUE]
# segments_per_round <- segments_per_round_dt %>% as.data.frame()
    
#   #right_join(timings, by = character()) %>%
#   #filter(start >= start.x & end <= end.x) %>%
#   #select(round, task, start = start.x, end = end.x, text)
# segments_per_round <- segments_per_round %>% relocate(round, task, start, end, text)
# # Combine text for each round and task
# combined_text_per_round <- segments_per_round %>%
#   group_by(round, task, start, end) %>%
#   summarise(combined_text = paste(text, collapse = " "), .groups = 'drop')  # Combine text entries into a single string

# combined_text_per_round
# # Save the result to a new CSV file
# #write_csv(combined_text_per_round, file.path(directory_path, "combined_text_per_round.csv"))

# # Print path to confirm where the file is saved
# #print(file.path(directory_path, "segments_per_round.csv"))

```


Now we want to do it for all the participants using map_dfr
```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
combined_segments_per_round_all <- fs::dir_ls(path = path_to_files, recurse = TRUE, regexp = "[A-Z]{3}-\\d{3}\\/timings_updated.csv$") %>%
  map_dfr(.f = function(timings_updated_path) {
    print(timings_updated_path)
    segments_path <- str_replace(timings_updated_path, "timings_updated.csv", "segments.csv")
    timings <- read_csv(timings_updated_path, show_col_types = FALSE)
    segments <- read_csv(segments_path, show_col_types = FALSE)
    
    # Process segments and join with timings
    segments_per_round <- segments %>%
      mutate(text = tolower(text),  # Convert text to lower case
             text = str_trim(text),  # Trim whitespace
             text = replace_contraction(text),  # Replace contractions using textclean
             text = str_replace_all(text, "[[:punct:]]", "")) %>%  # Remove punctuation
      mutate(across(c(start, end), as.numeric))
      
    
    timings_dt <- setDT(timings)
    segments_per_round_dt <- setDT(segments_per_round) %>% .[timings_dt, on = .(start <= end, end >= start), allow.cartesian = TRUE]
    segments_per_round <- segments_per_round_dt %>% as.data.frame()
        
      #right_join(timings, by = character()) %>%
      #filter(start >= start.x & end <= end.x) %>%
      #select(round, task, start = start.x, end = end.x, text)
    segments_per_round <- segments_per_round %>% relocate(round, task, start, end, text)
    # Combine text for each round and task
    combined_text_per_round <- segments_per_round %>%
      group_by(round, task, start, end) %>%
      summarise(combined_text = paste(text, collapse = " "), .groups = 'drop')  # Combine text entries into a single string
    # Now we just want to add in the subject id to the round and task
    subject_id <- substr(basename(dirname(timings_updated_path)), 1, 7)
    combined_text_per_round <- combined_text_per_round %>% mutate(subject_id = subject_id)
    combined_text_per_round
  })
#move subject_id before round
combined_segments_per_round_all <- combined_segments_per_round_all %>% relocate(subject_id, .before = round)
combined_segments_per_round_all

# Save the result to a new CSV file
#write_csv(combined_segments_per_round_all, file.path(path_to_files, "combined_segments_per_round_all.csv"))
```

We also want to have a file that just is subject_rounds written to a csv file as that includes every subject.
```{r}
subject_rounds_dt <- subject_rounds %>% setDT() %>% setkey("subject_id", "round_id", "task_id")
subject_rounds_dt
#write_csv(subject_rounds_dt, file.path(path_to_files, "subject_rounds.csv"))
```



